{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        print((\"atomic_numbers\", atomic_numbers))\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        # print('Embed',x.shape)\n",
    "        # print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                # print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                print(x.shape, basis.shape, \"intermediate iterations,\")\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                # print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            # print('Residual',x.shape)\n",
    "\n",
    "            # 5. Predict atomic energies with an ordinary dense layer.\n",
    "            # element_bias = self.param(\n",
    "            #    \"element_bias\",\n",
    "            #    lambda rng, shape: jnp.zeros(shape),\n",
    "            #    (self.max_atomic_number + 1),\n",
    "            # )\n",
    "\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(\n",
    "            x\n",
    "        )  # (..., Natoms, 1, 9, 1)\n",
    "        print(\"After dense:\", x.shape)\n",
    "        element_bias = self.param(\n",
    "            \"element_bias\",\n",
    "            lambda rng, shape: jnp.zeros(shape),\n",
    "            (self.max_atomic_number + 1),\n",
    "        )\n",
    "        # print(element_bias)\n",
    "        # x += element_bias[atomic_numbers]\n",
    "        # print(x.shape, ' after sum ')\n",
    "        x = jax.ops.segment_sum(\n",
    "            x, segment_ids=batch_segments, num_segments=batch_size\n",
    "        )\n",
    "        print(\"After segment_sum:\", x.shape)\n",
    "        x = jnp.sum(x, axis=1)\n",
    "        print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        # x = x[..., :3]\n",
    "        # x = jnp.squeeze(x)\n",
    "        print('After slicing:' ,x.shape)\n",
    "        # x = jnp.sum(x, axis=1)\n",
    "\n",
    "        # x = x[:, 1:4]\n",
    "\n",
    "        print(\"Forma final:\", x.shape)\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "            print(\"pase\", batch_segments, atomic_numbers)\n",
    "            # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "            # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        print(batch_segments.shape, \"batch\", batch_size)\n",
    "        dipole = self.dipole_moment(\n",
    "            atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "        )\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(dipole_prediction, dipole_target):\n",
    "    return jnp.mean(optax.l2_loss(dipole_prediction, dipole_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit, static_argnames=(\"model_apply\", \"optimizer_update\", \"batch_size\")\n",
    ")\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "    def loss_fn(params):\n",
    "        dipole = model_apply(\n",
    "            params,\n",
    "            atomic_numbers=batch[\"atomic_numbers\"],\n",
    "            positions=batch[\"positions\"],\n",
    "            dst_idx=batch[\"dst_idx\"],\n",
    "            src_idx=batch[\"src_idx\"],\n",
    "            batch_segments=batch[\"batch_segments\"],\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        loss = mean_squared_loss(\n",
    "            dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"model_apply\", \"batch_size\"))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "    dipole = model_apply(\n",
    "        params,\n",
    "        atomic_numbers=batch[\"atomic_numbers\"],\n",
    "        positions=batch[\"positions\"],\n",
    "        dst_idx=batch[\"dst_idx\"],\n",
    "        src_idx=batch[\"src_idx\"],\n",
    "        batch_segments=batch[\"batch_segments\"],\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "        dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    key, model, train_data, valid_data, num_epochs, learning_rate, batch_size\n",
    "):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(\n",
    "        len(train_data[\"atomic_numbers\"])\n",
    "    )\n",
    "    params = model.init(\n",
    "        init_key,\n",
    "        atomic_numbers=train_data[\"atomic_numbers\"],\n",
    "        positions=train_data[\"positions\"][0],\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx,\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    # Batches for the validation set need to be prepared only once.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Prepare batches.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "        # Loop over train batches.\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(train_batches):\n",
    "            \n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        valid_loss = 0.0\n",
    "        for i, batch in enumerate(valid_batches):\n",
    "            loss = eval_step(\n",
    "                model_apply=model.apply,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                params=params,\n",
    "            )\n",
    "            valid_loss += (loss - valid_loss) / (i + 1)\n",
    "\n",
    "        # Print progress.\n",
    "        print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "        print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z = jnp.full(16, 14)\n",
    "    Z = jnp.append(Z, 23)\n",
    "    Z = jnp.expand_dims(Z, axis=0)\n",
    "    Z = jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_data.npz\"\n",
    "dataset = np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print(\"Dipole moment shape array\", dataset[\"D\"].shape)\n",
    "print(\"Dipole moment units\", dataset[\"D_units\"])\n",
    "\n",
    "print(\"Atomic numbers\", dataset[\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train = 4000\n",
    "num_val = 1000\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "features = 32\n",
    "max_degree = 1\n",
    "num_iterations = 5\n",
    "num_basis_functions = 16\n",
    "cutoff = 10.0\n",
    "max_atomic_number = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n",
      "(17,) batch 1\n",
      "('atomic_numbers', Array([23, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],      dtype=int32))\n",
      "(17, 1, 1, 32) (272, 1, 4, 16) intermediate iterations,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 1, 4, 32) (272, 1, 4, 16) intermediate iterations,\n",
      "(17, 2, 4, 32) (272, 1, 4, 16) intermediate iterations,\n",
      "(17, 2, 4, 32) (272, 1, 4, 16) intermediate iterations,\n",
      "After dense: (17, 1, 9, 1)\n",
      "After segment_sum: (1, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Forma final: (1, 3)\n",
      "(8772,) batch 516\n",
      "('atomic_numbers', Traced<ShapedArray(int32[8772])>with<DynamicJaxprTrace(level=1/0)>)\n",
      "(8772, 1, 1, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 1, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 2, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 2, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "After dense: (8772, 1, 9, 1)\n",
      "After segment_sum: (516, 1, 9, 1)\n",
      "After sum: (516, 9, 1)\n",
      "After slicing: (516, 3)\n",
      "Forma final: (516, 3)\n",
      "(8772,) batch 516\n",
      "('atomic_numbers', Traced<ShapedArray(int32[8772])>with<DynamicJaxprTrace(level=1/0)>)\n",
      "(8772, 1, 1, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 1, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 2, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "(8772, 2, 4, 32) (140352, 1, 4, 16) intermediate iterations,\n",
      "After dense: (8772, 1, 9, 1)\n",
      "After segment_sum: (516, 1, 9, 1)\n",
      "After sum: (516, 9, 1)\n",
      "After slicing: (516, 3)\n",
      "Forma final: (516, 3)\n",
      "epoch:   1                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:   2                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   3                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   4                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   5                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   6                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   7                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   8                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:   9                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  10                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  11                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  12                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  13                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  14                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  15                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  16                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  17                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  18                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  19                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  20                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  21                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  22                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  23                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  24                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  25                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  26                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  27                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  28                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  29                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  30                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  31                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  32                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  33                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  34                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  35                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  36                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  37                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  38                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  39                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  40                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  41                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  42                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  43                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  44                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  45                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  46                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  47                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  48                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  49                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  50                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  51                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  52                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  53                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  54                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  55                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  56                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  57                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  58                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  59                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  60                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  61                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  62                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  63                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  64                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  65                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  66                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  67                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  68                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  69                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  70                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  71                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  72                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  73                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  74                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  75                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  76                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  77                    train:   valid:\n",
      "    loss [a.u.]                0.879    0.878\n",
      "epoch:  78                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  79                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  80                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  81                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  82                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  83                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  84                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  85                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  86                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  87                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  88                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  89                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  90                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  91                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  92                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  93                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  94                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  95                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  96                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  97                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n",
      "epoch:  98                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  99                    train:   valid:\n",
      "    loss [a.u.]                0.878    0.878\n",
      "epoch:  100                    train:   valid:\n",
      "    loss [a.u.]                0.877    0.878\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[309], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MP_Dipole_Moment(\n\u001b[1;32m      5\u001b[0m     features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m      6\u001b[0m     max_degree\u001b[38;5;241m=\u001b[39mmax_degree,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     cutoff\u001b[38;5;241m=\u001b[39mcutoff\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m params, list_train_loss, list_val_loss \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     12\u001b[0m     key\u001b[38;5;241m=\u001b[39mtrain_key,\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m     train_data\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[1;32m     15\u001b[0m     valid_data\u001b[38;5;241m=\u001b[39mvalid_data,\n\u001b[1;32m     16\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m     17\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m     18\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_datasets(filename, key, num_train, num_val)\n",
    "key, train_key = jax.random.split(key)\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MP_Dipole_Moment(\n",
    "    features=features,\n",
    "    max_degree=max_degree,\n",
    "    num_iterations=num_iterations,\n",
    "    num_basis_functions=num_basis_functions,\n",
    "    cutoff=cutoff\n",
    ")\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
