{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        # print('Embed',x.shape)\n",
    "        # print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                # print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                print(x.shape, basis.shape)\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                # print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            # print('Residual',x.shape)\n",
    "\n",
    "        # 5. Predict atomic energies with an ordinary dense layer.\n",
    "        # element_bias = self.param(\n",
    "        #    \"element_bias\",\n",
    "        #    lambda rng, shape: jnp.zeros(shape),\n",
    "        #    (self.max_atomic_number + 1),\n",
    "        # )\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(\n",
    "            x\n",
    "        )  # (..., Natoms, 1, 9, 1)\n",
    "        print('After dense:',x.shape)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        print('After slicing:' ,x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "            print(\"pase\", batch_segments, atomic_numbers)\n",
    "            # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "            # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        print(batch_segments.shape, \"batch\", batch_size)\n",
    "        dipole = self.dipole_moment(\n",
    "            atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "        )\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(dipole_prediction, dipole_target):\n",
    "    return jnp.mean(optax.l2_loss(dipole_prediction, dipole_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit, static_argnames=(\"model_apply\", \"optimizer_update\", \"batch_size\")\n",
    ")\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "    def loss_fn(params):\n",
    "        dipole = model_apply(\n",
    "            params,\n",
    "            atomic_numbers=batch[\"atomic_numbers\"],\n",
    "            positions=batch[\"positions\"],\n",
    "            dst_idx=batch[\"dst_idx\"],\n",
    "            src_idx=batch[\"src_idx\"],\n",
    "            batch_segments=batch[\"batch_segments\"],\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        loss = mean_squared_loss(\n",
    "            dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"model_apply\", \"batch_size\"))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "    dipole = model_apply(\n",
    "        params,\n",
    "        atomic_numbers=batch[\"atomic_numbers\"],\n",
    "        positions=batch[\"positions\"],\n",
    "        dst_idx=batch[\"dst_idx\"],\n",
    "        src_idx=batch[\"src_idx\"],\n",
    "        batch_segments=batch[\"batch_segments\"],\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "        dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    key, model, train_data, valid_data, num_epochs, learning_rate, batch_size\n",
    "):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(\n",
    "        len(train_data[\"atomic_numbers\"])\n",
    "    )\n",
    "    params = model.init(\n",
    "        init_key,\n",
    "        atomic_numbers=train_data[\"atomic_numbers\"],\n",
    "        positions=train_data[\"positions\"][0],\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx,\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    # Batches for the validation set need to be prepared only once.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Prepare batches.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "        # Loop over train batches.\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(train_batches):\n",
    "            \n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        valid_loss = 0.0\n",
    "        for i, batch in enumerate(valid_batches):\n",
    "            loss = eval_step(\n",
    "                model_apply=model.apply,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                params=params,\n",
    "            )\n",
    "            valid_loss += (loss - valid_loss) / (i + 1)\n",
    "\n",
    "        # Print progress.\n",
    "        print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "        print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z = jnp.full(16, 14)\n",
    "    Z = jnp.append(Z, 23)\n",
    "    Z = jnp.expand_dims(Z, axis=0)\n",
    "    Z = jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_data.npz\"\n",
    "dataset = np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print(\"Dipole moment shape array\", dataset[\"D\"].shape)\n",
    "print(\"Dipole moment units\", dataset[\"D_units\"])\n",
    "\n",
    "print(\"Atomic numbers\", dataset[\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train = 4000\n",
    "num_val = 1000\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n",
      "(17,) batch 1\n",
      "(17, 1, 1, 32) (272, 1, 9, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 1, 9, 32) (272, 1, 9, 8)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "(170,) batch 10\n",
      "(170, 1, 1, 32) (2720, 1, 9, 8)\n",
      "(170, 1, 9, 32) (2720, 1, 9, 8)\n",
      "After dense: (170, 1, 9, 1)\n",
      "After slicing: (1, 3)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "[Chex] Assertion assert_equal_shape failed: Arrays have different shapes: [(1, 3), (10, 3)].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MP_Dipole_Moment()\n\u001b[0;32m----> 5\u001b[0m params, list_train_loss, list_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 75\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     72\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_batches):\n\u001b[0;32m---> 75\u001b[0m     params, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_apply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss \u001b[38;5;241m-\u001b[39m train_loss) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Evaluate on validation set.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model_apply, optimizer_update, batch, batch_size, opt_state, params)\u001b[0m\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mean_squared_loss(\n\u001b[1;32m     16\u001b[0m         dipole_prediction\u001b[38;5;241m=\u001b[39mdipole, dipole_target\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdipole_moment\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 20\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer_update(grad, opt_state, params)\n\u001b[1;32m     22\u001b[0m params \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[1;32m      6\u001b[0m     dipole \u001b[38;5;241m=\u001b[39m model_apply(\n\u001b[1;32m      7\u001b[0m         params,\n\u001b[1;32m      8\u001b[0m         atomic_numbers\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matomic_numbers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdipole_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdipole\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdipole_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdipole_moment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m, in \u001b[0;36mmean_squared_loss\u001b[0;34m(dipole_prediction, dipole_target)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_loss\u001b[39m(dipole_prediction, dipole_target):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdipole_prediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdipole_target\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/e3x_tranfer/paper/lib/python3.11/site-packages/optax/losses/_regression.py:77\u001b[0m, in \u001b[0;36ml2_loss\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ml2_loss\u001b[39m(\n\u001b[1;32m     58\u001b[0m     predictions: chex\u001b[38;5;241m.\u001b[39mArray,\n\u001b[1;32m     59\u001b[0m     targets: Optional[chex\u001b[38;5;241m.\u001b[39mArray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m chex\u001b[38;5;241m.\u001b[39mArray:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the L2 loss for a set of predictions.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m  Note: the 0.5 term is standard in \"Pattern Recognition and Machine Learning\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    elementwise squared differences, with same shape as `predictions`.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msquared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/e3x_tranfer/paper/lib/python3.11/site-packages/optax/losses/_regression.py:52\u001b[0m, in \u001b[0;36msquared_error\u001b[0;34m(predictions, targets)\u001b[0m\n\u001b[1;32m     49\u001b[0m chex\u001b[38;5;241m.\u001b[39massert_type([predictions], \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;66;03m# Avoid broadcasting logic for \"-\" operator.\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m   \u001b[43mchex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_equal_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m errors \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;241m-\u001b[39m targets \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m predictions\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m errors \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Documents/e3x_tranfer/paper/lib/python3.11/site-packages/chex/_src/asserts_internal.py:278\u001b[0m, in \u001b[0;36mchex_assertion.<locals>._chex_assert_fn\u001b[0;34m(custom_message, custom_message_format_vars, include_default_message, exception_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mhost_assertion_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_message_format_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_message_format_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_default_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_default_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mConcretizationTypeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    286\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChex assertion detected `ConcretizationTypeError`: it is very \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlikely that it tried to access tensors\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m values during tracing. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that you defined a jittable version of this chex \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massertion; if that does not help, please file a bug.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/e3x_tranfer/paper/lib/python3.11/site-packages/chex/_src/asserts_internal.py:196\u001b[0m, in \u001b[0;36m_make_host_assertion.<locals>._assert_on_host\u001b[0;34m(custom_message, custom_message_format_vars, include_default_message, exception_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     custom_message \u001b[38;5;241m=\u001b[39m custom_message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mcustom_message_format_vars)\n\u001b[1;32m    194\u001b[0m   error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_type(error_msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: [Chex] Assertion assert_equal_shape failed: Arrays have different shapes: [(1, 3), (10, 3)]."
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_datasets(filename, key, num_train, num_val)\n",
    "key, train_key = jax.random.split(key)\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MP_Dipole_Moment()\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
