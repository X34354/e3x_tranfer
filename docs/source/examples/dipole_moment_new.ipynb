{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0), CudaDevice(id=1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import e3x\n",
    "\n",
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # Esto es más que suficiente para la mayoría de aplicaciones.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calcular vectores de desplazamiento.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Forma (num_pairs, 3).\n",
    "\n",
    "        # 2. Expandir vectores de desplazamiento en funciones base.\n",
    "        basis = e3x.nn.basis(\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embeder números atómicos en el espacio de características.\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        # x tiene forma: (num_atoms, 1, 1, features)\n",
    "\n",
    "        # 4. Realizar iteraciones (message-passing + refinamiento atomístico).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Iteración final.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Refinamiento atomístico MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Conexión residual.\n",
    "            x = e3x.nn.add(x, y)\n",
    "\n",
    "        # 5. Predecir contribuciones atómicas al momento dipolar con una capa densa.\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)\n",
    "        # x tiene forma: (num_atoms, 1, 9, 1)\n",
    "\n",
    "        # 6. Aplicar sesgo elemental.\n",
    "        element_bias = self.param(\n",
    "            \"element_bias\",\n",
    "            lambda rng, shape: jnp.zeros(shape),\n",
    "            (self.max_atomic_number + 1),\n",
    "        )\n",
    "        bias = element_bias[atomic_numbers]  # Forma: (num_atoms,)\n",
    "        bias = bias[:, None, None, None]  # Ajustar forma para broadcasting\n",
    "        x += bias  # Suma con broadcasting\n",
    "\n",
    "        # 7. Sumar contribuciones atómicas para obtener los momentos dipolares moleculares.\n",
    "        # Usamos segment_sum para acumular por segmentos de batch.\n",
    "        x = jax.ops.segment_sum(x, segment_ids=batch_segments, num_segments=batch_size)\n",
    "        # x tiene forma: (batch_size, 1, 9, 1)\n",
    "\n",
    "        # 8. Eliminar dimensiones de tamaño 1 (excepto la dimensión del batch).\n",
    "        x = jnp.squeeze(x, axis=(1, 3))  # Eliminar ejes 1 y 3\n",
    "        # x tiene forma: (batch_size, 9)\n",
    "\n",
    "        # 9. Extraer componentes relevantes (vector de momento dipolar).\n",
    "        x = x[..., 1:4]  # Seleccionar componentes correspondientes al momento dipolar\n",
    "        # x tiene forma: (batch_size, 3)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "        else:\n",
    "            assert batch_size is not None, \"batch_size debe proporcionarse si batch_segments se proporciona\"\n",
    "\n",
    "        dipole = self.dipole_moment(\n",
    "            atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "        )\n",
    "\n",
    "        return dipole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        print((\"atomic_numbers\", atomic_numbers))\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        # print('Embed',x.shape)\n",
    "        # print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                # print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                print(x.shape, basis.shape, \"intermediate iterations,\")\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                # print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            # print('Residual',x.shape)\n",
    "\n",
    "            # 5. Predict atomic energies with an ordinary dense layer.\n",
    "            # element_bias = self.param(\n",
    "            #    \"element_bias\",\n",
    "            #    lambda rng, shape: jnp.zeros(shape),\n",
    "            #    (self.max_atomic_number + 1),\n",
    "            # )\n",
    "\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(\n",
    "            x\n",
    "        )  # (..., Natoms, 1, 9, 1)\n",
    "        print(\"After dense:\", x.shape)\n",
    "        element_bias = self.param(\n",
    "            \"element_bias\",\n",
    "            lambda rng, shape: jnp.zeros(shape),\n",
    "            (self.max_atomic_number + 1),\n",
    "        )\n",
    "        print('element_bias',element_bias[atomic_numbers].shape)\n",
    "        bias= element_bias[atomic_numbers]\n",
    "        x += bias[:,None,None,None]\n",
    "        print(x.shape, ' after bias ')\n",
    "        x = jax.ops.segment_sum(\n",
    "            x, segment_ids=batch_segments, num_segments=batch_size\n",
    "        )\n",
    "        print(\"After segment_sum:\", x.shape)\n",
    "        #x = jnp.sum(x, axis=1)\n",
    "        x=jnp.squeeze(x, axis=0)\n",
    "        print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        # x = x[..., :3]\n",
    "        # x = jnp.squeeze(x)\n",
    "        print('After slicing:' ,x.shape)\n",
    "        # x = jnp.sum(x, axis=1)\n",
    "\n",
    "        # x = x[:, 1:4]\n",
    "\n",
    "        print(\"Forma final:\", x.shape)\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "            print(\"pase\", batch_segments, atomic_numbers)\n",
    "            # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "            # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        print(batch_segments.shape, \"batch\", batch_size)\n",
    "        dipole = self.dipole_moment(\n",
    "            atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "        )\n",
    "        #print(dipole)\n",
    "\n",
    "        return dipole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingModel(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # Esto es excesivo para la mayoría de las aplicaciones.\n",
    "\n",
    "    def energy(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size):\n",
    "        # 1. Calcular vectores de desplazamiento.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Forma (num_pairs, 3).\n",
    "\n",
    "        # 2. Expandir los vectores de desplazamiento en funciones base.\n",
    "        basis = e3x.nn.basis(  # Forma (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff)\n",
    "        )\n",
    "\n",
    "        # 3. Embedding de números atómicos en el espacio de características, x tiene forma (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(num_embeddings=self.max_atomic_number+1, features=self.features)(atomic_numbers)\n",
    "\n",
    "        # 4. Realizar iteraciones (message-passing + refinamiento atómico).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Iteración final.\n",
    "                y = e3x.nn.MessagePass(max_degree=0, include_pseudotensors=False)(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                x = e3x.nn.change_max_degree_or_type(x, max_degree=0, include_pseudotensors=False)\n",
    "            else:\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Refinamiento atómico MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Conexión residual.\n",
    "            x = e3x.nn.add(x, y)\n",
    "\n",
    "        # 5. Predecir cargas parciales atómicas con una capa densa.\n",
    "        charges = e3x.nn.Dense(1, use_bias=True)(x)  # (..., num_atoms, 1, 1, 1)\n",
    "        charges = jnp.squeeze(charges, axis=(-1, -2, -3))  # Eliminar últimas 3 dimensiones.\n",
    "\n",
    "        # 6. Calcular el momento dipolar.\n",
    "        # Multiplicar cargas parciales por posiciones y segmentar por batch.\n",
    "        dipole_contributions = charges[:, None] * positions  # (num_atoms, 3)\n",
    "        dipole_moment = jax.ops.segment_sum(dipole_contributions, segment_ids=batch_segments, num_segments=batch_size)\n",
    "\n",
    "        # Retornar el momento dipolar total sumando sobre las moléculas en el batch.\n",
    "        #total_dipole_moment = jnp.sum(dipole_moment, axis=0)  # Vector de dimensión (3,)\n",
    "\n",
    "        # No necesitamos calcular fuerzas, así que podemos retornar directamente el momento dipolar.\n",
    "        return dipole_moment\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers, positions, dst_idx, src_idx, batch_segments=None, batch_size=None):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "\n",
    "        # Llamamos al método energy para calcular el momento dipolar.\n",
    "        dipole_moment = self.energy(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "\n",
    "        return dipole_moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(dipole_prediction, dipole_target):\n",
    "    return jnp.mean(optax.l2_loss(dipole_prediction, dipole_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit, static_argnames=(\"model_apply\", \"optimizer_update\", \"batch_size\")\n",
    ")\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "    def loss_fn(params):\n",
    "        dipole = model_apply(\n",
    "            params,\n",
    "            atomic_numbers=batch[\"atomic_numbers\"],\n",
    "            positions=batch[\"positions\"],\n",
    "            dst_idx=batch[\"dst_idx\"],\n",
    "            src_idx=batch[\"src_idx\"],\n",
    "            batch_segments=batch[\"batch_segments\"],\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        loss = mean_squared_loss(\n",
    "            dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"model_apply\", \"batch_size\"))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "    dipole = model_apply(\n",
    "        params,\n",
    "        atomic_numbers=batch[\"atomic_numbers\"],\n",
    "        positions=batch[\"positions\"],\n",
    "        dst_idx=batch[\"dst_idx\"],\n",
    "        src_idx=batch[\"src_idx\"],\n",
    "        batch_segments=batch[\"batch_segments\"],\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print('dipole_prediction',dipole[0])\n",
    "    print('dipole_target',batch[\"dipole_moment\"][0])\n",
    "    loss = mean_squared_loss(\n",
    "        dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    key, model, train_data, valid_data, num_epochs, learning_rate, batch_size\n",
    "):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(\n",
    "        len(train_data[\"atomic_numbers\"])\n",
    "    )\n",
    "    params = model.init(\n",
    "        init_key,\n",
    "        atomic_numbers=train_data[\"atomic_numbers\"],\n",
    "        positions=train_data[\"positions\"][0],\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx,\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    # Batches for the validation set need to be prepared only once.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Prepare batches.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "        # Loop over train batches.\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(train_batches):\n",
    "            \n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        valid_loss = 0.0\n",
    "        for i, batch in enumerate(valid_batches):\n",
    "            loss = eval_step(\n",
    "                model_apply=model.apply,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                params=params,\n",
    "            )\n",
    "            valid_loss += (loss - valid_loss) / (i + 1)\n",
    "\n",
    "        # Print progress.\n",
    "        print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "        print(f\"    loss [a.u.]             {train_loss : 8.6f} {valid_loss : 8.3f}\")\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z = jnp.full(16, 14)\n",
    "    Z = jnp.append(Z, 23)\n",
    "    Z = jnp.expand_dims(Z, axis=0)\n",
    "    Z = jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "z\n",
      "E\n",
      "F\n",
      "F_units\n",
      "e_unit\n",
      "r_unit\n",
      "name\n",
      "theory\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "README\n",
      "F_min\n",
      "F_max\n",
      "F_mean\n",
      "F_var\n",
      "E_min\n",
      "E_max\n",
      "E_mean\n",
      "E_var\n",
      "md5\n",
      "Dipole moment shape array (2580, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "filename = \"Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.2580.R_E_F_D_Q.npz\"\n",
    "dataset = np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print(\"Dipole moment shape array\", dataset[\"D\"].shape)\n",
    "print(\"Dipole moment units\", dataset[\"D_units\"])\n",
    "\n",
    "print(\"Atomic numbers\", dataset[\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic numbers [14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 23]\n"
     ]
    }
   ],
   "source": [
    "Z = jnp.full(16, 14)\n",
    "Z = jnp.append(Z, 23)\n",
    "Z = jnp.expand_dims(Z, axis=0)\n",
    "num_data = len(dataset[\"E\"])\n",
    "#Z = jnp.repeat(Z, num_data, axis=0)\n",
    "# Create a dictionary to store modifiable copies\n",
    "modifiable_copies = {key: dataset[key].copy() for key in dataset}\n",
    "\n",
    "# Make any modifications you need\n",
    "# For example, if there's an array with the key 'arr_0'\n",
    "modifiable_copies['z'] = Z[0]  # Example modification\n",
    "\n",
    "filename = \"Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.2580.R_E_F_D_Q_v2.npz\"\n",
    "\n",
    "# Save the modified arrays to a new npz file if needed\n",
    "np.savez(filename, **modifiable_copies)\n",
    "\n",
    "\n",
    "dataset = np.load(filename)\n",
    "print(\"Atomic numbers\", dataset[\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train = 2100\n",
    "num_val = 400\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "features = 128\n",
    "max_degree = 2\n",
    "num_iterations = 3\n",
    "num_basis_functions = 16\n",
    "cutoff = 6.0\n",
    "max_atomic_number = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dipole_prediction Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "dipole_target Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "epoch:   1                    train:   valid:\n",
      "    loss [a.u.]              0.010161    0.005\n",
      "epoch:   2                    train:   valid:\n",
      "    loss [a.u.]              0.003036    0.002\n",
      "epoch:   3                    train:   valid:\n",
      "    loss [a.u.]              0.001270    0.001\n",
      "epoch:   4                    train:   valid:\n",
      "    loss [a.u.]              0.000694    0.001\n",
      "epoch:   5                    train:   valid:\n",
      "    loss [a.u.]              0.000504    0.001\n",
      "epoch:   6                    train:   valid:\n",
      "    loss [a.u.]              0.000404    0.000\n",
      "epoch:   7                    train:   valid:\n",
      "    loss [a.u.]              0.000358    0.000\n",
      "epoch:   8                    train:   valid:\n",
      "    loss [a.u.]              0.000330    0.000\n",
      "epoch:   9                    train:   valid:\n",
      "    loss [a.u.]              0.000307    0.000\n",
      "epoch:  10                    train:   valid:\n",
      "    loss [a.u.]              0.000295    0.000\n",
      "epoch:  11                    train:   valid:\n",
      "    loss [a.u.]              0.000291    0.000\n",
      "epoch:  12                    train:   valid:\n",
      "    loss [a.u.]              0.000283    0.000\n",
      "epoch:  13                    train:   valid:\n",
      "    loss [a.u.]              0.000281    0.000\n",
      "epoch:  14                    train:   valid:\n",
      "    loss [a.u.]              0.000284    0.000\n",
      "epoch:  15                    train:   valid:\n",
      "    loss [a.u.]              0.000278    0.000\n",
      "epoch:  16                    train:   valid:\n",
      "    loss [a.u.]              0.000279    0.000\n",
      "epoch:  17                    train:   valid:\n",
      "    loss [a.u.]              0.000279    0.000\n",
      "epoch:  18                    train:   valid:\n",
      "    loss [a.u.]              0.000277    0.000\n",
      "epoch:  19                    train:   valid:\n",
      "    loss [a.u.]              0.000276    0.000\n",
      "epoch:  20                    train:   valid:\n",
      "    loss [a.u.]              0.000273    0.000\n",
      "epoch:  21                    train:   valid:\n",
      "    loss [a.u.]              0.000275    0.000\n",
      "epoch:  22                    train:   valid:\n",
      "    loss [a.u.]              0.000272    0.000\n",
      "epoch:  23                    train:   valid:\n",
      "    loss [a.u.]              0.000272    0.000\n",
      "epoch:  24                    train:   valid:\n",
      "    loss [a.u.]              0.000272    0.000\n",
      "epoch:  25                    train:   valid:\n",
      "    loss [a.u.]              0.000269    0.000\n",
      "epoch:  26                    train:   valid:\n",
      "    loss [a.u.]              0.000267    0.000\n",
      "epoch:  27                    train:   valid:\n",
      "    loss [a.u.]              0.000262    0.000\n",
      "epoch:  28                    train:   valid:\n",
      "    loss [a.u.]              0.000261    0.000\n",
      "epoch:  29                    train:   valid:\n",
      "    loss [a.u.]              0.000257    0.000\n",
      "epoch:  30                    train:   valid:\n",
      "    loss [a.u.]              0.000252    0.000\n",
      "epoch:  31                    train:   valid:\n",
      "    loss [a.u.]              0.000239    0.000\n",
      "epoch:  32                    train:   valid:\n",
      "    loss [a.u.]              0.000227    0.000\n",
      "epoch:  33                    train:   valid:\n",
      "    loss [a.u.]              0.000211    0.000\n",
      "epoch:  34                    train:   valid:\n",
      "    loss [a.u.]              0.000193    0.000\n",
      "epoch:  35                    train:   valid:\n",
      "    loss [a.u.]              0.000175    0.000\n",
      "epoch:  36                    train:   valid:\n",
      "    loss [a.u.]              0.000166    0.000\n",
      "epoch:  37                    train:   valid:\n",
      "    loss [a.u.]              0.000158    0.000\n",
      "epoch:  38                    train:   valid:\n",
      "    loss [a.u.]              0.000155    0.000\n",
      "epoch:  39                    train:   valid:\n",
      "    loss [a.u.]              0.000152    0.000\n",
      "epoch:  40                    train:   valid:\n",
      "    loss [a.u.]              0.000155    0.000\n",
      "epoch:  41                    train:   valid:\n",
      "    loss [a.u.]              0.000154    0.000\n",
      "epoch:  42                    train:   valid:\n",
      "    loss [a.u.]              0.000149    0.000\n",
      "epoch:  43                    train:   valid:\n",
      "    loss [a.u.]              0.000144    0.000\n",
      "epoch:  44                    train:   valid:\n",
      "    loss [a.u.]              0.000144    0.000\n",
      "epoch:  45                    train:   valid:\n",
      "    loss [a.u.]              0.000141    0.000\n",
      "epoch:  46                    train:   valid:\n",
      "    loss [a.u.]              0.000148    0.000\n",
      "epoch:  47                    train:   valid:\n",
      "    loss [a.u.]              0.000150    0.000\n",
      "epoch:  48                    train:   valid:\n",
      "    loss [a.u.]              0.000136    0.000\n",
      "epoch:  49                    train:   valid:\n",
      "    loss [a.u.]              0.000141    0.000\n",
      "epoch:  50                    train:   valid:\n",
      "    loss [a.u.]              0.000140    0.000\n",
      "epoch:  51                    train:   valid:\n",
      "    loss [a.u.]              0.000138    0.000\n",
      "epoch:  52                    train:   valid:\n",
      "    loss [a.u.]              0.000140    0.000\n",
      "epoch:  53                    train:   valid:\n",
      "    loss [a.u.]              0.000140    0.000\n",
      "epoch:  54                    train:   valid:\n",
      "    loss [a.u.]              0.000135    0.000\n",
      "epoch:  55                    train:   valid:\n",
      "    loss [a.u.]              0.000133    0.000\n",
      "epoch:  56                    train:   valid:\n",
      "    loss [a.u.]              0.000134    0.000\n",
      "epoch:  57                    train:   valid:\n",
      "    loss [a.u.]              0.000134    0.000\n",
      "epoch:  58                    train:   valid:\n",
      "    loss [a.u.]              0.000131    0.000\n",
      "epoch:  59                    train:   valid:\n",
      "    loss [a.u.]              0.000130    0.000\n",
      "epoch:  60                    train:   valid:\n",
      "    loss [a.u.]              0.000129    0.000\n",
      "epoch:  61                    train:   valid:\n",
      "    loss [a.u.]              0.000130    0.000\n",
      "epoch:  62                    train:   valid:\n",
      "    loss [a.u.]              0.000130    0.000\n",
      "epoch:  63                    train:   valid:\n",
      "    loss [a.u.]              0.000131    0.000\n",
      "epoch:  64                    train:   valid:\n",
      "    loss [a.u.]              0.000132    0.000\n",
      "epoch:  65                    train:   valid:\n",
      "    loss [a.u.]              0.000125    0.000\n",
      "epoch:  66                    train:   valid:\n",
      "    loss [a.u.]              0.000124    0.000\n",
      "epoch:  67                    train:   valid:\n",
      "    loss [a.u.]              0.000125    0.000\n",
      "epoch:  68                    train:   valid:\n",
      "    loss [a.u.]              0.000126    0.000\n",
      "epoch:  69                    train:   valid:\n",
      "    loss [a.u.]              0.000125    0.000\n",
      "epoch:  70                    train:   valid:\n",
      "    loss [a.u.]              0.000122    0.000\n",
      "epoch:  71                    train:   valid:\n",
      "    loss [a.u.]              0.000120    0.000\n",
      "epoch:  72                    train:   valid:\n",
      "    loss [a.u.]              0.000121    0.000\n",
      "epoch:  73                    train:   valid:\n",
      "    loss [a.u.]              0.000126    0.000\n",
      "epoch:  74                    train:   valid:\n",
      "    loss [a.u.]              0.000123    0.000\n",
      "epoch:  75                    train:   valid:\n",
      "    loss [a.u.]              0.000116    0.000\n",
      "epoch:  76                    train:   valid:\n",
      "    loss [a.u.]              0.000116    0.000\n",
      "epoch:  77                    train:   valid:\n",
      "    loss [a.u.]              0.000116    0.000\n",
      "epoch:  78                    train:   valid:\n",
      "    loss [a.u.]              0.000119    0.000\n",
      "epoch:  79                    train:   valid:\n",
      "    loss [a.u.]              0.000119    0.000\n",
      "epoch:  80                    train:   valid:\n",
      "    loss [a.u.]              0.000123    0.000\n",
      "epoch:  81                    train:   valid:\n",
      "    loss [a.u.]              0.000118    0.000\n",
      "epoch:  82                    train:   valid:\n",
      "    loss [a.u.]              0.000125    0.000\n",
      "epoch:  83                    train:   valid:\n",
      "    loss [a.u.]              0.000126    0.000\n",
      "epoch:  84                    train:   valid:\n",
      "    loss [a.u.]              0.000121    0.000\n",
      "epoch:  85                    train:   valid:\n",
      "    loss [a.u.]              0.000124    0.000\n",
      "epoch:  86                    train:   valid:\n",
      "    loss [a.u.]              0.000118    0.000\n",
      "epoch:  87                    train:   valid:\n",
      "    loss [a.u.]              0.000122    0.000\n",
      "epoch:  88                    train:   valid:\n",
      "    loss [a.u.]              0.000118    0.000\n",
      "epoch:  89                    train:   valid:\n",
      "    loss [a.u.]              0.000117    0.000\n",
      "epoch:  90                    train:   valid:\n",
      "    loss [a.u.]              0.000115    0.000\n",
      "epoch:  91                    train:   valid:\n",
      "    loss [a.u.]              0.000113    0.000\n",
      "epoch:  92                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  93                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  94                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  95                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  96                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  97                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  98                    train:   valid:\n",
      "    loss [a.u.]              0.000113    0.000\n",
      "epoch:  99                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  100                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  101                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  102                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  103                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  104                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  105                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  106                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  107                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  108                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  109                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  110                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  111                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  112                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  113                    train:   valid:\n",
      "    loss [a.u.]              0.000113    0.000\n",
      "epoch:  114                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  115                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  116                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  117                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  118                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  119                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  120                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  121                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  122                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  123                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  124                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  125                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  126                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  127                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  128                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  129                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  130                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  131                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  132                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  133                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  134                    train:   valid:\n",
      "    loss [a.u.]              0.000115    0.000\n",
      "epoch:  135                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  136                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  137                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  138                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  139                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  140                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  141                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  142                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  143                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  144                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  145                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  146                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  147                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  148                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  149                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  150                    train:   valid:\n",
      "    loss [a.u.]              0.000115    0.000\n",
      "epoch:  151                    train:   valid:\n",
      "    loss [a.u.]              0.000110    0.000\n",
      "epoch:  152                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  153                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  154                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  155                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  156                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  157                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  158                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  159                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  160                    train:   valid:\n",
      "    loss [a.u.]              0.000109    0.000\n",
      "epoch:  161                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  162                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  163                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  164                    train:   valid:\n",
      "    loss [a.u.]              0.000105    0.000\n",
      "epoch:  165                    train:   valid:\n",
      "    loss [a.u.]              0.000111    0.000\n",
      "epoch:  166                    train:   valid:\n",
      "    loss [a.u.]              0.000117    0.000\n",
      "epoch:  167                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  168                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  169                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  170                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  171                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  172                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  173                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  174                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  175                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  176                    train:   valid:\n",
      "    loss [a.u.]              0.000108    0.000\n",
      "epoch:  177                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  178                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  179                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  180                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  181                    train:   valid:\n",
      "    loss [a.u.]              0.000105    0.000\n",
      "epoch:  182                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  183                    train:   valid:\n",
      "    loss [a.u.]              0.000112    0.000\n",
      "epoch:  184                    train:   valid:\n",
      "    loss [a.u.]              0.000107    0.000\n",
      "epoch:  185                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  186                    train:   valid:\n",
      "    loss [a.u.]              0.000103    0.000\n",
      "epoch:  187                    train:   valid:\n",
      "    loss [a.u.]              0.000105    0.000\n",
      "epoch:  188                    train:   valid:\n",
      "    loss [a.u.]              0.000103    0.000\n",
      "epoch:  189                    train:   valid:\n",
      "    loss [a.u.]              0.000105    0.000\n",
      "epoch:  190                    train:   valid:\n",
      "    loss [a.u.]              0.000103    0.000\n",
      "epoch:  191                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  192                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  193                    train:   valid:\n",
      "    loss [a.u.]              0.000101    0.000\n",
      "epoch:  194                    train:   valid:\n",
      "    loss [a.u.]              0.000103    0.000\n",
      "epoch:  195                    train:   valid:\n",
      "    loss [a.u.]              0.000101    0.000\n",
      "epoch:  196                    train:   valid:\n",
      "    loss [a.u.]              0.000103    0.000\n",
      "epoch:  197                    train:   valid:\n",
      "    loss [a.u.]              0.000104    0.000\n",
      "epoch:  198                    train:   valid:\n",
      "    loss [a.u.]              0.000105    0.000\n",
      "epoch:  199                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n",
      "epoch:  200                    train:   valid:\n",
      "    loss [a.u.]              0.000106    0.000\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_datasets(filename, key, num_train, num_val)\n",
    "key, train_key = jax.random.split(key)\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MessagePassingModel(\n",
    "    features=features,\n",
    "    max_degree=max_degree,\n",
    "    num_iterations=num_iterations,\n",
    "    num_basis_functions=num_basis_functions,\n",
    "    cutoff=cutoff,\n",
    "    max_atomic_number=max_atomic_number\n",
    ")\n",
    "params = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(valid_data[\"atomic_numbers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(valid_data[\"atomic_numbers\"]))\n",
    "dm=model.apply(params,\n",
    "    atomic_numbers=valid_data[\"atomic_numbers\"],\n",
    "    positions=valid_data[\"positions\"][i],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[-0.12295785,  0.07603097,  0.05366488]], dtype=float32), Array([[-0.05956385, -0.01028574, -0.08678952]], dtype=float32), Array([[-0.00113255,  0.13166693, -0.05121334]], dtype=float32), Array([[ 0.08384466, -0.1690137 , -0.05031166]], dtype=float32), Array([[ 0.06959398, -0.0640862 , -0.07719103]], dtype=float32), Array([[ 0.09159677, -0.06689365, -0.07660389]], dtype=float32), Array([[-0.03515494, -0.02944154, -0.11655372]], dtype=float32), Array([[0.00918785, 0.16521768, 0.06059112]], dtype=float32), Array([[ 0.02808684,  0.20254311, -0.12463631]], dtype=float32), Array([[ 0.05116147, -0.00539858, -0.08297139]], dtype=float32), Array([[ 0.02122052, -0.06878117,  0.00228417]], dtype=float32), Array([[ 0.0588226 , -0.01250644,  0.01518213]], dtype=float32), Array([[-0.08204719, -0.10819939, -0.11243004]], dtype=float32), Array([[0.04752875, 0.03249125, 0.148797  ]], dtype=float32), Array([[0.2572654 , 0.12914923, 0.04959355]], dtype=float32), Array([[ 0.10166347, -0.03851426, -0.15086389]], dtype=float32), Array([[ 0.0823275 , -0.11260915,  0.12112765]], dtype=float32), Array([[ 0.04431751,  0.16217715, -0.1846211 ]], dtype=float32), Array([[0.02301959, 0.07147896, 0.03543022]], dtype=float32), Array([[-0.03320241, -0.05735492, -0.08852377]], dtype=float32), Array([[0.11851682, 0.01034257, 0.07938567]], dtype=float32), Array([[ 0.05294278, -0.00178064, -0.00866287]], dtype=float32), Array([[-0.21322504, -0.0669283 ,  0.04388726]], dtype=float32), Array([[-0.03039405, -0.01127678, -0.09564433]], dtype=float32), Array([[ 0.18844326,  0.07427472, -0.0224953 ]], dtype=float32), Array([[ 0.01063102,  0.14319238, -0.02983811]], dtype=float32), Array([[ 0.02037742,  0.05130875, -0.11056629]], dtype=float32), Array([[-0.07160977, -0.07185815, -0.06338637]], dtype=float32), Array([[-0.04701626, -0.02410963,  0.13840841]], dtype=float32), Array([[-0.0970805 ,  0.09884917,  0.00673165]], dtype=float32), Array([[-0.02495423,  0.07406214,  0.00526613]], dtype=float32), Array([[-0.06215921, -0.03463401,  0.0511449 ]], dtype=float32), Array([[-0.08759639, -0.01517455,  0.08093165]], dtype=float32), Array([[ 0.04547712, -0.00736386, -0.06030117]], dtype=float32), Array([[-0.10283153,  0.04902568,  0.03982079]], dtype=float32), Array([[-0.01127732, -0.0821767 ,  0.22264282]], dtype=float32), Array([[-0.1211929 , -0.02867857,  0.13460986]], dtype=float32), Array([[0.03235453, 0.04379436, 0.06998734]], dtype=float32), Array([[-0.21603173, -0.04023956, -0.14283168]], dtype=float32), Array([[-0.15479016, -0.11328888,  0.02189995]], dtype=float32), Array([[ 0.14423674, -0.13024165,  0.07769211]], dtype=float32), Array([[ 0.05838677, -0.15373677,  0.29800332]], dtype=float32), Array([[ 0.00664401, -0.05365637,  0.08466533]], dtype=float32), Array([[ 0.04107311, -0.08285375,  0.0352256 ]], dtype=float32), Array([[ 0.09189382,  0.14945696, -0.27537662]], dtype=float32), Array([[-0.02056666,  0.034159  ,  0.09716789]], dtype=float32), Array([[-0.04845592,  0.02155429,  0.00167584]], dtype=float32), Array([[-0.13492385, -0.07530233, -0.12091772]], dtype=float32), Array([[ 0.04214349, -0.10954681,  0.21470428]], dtype=float32), Array([[-0.10110208, -0.08481222,  0.20791343]], dtype=float32), Array([[-0.11113772, -0.01754167,  0.02195485]], dtype=float32), Array([[-0.17008975,  0.02727899,  0.13836014]], dtype=float32), Array([[ 0.02919848,  0.03262815, -0.18193656]], dtype=float32), Array([[-0.02680835,  0.00738508, -0.00010498]], dtype=float32), Array([[ 0.00432682,  0.00944108, -0.0058816 ]], dtype=float32), Array([[ 0.12216084, -0.00535074, -0.24112421]], dtype=float32), Array([[ 0.0586449 , -0.06575313, -0.13728654]], dtype=float32), Array([[ 0.05312648, -0.04995793,  0.03070967]], dtype=float32), Array([[0.02727556, 0.07605395, 0.08256277]], dtype=float32), Array([[-0.04774997,  0.08202969,  0.14478189]], dtype=float32), Array([[-0.01070181,  0.01553735, -0.17813283]], dtype=float32), Array([[ 0.00511517, -0.07290357,  0.16785178]], dtype=float32), Array([[-0.04537854,  0.08392188, -0.01552171]], dtype=float32), Array([[-0.08780735, -0.00164056, -0.0618016 ]], dtype=float32), Array([[-0.05845511, -0.01220906,  0.21167365]], dtype=float32), Array([[-0.12361258, -0.12482443, -0.12908888]], dtype=float32), Array([[ 0.11110565, -0.06164303,  0.01345028]], dtype=float32), Array([[-0.05910749, -0.03645423, -0.09996398]], dtype=float32), Array([[ 0.03952041, -0.01088612,  0.1900783 ]], dtype=float32), Array([[ 0.02275605,  0.12112093, -0.11469519]], dtype=float32), Array([[ 0.12069604, -0.03025621, -0.00565524]], dtype=float32), Array([[ 0.00146346,  0.02167639, -0.06738061]], dtype=float32), Array([[-0.0551025 , -0.04297294, -0.03274031]], dtype=float32), Array([[-0.00397483,  0.11923987,  0.12930803]], dtype=float32), Array([[ 0.24035901, -0.01751868, -0.0486158 ]], dtype=float32), Array([[-0.06669021,  0.03266031, -0.08702645]], dtype=float32), Array([[-0.11014621,  0.00333431, -0.09785816]], dtype=float32), Array([[-0.07166645, -0.16563638, -0.15857494]], dtype=float32), Array([[ 0.05742471, -0.12453339,  0.03427672]], dtype=float32), Array([[-0.04315592, -0.1413346 , -0.30516446]], dtype=float32), Array([[ 0.03623513, -0.08619179, -0.09345216]], dtype=float32), Array([[0.04721892, 0.09740691, 0.02882217]], dtype=float32), Array([[ 0.07975298, -0.1080671 , -0.10827265]], dtype=float32), Array([[-0.08976276,  0.06679678,  0.0628546 ]], dtype=float32), Array([[-0.19296867, -0.07333598,  0.00118741]], dtype=float32), Array([[-0.12434983, -0.03521743,  0.00231278]], dtype=float32), Array([[-0.02006003, -0.24306443,  0.06931554]], dtype=float32), Array([[-0.02071482,  0.06238396, -0.06883669]], dtype=float32), Array([[-0.08394289,  0.0108795 ,  0.26271495]], dtype=float32), Array([[-0.16827744, -0.05076008, -0.03973307]], dtype=float32), Array([[-0.1035355, -0.1282175, -0.0697529]], dtype=float32), Array([[-0.03714824, -0.11098677,  0.02387656]], dtype=float32), Array([[-0.02999147,  0.181544  ,  0.06114925]], dtype=float32), Array([[ 0.03920348,  0.09803601, -0.01382569]], dtype=float32), Array([[0.17415702, 0.10741942, 0.02133781]], dtype=float32), Array([[0.07338199, 0.05286068, 0.00442149]], dtype=float32), Array([[ 0.01019977, -0.01409154, -0.02952185]], dtype=float32), Array([[-0.06395113,  0.01877889, -0.2200939 ]], dtype=float32), Array([[ 0.20945641,  0.0324806 , -0.06553242]], dtype=float32), Array([[-0.09525228,  0.1196788 ,  0.05162258]], dtype=float32), Array([[-0.15137759, -0.05434942,  0.22171472]], dtype=float32), Array([[0.19868538, 0.08041267, 0.03843005]], dtype=float32), Array([[ 0.04746233, -0.03972042, -0.08832411]], dtype=float32), Array([[-0.0471876 ,  0.13142726,  0.15024863]], dtype=float32), Array([[-0.21843094, -0.04476242,  0.08538474]], dtype=float32), Array([[-0.07089115, -0.01051232, -0.05247447]], dtype=float32), Array([[ 0.16621752, -0.01880825, -0.01379754]], dtype=float32), Array([[ 0.028328  ,  0.12075368, -0.11875694]], dtype=float32), Array([[-0.10666248,  0.00501248, -0.01625805]], dtype=float32), Array([[ 0.05881959,  0.10944532, -0.10755001]], dtype=float32), Array([[-0.19916397, -0.13293187,  0.02864993]], dtype=float32), Array([[ 0.01483213, -0.00085643,  0.08737809]], dtype=float32), Array([[0.03430825, 0.06879093, 0.04781219]], dtype=float32), Array([[ 0.11167513, -0.07739928, -0.06661969]], dtype=float32), Array([[-0.01187978, -0.01977676, -0.07932565]], dtype=float32), Array([[-0.13031468,  0.02280936,  0.01375571]], dtype=float32), Array([[-0.13374156, -0.08056425,  0.18115167]], dtype=float32), Array([[-0.02610907,  0.04036546,  0.14649282]], dtype=float32), Array([[-0.09255654, -0.083915  , -0.09706335]], dtype=float32), Array([[-0.01961219, -0.12361111, -0.07613079]], dtype=float32), Array([[ 0.08954052, -0.05474609,  0.08916521]], dtype=float32), Array([[0.03952083, 0.07361528, 0.01290751]], dtype=float32), Array([[ 0.06995001, -0.04959869,  0.11987808]], dtype=float32), Array([[ 0.14665517, -0.143121  ,  0.09994269]], dtype=float32), Array([[-0.02242066, -0.04828015, -0.07954289]], dtype=float32), Array([[0.24683616, 0.04420211, 0.13173112]], dtype=float32), Array([[ 0.04559752, -0.02163619,  0.0625762 ]], dtype=float32), Array([[ 0.0233949 , -0.00131786, -0.05447511]], dtype=float32), Array([[-0.00516891, -0.10335861,  0.03103797]], dtype=float32), Array([[-0.20692456, -0.09408161,  0.00060052]], dtype=float32), Array([[ 0.17851233, -0.02853656,  0.08447421]], dtype=float32), Array([[-0.05606696,  0.02039331, -0.03677621]], dtype=float32), Array([[ 0.00092068,  0.00069562, -0.10415091]], dtype=float32), Array([[ 0.21490431,  0.12683459, -0.00666906]], dtype=float32), Array([[0.00958985, 0.09741919, 0.01515585]], dtype=float32), Array([[ 0.02062083, -0.0109975 ,  0.04904447]], dtype=float32), Array([[-0.03985056, -0.0787711 ,  0.0687027 ]], dtype=float32), Array([[ 0.12817733, -0.12196526, -0.01232097]], dtype=float32), Array([[-0.03560606,  0.0291445 ,  0.20321377]], dtype=float32), Array([[ 0.00671753, -0.0677124 , -0.03250496]], dtype=float32), Array([[ 0.02572887, -0.11714865,  0.00052384]], dtype=float32), Array([[-0.00947286,  0.31616917, -0.02926648]], dtype=float32), Array([[0.03715947, 0.01255372, 0.10899659]], dtype=float32), Array([[ 0.1284851 ,  0.03310939, -0.02491975]], dtype=float32), Array([[ 0.121034  , -0.10128184,  0.0492544 ]], dtype=float32), Array([[-0.04119025, -0.01787777,  0.30772123]], dtype=float32), Array([[-0.1038342 , -0.04530296,  0.1863519 ]], dtype=float32), Array([[ 0.12351385, -0.21235496, -0.11110087]], dtype=float32), Array([[-0.05396959,  0.08890969,  0.0420165 ]], dtype=float32), Array([[0.04304853, 0.1696251 , 0.01775348]], dtype=float32), Array([[ 0.02497739, -0.06622618, -0.00684461]], dtype=float32), Array([[ 0.07572749, -0.03762409,  0.03108031]], dtype=float32), Array([[-0.05101964,  0.00623864,  0.05096243]], dtype=float32), Array([[-0.05515743, -0.01966479,  0.10025322]], dtype=float32), Array([[-0.15060498,  0.24651001, -0.06444138]], dtype=float32), Array([[-0.02760223,  0.11992335, -0.04809391]], dtype=float32), Array([[ 0.04505119,  0.01358895, -0.05382973]], dtype=float32), Array([[0.1017959 , 0.0403432 , 0.04697533]], dtype=float32), Array([[ 0.04405949, -0.08582729, -0.00458702]], dtype=float32), Array([[ 0.03990445, -0.09029472, -0.26518324]], dtype=float32), Array([[-0.08426714,  0.15675232, -0.03879967]], dtype=float32), Array([[-0.01725745, -0.04880711,  0.04473776]], dtype=float32), Array([[-0.0813351 ,  0.11286119,  0.19508508]], dtype=float32), Array([[0.00830629, 0.08213213, 0.09335089]], dtype=float32), Array([[ 0.02609625,  0.05202127, -0.08227859]], dtype=float32), Array([[-0.14801049, -0.21050344, -0.1418694 ]], dtype=float32), Array([[ 0.03456014, -0.15871373, -0.23576964]], dtype=float32), Array([[ 0.00338275, -0.02831963,  0.10227755]], dtype=float32), Array([[-0.3475949 ,  0.0735425 , -0.06015405]], dtype=float32), Array([[0.01702002, 0.16790518, 0.14622593]], dtype=float32), Array([[-0.02056309,  0.12493569,  0.02130339]], dtype=float32), Array([[ 0.07463205, -0.30009604,  0.0074566 ]], dtype=float32), Array([[ 0.11643176, -0.09953282,  0.1767863 ]], dtype=float32), Array([[ 0.04551714, -0.05943748, -0.09148288]], dtype=float32), Array([[ 0.18469273,  0.02274199, -0.0828107 ]], dtype=float32), Array([[-0.13749343,  0.16568816, -0.06771796]], dtype=float32), Array([[ 0.03291006, -0.0203646 ,  0.04436082]], dtype=float32), Array([[ 0.0865974 , -0.03802495,  0.0823456 ]], dtype=float32), Array([[0.04586652, 0.05707277, 0.0642022 ]], dtype=float32), Array([[ 0.02238762,  0.00482979, -0.04344963]], dtype=float32), Array([[ 0.00097115,  0.08142716, -0.00656343]], dtype=float32), Array([[-0.07572603, -0.14789568, -0.05740349]], dtype=float32), Array([[-0.09735929, -0.13852023, -0.10561725]], dtype=float32), Array([[-0.22450536,  0.1483047 , -0.23776603]], dtype=float32), Array([[ 0.10356048, -0.01739456,  0.03809866]], dtype=float32), Array([[ 0.07810763, -0.03101769, -0.00501797]], dtype=float32), Array([[0.04595491, 0.16038448, 0.00767609]], dtype=float32), Array([[-0.08269592, -0.05067468, -0.11522049]], dtype=float32), Array([[-0.12794171,  0.09069683,  0.02165955]], dtype=float32), Array([[ 0.05423543, -0.06142989,  0.09399858]], dtype=float32), Array([[ 0.01826869, -0.0144664 ,  0.07111888]], dtype=float32), Array([[-0.07284018, -0.03245418, -0.04386955]], dtype=float32), Array([[ 0.02072717, -0.01436684, -0.00522494]], dtype=float32), Array([[-0.09221971,  0.03371263, -0.03734195]], dtype=float32), Array([[-0.15090105, -0.03251796,  0.0346003 ]], dtype=float32), Array([[-0.14074364, -0.25742072, -0.18693794]], dtype=float32), Array([[ 0.01016617,  0.01850855, -0.09169474]], dtype=float32), Array([[ 0.16313377, -0.21307401,  0.04180445]], dtype=float32), Array([[-0.00133801, -0.01840961,  0.02714987]], dtype=float32), Array([[0.05393726, 0.07631358, 0.02882686]], dtype=float32), Array([[-0.03467181, -0.17360653, -0.04420696]], dtype=float32), Array([[-0.09944546,  0.09389751, -0.0669689 ]], dtype=float32), Array([[ 0.03697906, -0.06988932, -0.15178534]], dtype=float32), Array([[0.15528198, 0.11276061, 0.13659197]], dtype=float32), Array([[-0.02223961,  0.02802527,  0.05188252]], dtype=float32), Array([[ 0.00891203, -0.16277276, -0.02625149]], dtype=float32), Array([[-0.03635785,  0.01825717,  0.08216228]], dtype=float32), Array([[-0.14955613,  0.02740913, -0.18855077]], dtype=float32), Array([[-0.11110766,  0.1401605 , -0.28641257]], dtype=float32), Array([[-0.00999321, -0.03520682,  0.10760267]], dtype=float32), Array([[ 0.07178029, -0.02684319, -0.00606367]], dtype=float32), Array([[-0.09518778,  0.06481935,  0.12738101]], dtype=float32), Array([[0.00521362, 0.07096167, 0.11260512]], dtype=float32), Array([[0.13593517, 0.01122552, 0.08549784]], dtype=float32), Array([[0.07779601, 0.07735512, 0.16247503]], dtype=float32), Array([[-0.13891941,  0.04510057, -0.10347699]], dtype=float32), Array([[ 0.03408226, -0.01200506,  0.04017262]], dtype=float32), Array([[ 0.20573348, -0.12374596,  0.09835923]], dtype=float32), Array([[ 0.1351948 , -0.16602755, -0.04048534]], dtype=float32), Array([[-0.0474055 ,  0.09342358, -0.07722557]], dtype=float32), Array([[ 0.02924657, -0.08999124,  0.00115688]], dtype=float32), Array([[ 0.07653408, -0.04864401, -0.13316435]], dtype=float32), Array([[ 0.00868681,  0.14014342, -0.12680061]], dtype=float32), Array([[-0.11687756, -0.07436068,  0.04201351]], dtype=float32), Array([[ 0.00148168, -0.00483543,  0.01226082]], dtype=float32), Array([[0.08844317, 0.280451  , 0.0370246 ]], dtype=float32), Array([[-0.02905473, -0.13736007, -0.03251775]], dtype=float32), Array([[ 0.01764099, -0.06605414, -0.06607774]], dtype=float32), Array([[ 0.04238194, -0.07327203,  0.04889058]], dtype=float32), Array([[ 0.14569476, -0.02313652,  0.07430808]], dtype=float32), Array([[ 0.19181721, -0.1556749 , -0.12498286]], dtype=float32), Array([[0.00582529, 0.00168914, 0.03941664]], dtype=float32), Array([[-0.04394284, -0.14352965,  0.10890375]], dtype=float32), Array([[ 0.06949975, -0.1270684 , -0.07494694]], dtype=float32), Array([[-0.09889461,  0.12029921, -0.2397888 ]], dtype=float32), Array([[-0.09003469, -0.16104084, -0.05214646]], dtype=float32), Array([[-0.1319468 , -0.05185695,  0.04565448]], dtype=float32), Array([[ 0.05592385, -0.08764075, -0.04053706]], dtype=float32), Array([[-0.05898319,  0.03567886,  0.04587124]], dtype=float32), Array([[-0.01724051, -0.0322713 , -0.00232875]], dtype=float32), Array([[-0.1184154 , -0.29755014,  0.04148194]], dtype=float32), Array([[ 0.00938525, -0.01286891,  0.02962616]], dtype=float32), Array([[-0.03209797, -0.00979699, -0.02963465]], dtype=float32), Array([[ 0.06635007, -0.06059328, -0.00188471]], dtype=float32), Array([[-0.02631176,  0.04338031,  0.05105761]], dtype=float32), Array([[-0.01642865,  0.15639256,  0.1037977 ]], dtype=float32), Array([[ 0.12200551, -0.01909402, -0.16804239]], dtype=float32), Array([[-0.1833308 ,  0.13544908, -0.10892279]], dtype=float32), Array([[-0.08404604, -0.06002516, -0.01758649]], dtype=float32), Array([[ 0.01435792,  0.11059696, -0.06562804]], dtype=float32), Array([[ 0.00279933, -0.00679967,  0.05992065]], dtype=float32), Array([[ 0.05441746,  0.05061796, -0.12518393]], dtype=float32), Array([[-0.225986  , -0.03952889, -0.02667025]], dtype=float32), Array([[ 0.10659334, -0.24340528,  0.11892314]], dtype=float32), Array([[0.01101699, 0.10255837, 0.04692665]], dtype=float32), Array([[-0.09743357,  0.17059585, -0.08166677]], dtype=float32), Array([[ 0.05940041, -0.01412722, -0.01384386]], dtype=float32), Array([[ 0.01170683, -0.07202919,  0.10191658]], dtype=float32), Array([[-0.00136108, -0.03509024, -0.00044121]], dtype=float32), Array([[ 0.04506134,  0.07862771, -0.1306486 ]], dtype=float32), Array([[ 0.0295735 , -0.08609146,  0.11256757]], dtype=float32), Array([[ 0.01453592,  0.00551265, -0.04981843]], dtype=float32), Array([[ 0.10543212, -0.05545708, -0.20784254]], dtype=float32), Array([[ 0.07869555, -0.01731116, -0.07188728]], dtype=float32), Array([[-0.07196   ,  0.0518292 , -0.12178543]], dtype=float32), Array([[0.10433067, 0.01237479, 0.00075601]], dtype=float32), Array([[-0.09399487,  0.10109952, -0.03122723]], dtype=float32), Array([[-0.32188794,  0.03865835, -0.0900328 ]], dtype=float32), Array([[ 0.19366866,  0.14713207, -0.13975893]], dtype=float32), Array([[-0.30328575, -0.11817841, -0.05229755]], dtype=float32), Array([[ 0.13081615, -0.0972867 , -0.27970544]], dtype=float32), Array([[ 0.02709317, -0.02550246, -0.22574638]], dtype=float32), Array([[0.17185922, 0.0944296 , 0.23880543]], dtype=float32), Array([[ 0.08570853,  0.06621311, -0.10716024]], dtype=float32), Array([[-0.12708512,  0.16039926, -0.01698366]], dtype=float32), Array([[0.13819209, 0.16667953, 0.10752349]], dtype=float32), Array([[-0.13682133, -0.01895204, -0.03613099]], dtype=float32), Array([[-0.03524131, -0.08697316,  0.20166951]], dtype=float32), Array([[ 0.02531877, -0.08169758, -0.04043023]], dtype=float32), Array([[-0.04281756,  0.0619255 , -0.02623662]], dtype=float32), Array([[-0.00798293,  0.05296293, -0.00893961]], dtype=float32), Array([[-0.0359522 ,  0.02634737,  0.10889105]], dtype=float32), Array([[-0.08309403,  0.08816716,  0.22864404]], dtype=float32), Array([[-0.04124401,  0.12118391,  0.11146414]], dtype=float32), Array([[-0.31266135,  0.07146445, -0.03414828]], dtype=float32), Array([[-0.09208126,  0.28457236, -0.04187858]], dtype=float32), Array([[-0.06072996,  0.16693112,  0.08150041]], dtype=float32), Array([[-0.08933703,  0.06450278, -0.06171051]], dtype=float32), Array([[-0.12717038, -0.12308043, -0.1455541 ]], dtype=float32), Array([[-0.01701788, -0.0990791 , -0.06808656]], dtype=float32), Array([[-0.29013664,  0.18554711,  0.08973562]], dtype=float32), Array([[ 0.02827982, -0.01980734, -0.03446288]], dtype=float32), Array([[-0.10657156,  0.02067015, -0.07826539]], dtype=float32), Array([[ 0.06309621, -0.00470096,  0.04522154]], dtype=float32), Array([[-0.01459685, -0.03014004,  0.02873553]], dtype=float32), Array([[ 0.06804252,  0.08087823, -0.06600407]], dtype=float32), Array([[0.02569642, 0.11539336, 0.08277658]], dtype=float32), Array([[ 0.2172214 , -0.04596283, -0.16969809]], dtype=float32), Array([[ 0.01679674, -0.11476916,  0.11676719]], dtype=float32), Array([[0.25278485, 0.27927133, 0.01075578]], dtype=float32), Array([[-0.08104384,  0.20641015, -0.12404692]], dtype=float32), Array([[ 0.08708811,  0.02356941, -0.20231853]], dtype=float32), Array([[-0.10305504,  0.09092662,  0.09574538]], dtype=float32), Array([[-0.01137756, -0.06836294, -0.03391585]], dtype=float32), Array([[-0.03666264, -0.03605849,  0.13122341]], dtype=float32), Array([[-0.1075722 , -0.01046132, -0.24252667]], dtype=float32), Array([[-0.000835  ,  0.14868408,  0.0890259 ]], dtype=float32), Array([[-0.00182185,  0.09785447,  0.03305894]], dtype=float32), Array([[-0.16181314, -0.04100233, -0.01885679]], dtype=float32), Array([[0.00038966, 0.04588091, 0.25212052]], dtype=float32), Array([[-0.0288642 , -0.148106  ,  0.15090543]], dtype=float32), Array([[-0.30426753,  0.29529926,  0.01275978]], dtype=float32), Array([[-0.09449241,  0.17980991, -0.03939921]], dtype=float32), Array([[-0.01053786,  0.00102696,  0.13663213]], dtype=float32), Array([[-0.10633311, -0.03171174, -0.05122841]], dtype=float32), Array([[-0.0578309 , -0.10739282, -0.07307363]], dtype=float32), Array([[ 0.08388343, -0.07216948,  0.00999713]], dtype=float32), Array([[-0.14248306,  0.01722254,  0.31852365]], dtype=float32), Array([[ 0.05058201,  0.09643903, -0.03070814]], dtype=float32), Array([[0.04808646, 0.06175528, 0.01897322]], dtype=float32), Array([[-0.0735547 , -0.10784259,  0.05478363]], dtype=float32), Array([[ 0.31582353, -0.20747724,  0.15137853]], dtype=float32), Array([[-0.11599496, -0.08358745,  0.26560712]], dtype=float32), Array([[ 0.09844267,  0.12711677, -0.09303376]], dtype=float32), Array([[-0.10299331, -0.04491878, -0.01677537]], dtype=float32), Array([[ 0.03547955, -0.04023831,  0.01767582]], dtype=float32), Array([[-0.01644422, -0.00309059, -0.1017215 ]], dtype=float32), Array([[-0.08120424, -0.15536143,  0.08791646]], dtype=float32), Array([[-0.16638616, -0.13538568,  0.140015  ]], dtype=float32), Array([[-0.17555185,  0.03446548,  0.13309208]], dtype=float32), Array([[-0.06168506,  0.04575381,  0.09760877]], dtype=float32), Array([[ 0.12904659, -0.07815708, -0.12773171]], dtype=float32), Array([[ 0.0142905 , -0.06675766, -0.02245823]], dtype=float32), Array([[ 0.13244924,  0.01296055, -0.10025924]], dtype=float32), Array([[ 0.05692872, -0.04792377,  0.04773039]], dtype=float32), Array([[-0.025085  , -0.07190508,  0.03047742]], dtype=float32), Array([[ 0.07665816,  0.09119596, -0.10028659]], dtype=float32), Array([[ 0.08967755, -0.25311255,  0.16869023]], dtype=float32), Array([[ 0.00031029, -0.03539464, -0.04043224]], dtype=float32), Array([[-0.02585685,  0.01103728,  0.10494936]], dtype=float32), Array([[ 0.23705135,  0.05808801, -0.18592161]], dtype=float32), Array([[-0.12071958,  0.06869556,  0.05042726]], dtype=float32), Array([[ 0.18343529, -0.03364277,  0.0378964 ]], dtype=float32), Array([[-0.05183704, -0.10951898, -0.18326944]], dtype=float32), Array([[ 0.04335871,  0.2012395 , -0.08779612]], dtype=float32), Array([[-0.03675959, -0.04934214, -0.07355371]], dtype=float32), Array([[-0.00671986, -0.03872713,  0.09911007]], dtype=float32), Array([[-0.10975172,  0.07204831, -0.00131358]], dtype=float32), Array([[-0.00711823, -0.03779504, -0.01732552]], dtype=float32), Array([[0.02080496, 0.00791115, 0.03602874]], dtype=float32), Array([[0.10097289, 0.08423981, 0.01588237]], dtype=float32), Array([[ 0.11403155, -0.00961803,  0.04907897]], dtype=float32), Array([[-0.10982864,  0.16133572,  0.06726518]], dtype=float32), Array([[-0.11337247,  0.03599715,  0.14410438]], dtype=float32), Array([[ 0.01959258,  0.01265353, -0.12984368]], dtype=float32), Array([[ 0.01527923, -0.10027027, -0.02500374]], dtype=float32), Array([[-0.01672643,  0.11448369, -0.13644008]], dtype=float32), Array([[ 0.11655903,  0.22727008, -0.01821427]], dtype=float32), Array([[ 0.05619706,  0.02415377, -0.06913508]], dtype=float32), Array([[-0.18481642,  0.10962391,  0.0020739 ]], dtype=float32), Array([[0.0136743 , 0.00948688, 0.02135989]], dtype=float32), Array([[ 0.1568097 ,  0.03439178, -0.04738592]], dtype=float32), Array([[ 0.01438755,  0.07626557, -0.08226348]], dtype=float32), Array([[-0.02105246, -0.02540278, -0.07402486]], dtype=float32), Array([[0.05173851, 0.00682598, 0.07090489]], dtype=float32), Array([[-0.01038396, -0.04462412, -0.12865564]], dtype=float32), Array([[-0.12650338,  0.14828144,  0.20560306]], dtype=float32), Array([[-0.06607854,  0.05925027, -0.01281533]], dtype=float32), Array([[-0.04076591,  0.07932829, -0.02191944]], dtype=float32), Array([[-0.02199617, -0.0726933 , -0.04305559]], dtype=float32), Array([[ 0.06115833,  0.13462296, -0.02321307]], dtype=float32), Array([[0.14071496, 0.10913911, 0.1770954 ]], dtype=float32), Array([[0.01891722, 0.0449245 , 0.02337697]], dtype=float32), Array([[-0.06002098, -0.04677263,  0.02336077]], dtype=float32), Array([[0.02459342, 0.06563923, 0.01869205]], dtype=float32), Array([[-0.0146502 , -0.01358247,  0.02343115]], dtype=float32), Array([[-0.08687505, -0.04381558, -0.07136101]], dtype=float32), Array([[ 0.15571064, -0.06674525,  0.01373625]], dtype=float32), Array([[ 0.00860348,  0.15069555, -0.06554006]], dtype=float32), Array([[-0.05211952, -0.09302529, -0.08571658]], dtype=float32), Array([[0.05229539, 0.08813868, 0.21275637]], dtype=float32), Array([[-0.03320673,  0.02654792, -0.14922252]], dtype=float32), Array([[-0.02335213,  0.0318437 , -0.1837134 ]], dtype=float32), Array([[0.11295488, 0.07820372, 0.06527194]], dtype=float32), Array([[-0.03806433,  0.08851136, -0.10120638]], dtype=float32), Array([[-0.11348325,  0.02949008,  0.03527712]], dtype=float32), Array([[-0.15368244,  0.02368478, -0.2014373 ]], dtype=float32), Array([[0.0073019 , 0.07066307, 0.01946861]], dtype=float32), Array([[ 0.05781674, -0.03233879,  0.02602884]], dtype=float32), Array([[-0.07985085,  0.02724328, -0.01779757]], dtype=float32), Array([[ 0.09227857, -0.05083841, -0.01340474]], dtype=float32), Array([[ 0.08536184, -0.05101551,  0.00450461]], dtype=float32), Array([[-0.01094407, -0.09767161, -0.08196306]], dtype=float32), Array([[ 0.00384971,  0.01966999, -0.00252344]], dtype=float32), Array([[-0.014441  , -0.05854121,  0.04516393]], dtype=float32), Array([[-0.05743683,  0.02990597, -0.00147855]], dtype=float32), Array([[-0.11382209, -0.11702444,  0.23783264]], dtype=float32), Array([[-0.00834012, -0.04513668,  0.1364251 ]], dtype=float32), Array([[ 0.04572982,  0.02845433, -0.01835318]], dtype=float32), Array([[-0.1599641 , -0.11381334,  0.03771171]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(valid_data[\"atomic_numbers\"]))\n",
    "\n",
    "predict = []\n",
    "for i in range(len(valid_data[\"positions\"])) : \n",
    "    dm=model.apply(params,\n",
    "        atomic_numbers=valid_data[\"atomic_numbers\"],\n",
    "        positions=valid_data[\"positions\"][i],\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx)\n",
    "    predict.append(dm)\n",
    "\n",
    "\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.029067723338681\n"
     ]
    }
   ],
   "source": [
    "from math import acos, degrees\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "def angle_between(a, b):\n",
    "\n",
    "    theta_degrees = degrees(acos((np.dot(a, b)) / (la.norm(a) * la.norm(b))))\n",
    "    return theta_degrees\n",
    "\n",
    "\n",
    "total = 0\n",
    "list_angle = []\n",
    "to = 0\n",
    "for i in range(len((predict))):\n",
    "    v = predict[i][0]\n",
    "    u = valid_data['dipole_moment'][i]\n",
    "    angle_degrees = angle_between(v, u)\n",
    "    list_angle.append(angle_degrees)\n",
    "    if angle_degrees < 80 : \n",
    "        to += 1\n",
    "        total += angle_degrees**2\n",
    "\n",
    "print(np.sqrt(total / to))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std predict : 0.102347165\n",
      "std target  : 0.10590497\n",
      "mean_absolute_error : 0.012135896\n",
      "mean squared error 0.0002578492\n",
      "RMSE  0.016042473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "print(\"std predict :\", np.std(predict))\n",
    "print(\"std target  :\", np.std(valid_data['dipole_moment']))\n",
    "print(\"mean_absolute_error :\", mean_absolute_error(valid_data['dipole_moment'], predict_v2))\n",
    "print(\"mean squared error\", jnp.mean((predict_v2 - valid_data['dipole_moment']) ** 2))\n",
    "print(\"RMSE \", root_mean_squared_error(valid_data['dipole_moment'], predict_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "treshold = 12\n",
    "\n",
    "# Filtrar la lista, manteniendo solo los valores mayores o iguales al umbral\n",
    "nueva_lista = [x for x in list_angle if x <= 70]\n",
    "plt.hist(\n",
    "    nueva_lista,\n",
    "    bins=80,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(\"Ángulo (°)\")\n",
    "plt.ylabel(\"Densidad de Frecuencia\")\n",
    "plt.title(\"Histograma de la Distribución de Ángulos\")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12198372 0.14458028 0.18386792 0.14844882 0.09142367 0.12233304\n",
      " 0.09875564 0.12966756 0.13533246 0.21573975 0.15001073 0.17725076\n",
      " 0.08104706 0.13220461 0.11989837 0.11031209 0.10903163 0.09302386\n",
      " 0.12602566 0.19377992 0.10574005 0.16737617 0.06404397 0.13509074\n",
      " 0.06951923 0.20472987 0.1108366  0.09546688 0.07608118 0.16740067\n",
      " 0.07786979 0.16115229 0.0788049  0.08201295 0.08061319 0.11326271\n",
      " 0.03765873 0.04177029 0.08651437 0.09281715 0.05288824 0.1129159\n",
      " 0.15441698 0.18514698 0.10999778 0.16190019 0.08857362 0.1410594\n",
      " 0.16841183 0.04725379 0.22845671 0.13131399 0.14379985 0.29672028\n",
      " 0.28672251 0.05590574 0.14593797 0.11314618 0.13398152 0.12497375\n",
      " 0.06453923 0.10443532 0.09424424 0.07046972 0.1398488  0.05827671\n",
      " 0.06667348 0.11812054 0.13097856 0.09760302 0.12861    0.09348492\n",
      " 0.263731   0.09060649 0.09392728 0.10701094 0.1448395  0.08753855\n",
      " 0.10119845 0.09315706 0.07972013 0.19355721 0.19742662 0.15613111\n",
      " 0.07622879 0.15824452 0.10251764 0.1029212  0.07513305 0.06606185\n",
      " 0.15748003 0.05546716 0.14907426 0.20388857 0.1395984  0.11212448\n",
      " 0.16712609 0.07179446 0.15119461 0.11661518 0.10651667 0.07100992\n",
      " 0.10201809 0.145002   0.08057523 0.16486464 0.11425492 0.08864415\n",
      " 0.17585011 0.07745221 0.07934987 0.14884536 0.18031449 0.07995441\n",
      " 0.14387813 0.19097117 0.1798724  0.07912772 0.08298721 0.19177065\n",
      " 0.09309861 0.04999967 0.09959123 0.11486351 0.10962608 0.08378827\n",
      " 0.11807487 0.205068   0.09274561 0.09944437 0.10488409 0.15604845\n",
      " 0.04592826 0.09730435 0.2082482  0.08419075 0.16593805 0.08778653\n",
      " 0.12794929 0.27065806 0.20871155 0.07923404 0.18259116 0.15199783\n",
      " 0.00705901 0.0858606  0.09218624 0.1485932  0.2769761  0.08456496\n",
      " 0.15751133 0.11111173 0.13638563 0.1421069  0.13255038 0.13681277\n",
      " 0.25419309 0.09218941 0.18952296 0.09149025 0.20595287 0.10251072\n",
      " 0.20000622 0.05657575 0.12839423 0.13311671 0.08692763 0.0870484\n",
      " 0.12796146 0.17171114 0.09812857 0.10601595 0.11507825 0.09624547\n",
      " 0.07897563 0.33127706 0.14607128 0.11856543 0.1056864  0.13444693\n",
      " 0.07689564 0.09091552 0.07894157 0.15669297 0.05397471 0.07008334\n",
      " 0.06530159 0.14315031 0.18127802 0.13230441 0.19253622 0.17839309\n",
      " 0.12745042 0.07929513 0.10495295 0.26706505 0.04456131 0.24371578\n",
      " 0.14875306 0.10507046 0.17619489 0.07386885 0.1266659  0.17858734\n",
      " 0.10488355 0.1502272  0.10113424 0.05961916 0.08757278 0.12298041\n",
      " 0.10821365 0.07902349 0.05693232 0.06603383 0.04401829 0.11626156\n",
      " 0.1281546  0.10936239 0.05209861 0.13570508 0.12115972 0.11405393\n",
      " 0.14901158 0.17262834 0.09270899 0.14775577 0.11161528 0.11405393\n",
      " 0.20216519 0.09162297 0.11700329 0.12694139 0.07269512 0.09507106\n",
      " 0.1253386  0.09104994 0.19693348 0.09868591 0.14417543 0.06495097\n",
      " 0.13559312 0.21773952 0.16475193 0.25018692 0.11644822 0.06934488\n",
      " 0.08305341 0.14600967 0.14445831 0.11279353 0.11093597 0.10694089\n",
      " 0.05580242 0.15959339 0.08748758 0.16956017 0.07238607 0.22290944\n",
      " 0.10146654 0.09964967 0.19361466 0.07568185 0.23552243 0.07891252\n",
      " 0.14224646 0.09950558 0.09269652 0.11660773 0.1565878  0.08717148\n",
      " 0.09853301 0.04975956 0.12625159 0.14093211 0.03878146 0.13372669\n",
      " 0.10849453 0.11611968 0.15631776 0.12157917 0.0880042  0.10419502\n",
      " 0.13258084 0.0781705  0.13270909 0.14135819 0.11830984 0.09599085\n",
      " 0.04784846 0.0633839  0.06888035 0.14219168 0.18464042 0.25208039\n",
      " 0.11125237 0.12043053 0.12004061 0.07249387 0.04920102 0.06089586\n",
      " 0.11108227 0.13175393 0.18017487 0.15329906 0.14100946 0.14691631\n",
      " 0.10298326 0.16899712 0.08429358 0.11148768 0.09518867 0.08191145\n",
      " 0.1116479  0.19176816 0.06157182 0.12487741 0.10626383 0.21103102\n",
      " 0.17682885 0.13613911 0.09245776 0.09089237 0.16511629 0.04004835\n",
      " 0.19049577 0.12417625 0.15584519 0.09966661 0.11767783 0.05125754\n",
      " 0.11177718 0.12519558 0.21059313 0.17307312 0.12174423 0.08793213\n",
      " 0.1115634  0.11468673 0.09842113 0.12077542 0.09589308 0.11731372\n",
      " 0.11461042 0.13258804 0.12598556 0.14082386 0.10893081 0.22708124\n",
      " 0.13341095 0.17486136 0.10986305 0.10385169 0.15306611 0.10651307\n",
      " 0.18850876 0.1355074  0.06685498 0.04448394 0.11110357 0.22568994\n",
      " 0.18741275 0.22805805 0.1368497  0.12420613 0.1689698  0.09911297\n",
      " 0.13383456 0.19054094 0.10386389 0.05992153 0.1311477  0.10553818\n",
      " 0.11480358 0.13969853 0.10419612 0.071776   0.1055947  0.11676308\n",
      " 0.08983607 0.13606183 0.1658149  0.10700283 0.17862941 0.11425325\n",
      " 0.05554702 0.25191759 0.17698569 0.1572293  0.13421225 0.08539272\n",
      " 0.04919581 0.20334134 0.11325116 0.04714161 0.17935291 0.11848964\n",
      " 0.14062701]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(np.array(nueva_lista) /len(nueva_lista)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
