{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        print((\"atomic_numbers\", atomic_numbers))\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        # print('Embed',x.shape)\n",
    "        # print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                # print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                print(x.shape, basis.shape, \"intermediate iterations,\")\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                # print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            # print('Residual',x.shape)\n",
    "\n",
    "            # 5. Predict atomic energies with an ordinary dense layer.\n",
    "            # element_bias = self.param(\n",
    "            #    \"element_bias\",\n",
    "            #    lambda rng, shape: jnp.zeros(shape),\n",
    "            #    (self.max_atomic_number + 1),\n",
    "            # )\n",
    "\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(\n",
    "            x\n",
    "        )  # (..., Natoms, 1, 9, 1)\n",
    "        print(\"After dense:\", x.shape)\n",
    "        element_bias = self.param(\n",
    "            \"element_bias\",\n",
    "            lambda rng, shape: jnp.zeros(shape),\n",
    "            (self.max_atomic_number + 1),\n",
    "        )\n",
    "        print('element_bias',element_bias[atomic_numbers].shape)\n",
    "        bias= element_bias[atomic_numbers]\n",
    "        x += bias[:,None,None,None]\n",
    "        print(x.shape, ' after bias ')\n",
    "        x = jax.ops.segment_sum(\n",
    "            x, segment_ids=batch_segments, num_segments=batch_size\n",
    "        )\n",
    "        print(\"After segment_sum:\", x.shape)\n",
    "        #x = jnp.sum(x, axis=1)\n",
    "        x=jnp.squeeze(x, axis=0)\n",
    "        print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        # x = x[..., :3]\n",
    "        # x = jnp.squeeze(x)\n",
    "        print('After slicing:' ,x.shape)\n",
    "        # x = jnp.sum(x, axis=1)\n",
    "\n",
    "        # x = x[:, 1:4]\n",
    "\n",
    "        print(\"Forma final:\", x.shape)\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "            print(\"pase\", batch_segments, atomic_numbers)\n",
    "            # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "            # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        print(batch_segments.shape, \"batch\", batch_size)\n",
    "        dipole = self.dipole_moment(\n",
    "            atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "        )\n",
    "        #print(dipole)\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(dipole_prediction, dipole_target):\n",
    "    return jnp.mean(optax.l2_loss(dipole_prediction, dipole_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(\n",
    "    jax.jit, static_argnames=(\"model_apply\", \"optimizer_update\", \"batch_size\")\n",
    ")\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "    def loss_fn(params):\n",
    "        dipole = model_apply(\n",
    "            params,\n",
    "            atomic_numbers=batch[\"atomic_numbers\"],\n",
    "            positions=batch[\"positions\"],\n",
    "            dst_idx=batch[\"dst_idx\"],\n",
    "            src_idx=batch[\"src_idx\"],\n",
    "            batch_segments=batch[\"batch_segments\"],\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        loss = mean_squared_loss(\n",
    "            dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=(\"model_apply\", \"batch_size\"))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "    dipole = model_apply(\n",
    "        params,\n",
    "        atomic_numbers=batch[\"atomic_numbers\"],\n",
    "        positions=batch[\"positions\"],\n",
    "        dst_idx=batch[\"dst_idx\"],\n",
    "        src_idx=batch[\"src_idx\"],\n",
    "        batch_segments=batch[\"batch_segments\"],\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print('dipole_prediction',dipole[0])\n",
    "    print('dipole_target',batch[\"dipole_moment\"][0])\n",
    "    loss = mean_squared_loss(\n",
    "        dipole_prediction=dipole, dipole_target=batch[\"dipole_moment\"]\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    key, model, train_data, valid_data, num_epochs, learning_rate, batch_size\n",
    "):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(\n",
    "        len(train_data[\"atomic_numbers\"])\n",
    "    )\n",
    "    params = model.init(\n",
    "        init_key,\n",
    "        atomic_numbers=train_data[\"atomic_numbers\"],\n",
    "        positions=train_data[\"positions\"][0],\n",
    "        dst_idx=dst_idx,\n",
    "        src_idx=src_idx,\n",
    "    )\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    # Batches for the validation set need to be prepared only once.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "    # Train for 'num_epochs' epochs.\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Prepare batches.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "        # Loop over train batches.\n",
    "        train_loss = 0.0\n",
    "        for i, batch in enumerate(train_batches):\n",
    "            \n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                opt_state=opt_state,\n",
    "                params=params,\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "\n",
    "        # Evaluate on validation set.\n",
    "        valid_loss = 0.0\n",
    "        for i, batch in enumerate(valid_batches):\n",
    "            loss = eval_step(\n",
    "                model_apply=model.apply,\n",
    "                batch=batch,\n",
    "                batch_size=batch_size,\n",
    "                params=params,\n",
    "            )\n",
    "            valid_loss += (loss - valid_loss) / (i + 1)\n",
    "\n",
    "        # Print progress.\n",
    "        print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "        print(f\"    loss [a.u.]             {train_loss : 8.6f} {valid_loss : 8.3f}\")\n",
    "\n",
    "    # Return final model parameters.\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z = jnp.full(16, 14)\n",
    "    Z = jnp.append(Z, 23)\n",
    "    Z = jnp.expand_dims(Z, axis=0)\n",
    "    Z = jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        # energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        # forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        #atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment=jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        atomic_numbers=jnp.asarray(dataset[\"z\"]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_data.npz\"\n",
    "dataset = np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print(\"Dipole moment shape array\", dataset[\"D\"].shape)\n",
    "print(\"Dipole moment units\", dataset[\"D_units\"])\n",
    "\n",
    "print(\"Atomic numbers\", dataset[\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train = 100\n",
    "num_val = 20\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters.\n",
    "features = 32\n",
    "max_degree = 2\n",
    "num_iterations = 3\n",
    "num_basis_functions = 16\n",
    "cutoff = 6.0\n",
    "max_atomic_number = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n",
      "(17,) batch 1\n",
      "('atomic_numbers', Array([23, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],      dtype=int32))\n",
      "(17, 1, 1, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "(17, 1, 9, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "After dense: (17, 1, 9, 1)\n",
      "element_bias (17,)\n",
      "(17, 1, 9, 1)  after bias \n",
      "After segment_sum: (1, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Forma final: (1, 3)\n",
      "(17,) batch 1\n",
      "('atomic_numbers', Traced<ShapedArray(int32[17])>with<DynamicJaxprTrace(level=1/0)>)\n",
      "(17, 1, 1, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "(17, 1, 9, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "After dense: (17, 1, 9, 1)\n",
      "element_bias (17,)\n",
      "(17, 1, 9, 1)  after bias \n",
      "After segment_sum: (1, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Forma final: (1, 3)\n",
      "(17,) batch 1\n",
      "('atomic_numbers', Traced<ShapedArray(int32[17])>with<DynamicJaxprTrace(level=1/0)>)\n",
      "(17, 1, 1, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "(17, 1, 9, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "After dense: (17, 1, 9, 1)\n",
      "element_bias (17,)\n",
      "(17, 1, 9, 1)  after bias \n",
      "After segment_sum: (1, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Forma final: (1, 3)\n",
      "dipole_prediction Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "dipole_target Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "epoch:   1                    train:   valid:\n",
      "    loss [a.u.]              0.526103    0.431\n",
      "epoch:   2                    train:   valid:\n",
      "    loss [a.u.]              0.429020    0.431\n",
      "epoch:   3                    train:   valid:\n",
      "    loss [a.u.]              0.428513    0.431\n",
      "epoch:   4                    train:   valid:\n",
      "    loss [a.u.]              0.428358    0.431\n",
      "epoch:   5                    train:   valid:\n",
      "    loss [a.u.]              0.427072    0.431\n",
      "epoch:   6                    train:   valid:\n",
      "    loss [a.u.]              0.426887    0.431\n",
      "epoch:   7                    train:   valid:\n",
      "    loss [a.u.]              0.426815    0.433\n",
      "epoch:   8                    train:   valid:\n",
      "    loss [a.u.]              0.425698    0.433\n",
      "epoch:   9                    train:   valid:\n",
      "    loss [a.u.]              0.425783    0.432\n",
      "epoch:  10                    train:   valid:\n",
      "    loss [a.u.]              0.425413    0.433\n",
      "epoch:  11                    train:   valid:\n",
      "    loss [a.u.]              0.425303    0.432\n",
      "epoch:  12                    train:   valid:\n",
      "    loss [a.u.]              0.425132    0.434\n",
      "epoch:  13                    train:   valid:\n",
      "    loss [a.u.]              0.425502    0.433\n",
      "epoch:  14                    train:   valid:\n",
      "    loss [a.u.]              0.425082    0.433\n",
      "epoch:  15                    train:   valid:\n",
      "    loss [a.u.]              0.424878    0.436\n",
      "epoch:  16                    train:   valid:\n",
      "    loss [a.u.]              0.424059    0.437\n",
      "epoch:  17                    train:   valid:\n",
      "    loss [a.u.]              0.424671    0.434\n",
      "epoch:  18                    train:   valid:\n",
      "    loss [a.u.]              0.422807    0.436\n",
      "epoch:  19                    train:   valid:\n",
      "    loss [a.u.]              0.421404    0.443\n",
      "epoch:  20                    train:   valid:\n",
      "    loss [a.u.]              0.422476    0.440\n",
      "epoch:  21                    train:   valid:\n",
      "    loss [a.u.]              0.421006    0.443\n",
      "epoch:  22                    train:   valid:\n",
      "    loss [a.u.]              0.419840    0.444\n",
      "epoch:  23                    train:   valid:\n",
      "    loss [a.u.]              0.419858    0.447\n",
      "epoch:  24                    train:   valid:\n",
      "    loss [a.u.]              0.419445    0.450\n",
      "epoch:  25                    train:   valid:\n",
      "    loss [a.u.]              0.419808    0.446\n",
      "epoch:  26                    train:   valid:\n",
      "    loss [a.u.]              0.418377    0.451\n",
      "epoch:  27                    train:   valid:\n",
      "    loss [a.u.]              0.421629    0.449\n",
      "epoch:  28                    train:   valid:\n",
      "    loss [a.u.]              0.419315    0.443\n",
      "epoch:  29                    train:   valid:\n",
      "    loss [a.u.]              0.419741    0.446\n",
      "epoch:  30                    train:   valid:\n",
      "    loss [a.u.]              0.418413    0.451\n",
      "epoch:  31                    train:   valid:\n",
      "    loss [a.u.]              0.419351    0.451\n",
      "epoch:  32                    train:   valid:\n",
      "    loss [a.u.]              0.418467    0.456\n",
      "epoch:  33                    train:   valid:\n",
      "    loss [a.u.]              0.417779    0.457\n",
      "epoch:  34                    train:   valid:\n",
      "    loss [a.u.]              0.419061    0.445\n",
      "epoch:  35                    train:   valid:\n",
      "    loss [a.u.]              0.418498    0.454\n",
      "epoch:  36                    train:   valid:\n",
      "    loss [a.u.]              0.418929    0.447\n",
      "epoch:  37                    train:   valid:\n",
      "    loss [a.u.]              0.418513    0.452\n",
      "epoch:  38                    train:   valid:\n",
      "    loss [a.u.]              0.419422    0.447\n",
      "epoch:  39                    train:   valid:\n",
      "    loss [a.u.]              0.417845    0.452\n",
      "epoch:  40                    train:   valid:\n",
      "    loss [a.u.]              0.418669    0.452\n",
      "epoch:  41                    train:   valid:\n",
      "    loss [a.u.]              0.419810    0.448\n",
      "epoch:  42                    train:   valid:\n",
      "    loss [a.u.]              0.416988    0.454\n",
      "epoch:  43                    train:   valid:\n",
      "    loss [a.u.]              0.417580    0.452\n",
      "epoch:  44                    train:   valid:\n",
      "    loss [a.u.]              0.418209    0.456\n",
      "epoch:  45                    train:   valid:\n",
      "    loss [a.u.]              0.418886    0.451\n",
      "epoch:  46                    train:   valid:\n",
      "    loss [a.u.]              0.417388    0.449\n",
      "epoch:  47                    train:   valid:\n",
      "    loss [a.u.]              0.417948    0.452\n",
      "epoch:  48                    train:   valid:\n",
      "    loss [a.u.]              0.417535    0.454\n",
      "epoch:  49                    train:   valid:\n",
      "    loss [a.u.]              0.416352    0.460\n",
      "epoch:  50                    train:   valid:\n",
      "    loss [a.u.]              0.417013    0.459\n",
      "epoch:  51                    train:   valid:\n",
      "    loss [a.u.]              0.416513    0.457\n",
      "epoch:  52                    train:   valid:\n",
      "    loss [a.u.]              0.415835    0.461\n",
      "epoch:  53                    train:   valid:\n",
      "    loss [a.u.]              0.418141    0.454\n",
      "epoch:  54                    train:   valid:\n",
      "    loss [a.u.]              0.416116    0.453\n",
      "epoch:  55                    train:   valid:\n",
      "    loss [a.u.]              0.415695    0.463\n",
      "epoch:  56                    train:   valid:\n",
      "    loss [a.u.]              0.416045    0.455\n",
      "epoch:  57                    train:   valid:\n",
      "    loss [a.u.]              0.413995    0.466\n",
      "epoch:  58                    train:   valid:\n",
      "    loss [a.u.]              0.416941    0.455\n",
      "epoch:  59                    train:   valid:\n",
      "    loss [a.u.]              0.413577    0.458\n",
      "epoch:  60                    train:   valid:\n",
      "    loss [a.u.]              0.414685    0.466\n",
      "epoch:  61                    train:   valid:\n",
      "    loss [a.u.]              0.417659    0.460\n",
      "epoch:  62                    train:   valid:\n",
      "    loss [a.u.]              0.413556    0.471\n",
      "epoch:  63                    train:   valid:\n",
      "    loss [a.u.]              0.414734    0.464\n",
      "epoch:  64                    train:   valid:\n",
      "    loss [a.u.]              0.413616    0.461\n",
      "epoch:  65                    train:   valid:\n",
      "    loss [a.u.]              0.414285    0.459\n",
      "epoch:  66                    train:   valid:\n",
      "    loss [a.u.]              0.414499    0.458\n",
      "epoch:  67                    train:   valid:\n",
      "    loss [a.u.]              0.415410    0.463\n",
      "epoch:  68                    train:   valid:\n",
      "    loss [a.u.]              0.412844    0.460\n",
      "epoch:  69                    train:   valid:\n",
      "    loss [a.u.]              0.414119    0.461\n",
      "epoch:  70                    train:   valid:\n",
      "    loss [a.u.]              0.413926    0.467\n",
      "epoch:  71                    train:   valid:\n",
      "    loss [a.u.]              0.413108    0.463\n",
      "epoch:  72                    train:   valid:\n",
      "    loss [a.u.]              0.412463    0.466\n",
      "epoch:  73                    train:   valid:\n",
      "    loss [a.u.]              0.414072    0.465\n",
      "epoch:  74                    train:   valid:\n",
      "    loss [a.u.]              0.413518    0.454\n",
      "epoch:  75                    train:   valid:\n",
      "    loss [a.u.]              0.412518    0.456\n",
      "epoch:  76                    train:   valid:\n",
      "    loss [a.u.]              0.411991    0.466\n",
      "epoch:  77                    train:   valid:\n",
      "    loss [a.u.]              0.413167    0.464\n",
      "epoch:  78                    train:   valid:\n",
      "    loss [a.u.]              0.412994    0.464\n",
      "epoch:  79                    train:   valid:\n",
      "    loss [a.u.]              0.414573    0.459\n",
      "epoch:  80                    train:   valid:\n",
      "    loss [a.u.]              0.411517    0.462\n",
      "epoch:  81                    train:   valid:\n",
      "    loss [a.u.]              0.413040    0.467\n",
      "epoch:  82                    train:   valid:\n",
      "    loss [a.u.]              0.414201    0.472\n",
      "epoch:  83                    train:   valid:\n",
      "    loss [a.u.]              0.413635    0.460\n",
      "epoch:  84                    train:   valid:\n",
      "    loss [a.u.]              0.411519    0.461\n",
      "epoch:  85                    train:   valid:\n",
      "    loss [a.u.]              0.412357    0.460\n",
      "epoch:  86                    train:   valid:\n",
      "    loss [a.u.]              0.412325    0.459\n",
      "epoch:  87                    train:   valid:\n",
      "    loss [a.u.]              0.411152    0.452\n",
      "epoch:  88                    train:   valid:\n",
      "    loss [a.u.]              0.411089    0.459\n",
      "epoch:  89                    train:   valid:\n",
      "    loss [a.u.]              0.411805    0.463\n",
      "epoch:  90                    train:   valid:\n",
      "    loss [a.u.]              0.411061    0.461\n",
      "epoch:  91                    train:   valid:\n",
      "    loss [a.u.]              0.411111    0.451\n",
      "epoch:  92                    train:   valid:\n",
      "    loss [a.u.]              0.410064    0.466\n",
      "epoch:  93                    train:   valid:\n",
      "    loss [a.u.]              0.412764    0.453\n",
      "epoch:  94                    train:   valid:\n",
      "    loss [a.u.]              0.410362    0.461\n",
      "epoch:  95                    train:   valid:\n",
      "    loss [a.u.]              0.409681    0.466\n",
      "epoch:  96                    train:   valid:\n",
      "    loss [a.u.]              0.410229    0.468\n",
      "epoch:  97                    train:   valid:\n",
      "    loss [a.u.]              0.411858    0.463\n",
      "epoch:  98                    train:   valid:\n",
      "    loss [a.u.]              0.409307    0.457\n",
      "epoch:  99                    train:   valid:\n",
      "    loss [a.u.]              0.411731    0.460\n",
      "epoch:  100                    train:   valid:\n",
      "    loss [a.u.]              0.409348    0.466\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = prepare_datasets(filename, key, num_train, num_val)\n",
    "key, train_key = jax.random.split(key)\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MP_Dipole_Moment(\n",
    "    features=features,\n",
    "    max_degree=max_degree,\n",
    "    num_iterations=num_iterations,\n",
    "    num_basis_functions=num_basis_functions,\n",
    "    cutoff=cutoff,\n",
    "    max_atomic_number=max_atomic_number\n",
    ")\n",
    "params = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Dense_0', 'Dense_1', 'Dense_2', 'Dense_3', 'Embed_0', 'MessagePass_0', 'MessagePass_1', 'MessagePass_2', 'element_bias'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.05393197 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05393196]\n",
      "{'0+': {'bias': Array([-2.53218375e-02, -9.53917354e-02,  1.57667831e-01, -7.80547559e-02,\n",
      "        1.36273623e-01,  1.95839822e-01,  2.66463216e-02,  1.13846228e-01,\n",
      "        6.53901175e-02, -5.57711311e-02,  2.96855648e-03, -4.23120148e-02,\n",
      "        1.06172115e-01, -8.18778127e-02,  1.87666953e-01, -4.21365499e-02,\n",
      "       -1.36071682e-01,  5.61018325e-02,  8.82903785e-02,  4.70986729e-03,\n",
      "       -2.60044456e-01,  3.94901708e-02,  1.52000010e-01,  3.76045220e-02,\n",
      "        5.37868366e-02, -1.42639047e-02, -5.71135012e-03,  7.24664715e-05,\n",
      "       -6.48561791e-02,  7.30109513e-02, -1.40815064e-01,  9.64180157e-02],      dtype=float32), 'kernel': Array([[-0.0689008 ,  0.22902995,  0.46396974, ..., -0.14538912,\n",
      "         0.01942987,  0.41012758],\n",
      "       [ 0.19594084, -0.2631752 ,  0.1381051 , ...,  0.2726376 ,\n",
      "        -0.18840882,  0.3569617 ],\n",
      "       [-0.1974113 , -0.274881  ,  0.2846897 , ..., -0.13474907,\n",
      "        -0.18574214, -0.11215468],\n",
      "       ...,\n",
      "       [-0.31070358, -0.08854278,  0.505654  , ...,  0.29412675,\n",
      "         0.03743745,  0.34774172],\n",
      "       [ 0.03713622,  0.06676285, -0.12532385, ...,  0.15104769,\n",
      "         0.31036332, -0.11334623],\n",
      "       [-0.16148503, -0.10212184,  0.0981207 , ...,  0.07696035,\n",
      "        -0.28947043,  0.06239758]], dtype=float32)}, '1-': {'kernel': Array([[-0.09089562,  0.00978441, -0.09924425, ...,  0.06972293,\n",
      "         0.28418896, -0.3847888 ],\n",
      "       [-0.24164511,  0.02801222, -0.04340525, ...,  0.20814122,\n",
      "         0.37814325, -0.2568821 ],\n",
      "       [ 0.37025025,  0.18482544, -0.14123121, ..., -0.06900544,\n",
      "        -0.1612    ,  0.15160212],\n",
      "       ...,\n",
      "       [ 0.36885852,  0.2177251 , -0.10248187, ...,  0.298168  ,\n",
      "        -0.19871925, -0.19629991],\n",
      "       [-0.12010543, -0.2091058 , -0.03203931, ...,  0.05368861,\n",
      "        -0.08503079, -0.30051985],\n",
      "       [ 0.21128373, -0.32333475, -0.05074942, ..., -0.16314802,\n",
      "        -0.39421558,  0.00136624]], dtype=float32)}, '2+': {'kernel': Array([[-0.50851655, -0.04106833, -0.3992504 , ..., -0.1814714 ,\n",
      "         0.2769008 , -0.7326096 ],\n",
      "       [-0.3268341 ,  0.04981106, -0.08084136, ..., -0.19330291,\n",
      "         0.5549439 ,  0.21817258],\n",
      "       [ 0.43078074,  0.31300476,  0.13782531, ...,  0.3740985 ,\n",
      "         0.0591685 ,  0.38442194],\n",
      "       ...,\n",
      "       [-0.17662743,  0.3339395 , -0.00878387, ..., -0.1402566 ,\n",
      "         0.37594417, -0.32168517],\n",
      "       [ 0.24812286,  0.1669533 , -0.00793341, ..., -0.14341867,\n",
      "         0.02783956,  0.48777163],\n",
      "       [-0.04314757, -0.29296544,  0.00871331, ..., -0.11960911,\n",
      "         0.25023985, -0.08226088]], dtype=float32)}}\n",
      "{'0+': {'bias': Array([-0.0975676 , -0.03774298,  0.10216592,  0.04987016,  0.06017723,\n",
      "        0.0696945 ,  0.23978695,  0.1176585 ,  0.05073309, -0.04601731,\n",
      "        0.06385957, -0.02359841,  0.01715957,  0.04866588,  0.06988756,\n",
      "       -0.11346275, -0.02234485,  0.15288073, -0.05178523,  0.00640777,\n",
      "        0.06463357, -0.09221953,  0.03155629,  0.1924421 ,  0.02257557,\n",
      "       -0.00291728,  0.1590897 , -0.05295299,  0.18112507,  0.05837373,\n",
      "       -0.15400718, -0.08161224], dtype=float32), 'kernel': Array([[-0.33385542, -0.27213278, -0.16811942, ...,  0.09357652,\n",
      "        -0.33692726, -0.16358037],\n",
      "       [ 0.04155058, -0.16727465,  0.06159088, ...,  0.08956315,\n",
      "        -0.26458126,  0.00544048],\n",
      "       [ 0.10538798, -0.01505153,  0.18703626, ...,  0.05761316,\n",
      "         0.62589294, -0.18907617],\n",
      "       ...,\n",
      "       [-0.10726285, -0.30315813, -0.02221097, ..., -0.07148637,\n",
      "         0.11607945, -0.00176295],\n",
      "       [ 0.04080906,  0.19250987, -0.31051874, ..., -0.21969868,\n",
      "         0.09466634,  0.15731925],\n",
      "       [-0.11429372, -0.01091404,  0.16899209, ..., -0.29641354,\n",
      "        -0.28327736, -0.16717999]], dtype=float32)}, '0-': {'kernel': Array([[-0.33395204,  0.0822647 ,  0.14765416, ...,  0.12456185,\n",
      "        -0.02936476, -0.06891087],\n",
      "       [ 0.04408036,  0.05013852, -0.12577765, ..., -0.2689709 ,\n",
      "         0.03042069,  0.09386874],\n",
      "       [-0.06192017, -0.01631661, -0.05541368, ...,  0.0190136 ,\n",
      "         0.16475505,  0.08308858],\n",
      "       ...,\n",
      "       [-0.13699633,  0.19290829, -0.20269854, ..., -0.25343347,\n",
      "         0.0008978 ,  0.05000874],\n",
      "       [ 0.11085864, -0.1436561 , -0.12747726, ...,  0.01574527,\n",
      "         0.32255837,  0.11186204],\n",
      "       [ 0.20265262, -0.12348641, -0.3212904 , ..., -0.03595603,\n",
      "         0.08135206,  0.06989858]], dtype=float32)}, '1+': {'kernel': Array([[-0.07035399, -0.17299898, -0.12280534, ...,  0.1292225 ,\n",
      "        -0.13378036,  0.32769188],\n",
      "       [-0.27639747, -0.10200124,  0.07097106, ...,  0.22196148,\n",
      "        -0.31150594,  0.38636833],\n",
      "       [ 0.38471735,  0.36943343,  0.12054966, ...,  0.09900343,\n",
      "         0.03764466, -0.17099616],\n",
      "       ...,\n",
      "       [ 0.08283272, -0.01938261,  0.03365609, ...,  0.05320615,\n",
      "         0.21333995,  0.3688262 ],\n",
      "       [ 0.36525777,  0.00809145, -0.36891466, ..., -0.06771629,\n",
      "         0.0423319 ,  0.00381158],\n",
      "       [-0.38195708, -0.3864377 , -0.37295324, ..., -0.6369551 ,\n",
      "        -0.4972716 ,  0.43214995]], dtype=float32)}, '1-': {'kernel': Array([[-0.00351942,  0.06935073, -0.07278229, ...,  0.04114745,\n",
      "         0.00487661,  0.10444721],\n",
      "       [-0.27902895, -0.1708233 ,  0.2689707 , ..., -0.19897653,\n",
      "        -0.07313193,  0.07995852],\n",
      "       [-0.03250728, -0.18402505,  0.11592137, ...,  0.16686049,\n",
      "        -0.02716767,  0.34063676],\n",
      "       ...,\n",
      "       [-0.03880244,  0.08349597,  0.03459574, ..., -0.03729434,\n",
      "        -0.01266344, -0.18311997],\n",
      "       [-0.19397794,  0.35973176,  0.26021814, ...,  0.04039518,\n",
      "        -0.20415914,  0.33001298],\n",
      "       [ 0.10394787,  0.04324116,  0.11201347, ..., -0.06111469,\n",
      "        -0.06840263,  0.19702092]], dtype=float32)}, '2+': {'kernel': Array([[-0.06057851, -0.0258493 ,  0.16032799, ...,  0.1204026 ,\n",
      "        -0.03058571, -0.28169444],\n",
      "       [ 0.18859734, -0.18834485, -0.10168793, ...,  0.07604823,\n",
      "         0.06638335, -0.08735111],\n",
      "       [ 0.08732858,  0.03283987,  0.02995976, ..., -0.05394564,\n",
      "        -0.04683091,  0.00132058],\n",
      "       ...,\n",
      "       [-0.14753114,  0.12390878,  0.00326923, ...,  0.11727395,\n",
      "        -0.07552789, -0.04541767],\n",
      "       [ 0.1113441 , -0.3372432 ,  0.16370448, ..., -0.12002392,\n",
      "         0.00637799, -0.04620937],\n",
      "       [ 0.08136614, -0.075982  ,  0.18964025, ...,  0.19514929,\n",
      "         0.32659358, -0.0240007 ]], dtype=float32)}, '2-': {'kernel': Array([[ 0.14554563, -0.2771793 , -0.28003407, ..., -0.20135276,\n",
      "        -0.2676212 ,  0.10312856],\n",
      "       [-0.2682406 , -0.35049996,  0.36086616, ..., -0.14662887,\n",
      "        -0.39680713, -0.15463568],\n",
      "       [ 0.21023583, -0.1543202 , -0.3993826 , ..., -0.40546173,\n",
      "        -0.5780252 ,  0.32019198],\n",
      "       ...,\n",
      "       [ 0.30072534, -0.20498249,  0.47685355, ...,  0.36545488,\n",
      "        -0.21218121, -0.17940493],\n",
      "       [-0.01828033, -0.6130359 ,  0.78579456, ...,  0.979105  ,\n",
      "        -0.14984468, -0.27019867],\n",
      "       [-0.8230954 , -0.10397039,  0.6516288 , ...,  0.5792446 ,\n",
      "         0.5104735 , -0.4699742 ]], dtype=float32)}}\n",
      "{'0+': {'bias': Array([ 0.23601085,  0.10956506,  0.12166113,  0.16858417,  0.06189826,\n",
      "        0.10135844,  0.03585061,  0.20516707, -0.05696451,  0.15403035,\n",
      "        0.13284981,  0.11652177,  0.03111161,  0.00310521, -0.03663199,\n",
      "        0.01722448,  0.14464428,  0.10809998, -0.02561975,  0.11367738,\n",
      "        0.02701085,  0.00731174,  0.22153668,  0.12778544,  0.14872989,\n",
      "        0.17716683,  0.14226705,  0.07537059,  0.18419105,  0.05120641,\n",
      "        0.15116985,  0.08735307], dtype=float32), 'kernel': Array([[ 0.100494  ,  0.03286062,  0.05186499, ...,  0.07298348,\n",
      "         0.17149772,  0.39427727],\n",
      "       [-0.1987141 , -0.26202434, -0.3955984 , ..., -0.33390465,\n",
      "         0.04987125, -0.08532527],\n",
      "       [ 0.293739  ,  0.4589209 , -0.00932596, ..., -0.23690909,\n",
      "         0.15220915,  0.18541405],\n",
      "       ...,\n",
      "       [ 0.1931439 ,  0.38655782, -0.1603578 , ..., -0.10867779,\n",
      "        -0.3522066 ,  0.06997728],\n",
      "       [-0.68573225, -0.42370105, -0.19940814, ..., -0.2607267 ,\n",
      "         0.01376025, -0.02643618],\n",
      "       [-0.4138776 ,  0.46240744, -0.04525812, ..., -0.05377129,\n",
      "        -0.30751854, -0.09102774]], dtype=float32)}, '1-': {'kernel': Array([[ 0.03337304,  0.12903087, -0.26279253, ..., -0.38744307,\n",
      "        -0.02543411,  0.43577653],\n",
      "       [-0.32868412, -0.24223109, -0.06655285, ...,  0.49491224,\n",
      "        -0.02612559, -0.03041749],\n",
      "       [-0.58943164, -0.02574492, -0.32494926, ...,  0.23330395,\n",
      "        -0.14102116, -0.0853506 ],\n",
      "       ...,\n",
      "       [ 0.28676406,  0.03308408, -0.41257182, ..., -0.25206748,\n",
      "         0.17800437,  0.03643949],\n",
      "       [ 0.14544326,  0.09126101,  0.02804117, ..., -0.08836327,\n",
      "         0.13396153,  0.12395021],\n",
      "       [-0.2014157 ,  0.04626263, -0.14410542, ..., -0.2617506 ,\n",
      "        -0.19638322,  0.08182282]], dtype=float32)}, '2+': {'kernel': Array([[-0.01601361,  0.06895901,  0.03457328, ...,  0.38012978,\n",
      "         0.06578849,  0.173119  ],\n",
      "       [ 0.30511734,  0.25778913, -0.0902865 , ...,  0.02748968,\n",
      "        -0.07190903,  0.2114029 ],\n",
      "       [-0.10035402, -0.10019394, -0.16210261, ..., -0.05050901,\n",
      "        -0.15958181, -0.16956176],\n",
      "       ...,\n",
      "       [ 0.01553668,  0.02105992,  0.09812853, ...,  0.05520246,\n",
      "        -0.34395263, -0.08521933],\n",
      "       [-0.31749043, -0.22497377, -0.01769791, ..., -0.07629272,\n",
      "        -0.31046468, -0.05916417],\n",
      "       [-0.14848633, -0.38039935, -0.16144878, ...,  0.04146574,\n",
      "        -0.01802961,  0.20397264]], dtype=float32)}}\n",
      "{'kernel': Array([[-0.13449307],\n",
      "       [-0.11680014],\n",
      "       [ 0.13618721],\n",
      "       [ 0.13564661],\n",
      "       [-0.04508385],\n",
      "       [ 0.07951174],\n",
      "       [ 0.1291075 ],\n",
      "       [ 0.07984404],\n",
      "       [-0.02427226],\n",
      "       [-0.12306217],\n",
      "       [ 0.12273863],\n",
      "       [-0.05507246],\n",
      "       [-0.04690813],\n",
      "       [-0.02732289],\n",
      "       [ 0.04980756],\n",
      "       [ 0.08343186],\n",
      "       [-0.11989345],\n",
      "       [-0.0871411 ],\n",
      "       [-0.06588367],\n",
      "       [ 0.07902376],\n",
      "       [ 0.06555042],\n",
      "       [ 0.05863173],\n",
      "       [-0.12195574],\n",
      "       [-0.15320444],\n",
      "       [ 0.12824114],\n",
      "       [-0.11938582],\n",
      "       [-0.10326934],\n",
      "       [-0.08415407],\n",
      "       [ 0.13208064],\n",
      "       [ 0.12946641],\n",
      "       [-0.06822335],\n",
      "       [-0.09451055]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(params['params']['element_bias'])\n",
    "print(params['params']['Dense_0'])\n",
    "print(params['params']['Dense_1'])\n",
    "print(params['params']['Dense_2'])\n",
    "print(params['params']['Dense_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(23, dtype=int32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[\"atomic_numbers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pase [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n",
      "(17,) batch 1\n",
      "('atomic_numbers', Array([23, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14],      dtype=int32))\n",
      "(17, 1, 1, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "(17, 1, 9, 32) (272, 1, 9, 16) intermediate iterations,\n",
      "After dense: (17, 1, 9, 1)\n",
      "element_bias (17,)\n",
      "(17, 1, 9, 1)  after bias \n",
      "After segment_sum: (1, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Forma final: (1, 3)\n",
      "dipole moment prediction [[1.3089142  0.4336555  0.46206573]]\n",
      "dipole moment target [ 1.4130961  -0.46504444  1.7085109 ]\n"
     ]
    }
   ],
   "source": [
    "i=4\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "\n",
    "dm=model.apply(params,\n",
    "    atomic_numbers=valid_data[\"atomic_numbers\"],\n",
    "    positions=valid_data[\"positions\"][i],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx)\n",
    "\n",
    "print('dipole moment prediction',dm)\n",
    "print('dipole moment target',valid_data['dipole_moment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
