{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment of inertia tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "\n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "\n",
    "  # Calculate moment of inertia tensors.\n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "\n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2,\n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "\n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "\n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['masses'][0:1].shape)\\nprint(train_data['positions'][0:1].shape)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['masses'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  1.359933 valid loss  0.650806\n",
      "epoch    2 train loss  0.471154 valid loss  0.361696\n",
      "epoch    3 train loss  0.355795 valid loss  0.330647\n",
      "epoch    4 train loss  0.335975 valid loss  0.313806\n",
      "epoch    5 train loss  0.313707 valid loss  0.307905\n",
      "epoch    6 train loss  0.295819 valid loss  0.261203\n",
      "epoch    7 train loss  0.269274 valid loss  0.236152\n",
      "epoch    8 train loss  0.247977 valid loss  0.230414\n",
      "epoch    9 train loss  0.231734 valid loss  0.205375\n",
      "epoch   10 train loss  0.225083 valid loss  0.209193\n",
      "epoch   11 train loss  0.207602 valid loss  0.188981\n",
      "epoch   12 train loss  0.200761 valid loss  0.185399\n",
      "epoch   13 train loss  0.190793 valid loss  0.175384\n",
      "epoch   14 train loss  0.178643 valid loss  0.169232\n",
      "epoch   15 train loss  0.161587 valid loss  0.144264\n",
      "epoch   16 train loss  0.147181 valid loss  0.133549\n",
      "epoch   17 train loss  0.129426 valid loss  0.109603\n",
      "epoch   18 train loss  0.105608 valid loss  0.088042\n",
      "epoch   19 train loss  0.089911 valid loss  0.065109\n",
      "epoch   20 train loss  0.063822 valid loss  0.046581\n",
      "epoch   21 train loss  0.042836 valid loss  0.039833\n",
      "epoch   22 train loss  0.045359 valid loss  0.037879\n",
      "epoch   23 train loss  0.040164 valid loss  0.048566\n",
      "epoch   24 train loss  0.041613 valid loss  0.038852\n",
      "epoch   25 train loss  0.037659 valid loss  0.036376\n",
      "epoch   26 train loss  0.034417 valid loss  0.038344\n",
      "epoch   27 train loss  0.035188 valid loss  0.030785\n",
      "epoch   28 train loss  0.033791 valid loss  0.031197\n",
      "epoch   29 train loss  0.033894 valid loss  0.027737\n",
      "epoch   30 train loss  0.033661 valid loss  0.030736\n",
      "epoch   31 train loss  0.031063 valid loss  0.025989\n",
      "epoch   32 train loss  0.029492 valid loss  0.026486\n",
      "epoch   33 train loss  0.029381 valid loss  0.024327\n",
      "epoch   34 train loss  0.028494 valid loss  0.023874\n",
      "epoch   35 train loss  0.029745 valid loss  0.028929\n",
      "epoch   36 train loss  0.028127 valid loss  0.023015\n",
      "epoch   37 train loss  0.026591 valid loss  0.024094\n",
      "epoch   38 train loss  0.030150 valid loss  0.028875\n",
      "epoch   39 train loss  0.029794 valid loss  0.023051\n",
      "epoch   40 train loss  0.031116 valid loss  0.028424\n",
      "epoch   41 train loss  0.028212 valid loss  0.021998\n",
      "epoch   42 train loss  0.025479 valid loss  0.021279\n",
      "epoch   43 train loss  0.025208 valid loss  0.024125\n",
      "epoch   44 train loss  0.026061 valid loss  0.021150\n",
      "epoch   45 train loss  0.033841 valid loss  0.057384\n",
      "epoch   46 train loss  0.027158 valid loss  0.020290\n",
      "epoch   47 train loss  0.023987 valid loss  0.019953\n",
      "epoch   48 train loss  0.024759 valid loss  0.024592\n",
      "epoch   49 train loss  0.025928 valid loss  0.024374\n",
      "epoch   50 train loss  0.023460 valid loss  0.018815\n",
      "epoch   51 train loss  0.024572 valid loss  0.019993\n",
      "epoch   52 train loss  0.022887 valid loss  0.018372\n",
      "epoch   53 train loss  0.026181 valid loss  0.025382\n",
      "epoch   54 train loss  0.025671 valid loss  0.021562\n",
      "epoch   55 train loss  0.023084 valid loss  0.017371\n",
      "epoch   56 train loss  0.024710 valid loss  0.019425\n",
      "epoch   57 train loss  0.029084 valid loss  0.029276\n",
      "epoch   58 train loss  0.022432 valid loss  0.015814\n",
      "epoch   59 train loss  0.020231 valid loss  0.016195\n",
      "epoch   60 train loss  0.018062 valid loss  0.014702\n",
      "epoch   61 train loss  0.016403 valid loss  0.013027\n",
      "epoch   62 train loss  0.016155 valid loss  0.012115\n",
      "epoch   63 train loss  0.013847 valid loss  0.010955\n",
      "epoch   64 train loss  0.014790 valid loss  0.011325\n",
      "epoch   65 train loss  0.019167 valid loss  0.010374\n",
      "epoch   66 train loss  0.012386 valid loss  0.007711\n",
      "epoch   67 train loss  0.009181 valid loss  0.006592\n",
      "epoch   68 train loss  0.006980 valid loss  0.005574\n",
      "epoch   69 train loss  0.005331 valid loss  0.004197\n",
      "epoch   70 train loss  0.006010 valid loss  0.013546\n",
      "epoch   71 train loss  0.004674 valid loss  0.004104\n",
      "epoch   72 train loss  0.002438 valid loss  0.001107\n",
      "epoch   73 train loss  0.001398 valid loss  0.000977\n",
      "epoch   74 train loss  0.001085 valid loss  0.000696\n",
      "epoch   75 train loss  0.000974 valid loss  0.000871\n",
      "epoch   76 train loss  0.000811 valid loss  0.000642\n",
      "epoch   77 train loss  0.000723 valid loss  0.000854\n",
      "epoch   78 train loss  0.000840 valid loss  0.001953\n",
      "epoch   79 train loss  0.000751 valid loss  0.000519\n",
      "epoch   80 train loss  0.000636 valid loss  0.000593\n",
      "epoch   81 train loss  0.000609 valid loss  0.000431\n",
      "epoch   82 train loss  0.000456 valid loss  0.000393\n",
      "epoch   83 train loss  0.000414 valid loss  0.000432\n",
      "epoch   84 train loss  0.000414 valid loss  0.000325\n",
      "epoch   85 train loss  0.000374 valid loss  0.000378\n",
      "epoch   86 train loss  0.000338 valid loss  0.000319\n",
      "epoch   87 train loss  0.000391 valid loss  0.000368\n",
      "epoch   88 train loss  0.000297 valid loss  0.000303\n",
      "epoch   89 train loss  0.000289 valid loss  0.000255\n",
      "epoch   90 train loss  0.000279 valid loss  0.000331\n",
      "epoch   91 train loss  0.000330 valid loss  0.000231\n",
      "epoch   92 train loss  0.000239 valid loss  0.000275\n",
      "epoch   93 train loss  0.000244 valid loss  0.000312\n",
      "epoch   94 train loss  0.000205 valid loss  0.000246\n",
      "epoch   95 train loss  0.000211 valid loss  0.000307\n",
      "epoch   96 train loss  0.000207 valid loss  0.000279\n",
      "epoch   97 train loss  0.000195 valid loss  0.000168\n",
      "epoch   98 train loss  0.000177 valid loss  0.000205\n",
      "epoch   99 train loss  0.000346 valid loss  0.000148\n",
      "epoch  100 train loss  0.000156 valid loss  0.000148\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (10, 4)\n",
      "x shape: (10, 1, 4, 1)\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (1, 8) but got shape (1, 1) instead for parameter \"kernel\" in \"/Dense_0/0+\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m masses, positions, target \u001b[38;5;241m=\u001b[39m valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasses\u001b[39m\u001b[38;5;124m'\u001b[39m][i], valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m'\u001b[39m][i], valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minertia_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m----> 3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n",
      "    \u001b[1;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[46], line 12\u001b[0m, in \u001b[0;36mDipole_Moment.__call__\u001b[1;34m(self, atomic_numbers, positions)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Apply transformations.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43me3x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter Dense layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m e3x\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mTensorDense(max_degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(x)  \n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\e3x\\nn\\modules.py:241\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Has no pseudotensors.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m   dense \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_dense_for_each_degree(max_degree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias)\n\u001b[0;32m    239\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    240\u001b[0m       [\n\u001b[1;32m--> 241\u001b[0m           \u001b[43mdense\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    243\u001b[0m       ],\n\u001b[0;32m    244\u001b[0m       axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    245\u001b[0m   )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape has passed checks even though it is invalid!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\flax\\linen\\linear.py:254\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    261\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[0;32m    262\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[0;32m    263\u001b[0m     )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\flax\\core\\scope.py:960\u001b[0m, in \u001b[0;36mScope.param\u001b[1;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[0;32m    955\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[1;32m--> 960\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[0;32m    961\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[0;32m    962\u001b[0m       )\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (1, 8) but got shape (1, 1) instead for parameter \"kernel\" in \"/Dense_0/0+\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "filename='Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.5042.R_E_F_D_Q.npz'\n",
    "dataset= np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print('Dipole moment shape array',dataset['D'].shape)\n",
    "print('Dipole moment units', dataset['D_units'])\n",
    "\n",
    "print('Atomic numbers', dataset['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z=jnp.full(16,14)\n",
    "    Z=jnp.append(Z,23)\n",
    "    Z=jnp.expand_dims(Z,axis=0)\n",
    "    Z=jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train=1000\n",
    "num_val=200\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "print(train_data['dipole_moment'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "  #features = 1\n",
    "  #max_degree = 1\n",
    "  @nn.compact\n",
    "  def __call__(self,atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((atomic_numbers[...,None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    #print(\"Initial shape:\", x.shape)\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "    #print(\"x shape:\", x.shape)\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=1)(x) \n",
    "    #print(\"After Dense layer:\", x.shape)\n",
    "    x = e3x.nn.TensorDense(max_degree=1)(x)  \n",
    "    #print(\"After TensorDense layer:\", x.shape)\n",
    "    x=jnp.sum(x, axis=-4) \n",
    "    #print(\"After sum:\", x.shape)\n",
    "    y = x[..., 1, 1:4, 0]\n",
    "    #print(\"After slicing:\", y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version\n",
      "------------------- ----------\n",
      "absl-py             2.1.0\n",
      "ase                 3.23.0\n",
      "asttokens           2.4.1\n",
      "attrs               23.2.0\n",
      "chex                0.1.85\n",
      "colorama            0.4.6\n",
      "comm                0.2.1\n",
      "contextlib2         21.6.0\n",
      "contourpy           1.2.0\n",
      "cycler              0.12.1\n",
      "dataclasses         0.6\n",
      "debugpy             1.8.1\n",
      "decorator           5.1.1\n",
      "dm-haiku            0.0.12\n",
      "e3nn-jax            0.20.6\n",
      "e3x                 1.0.1\n",
      "einops              0.7.0\n",
      "etils               1.7.0\n",
      "exceptiongroup      1.2.0\n",
      "executing           2.0.1\n",
      "flax                0.8.1\n",
      "fonttools           4.49.0\n",
      "fsspec              2024.2.0\n",
      "importlib-metadata  7.0.1\n",
      "importlib_resources 6.1.3\n",
      "ipykernel           6.29.2\n",
      "ipython             8.22.1\n",
      "jax                 0.4.25\n",
      "jax-md              0.2.8\n",
      "jaxlib              0.4.25\n",
      "jaxtyping           0.2.28\n",
      "jedi                0.19.1\n",
      "jmp                 0.0.4\n",
      "jraph               0.0.6.dev0\n",
      "jupyter_client      8.6.0\n",
      "jupyter_core        5.7.1\n",
      "kiwisolver          1.4.5\n",
      "markdown-it-py      3.0.0\n",
      "matplotlib          3.8.3\n",
      "matplotlib-inline   0.1.6\n",
      "mdurl               0.1.2\n",
      "ml_collections      0.1.1\n",
      "ml-dtypes           0.3.2\n",
      "more-itertools      10.2.0\n",
      "mpmath              1.3.0\n",
      "msgpack             1.0.8\n",
      "nest_asyncio        1.6.0\n",
      "numpy               1.26.4\n",
      "opt-einsum          3.3.0\n",
      "optax               0.2.1\n",
      "orbax-checkpoint    0.5.3\n",
      "packaging           23.2\n",
      "pandas              2.2.2\n",
      "parso               0.8.3\n",
      "pickleshare         0.7.5\n",
      "pillow              10.2.0\n",
      "pip                 24.0\n",
      "platformdirs        4.2.0\n",
      "prompt-toolkit      3.0.42\n",
      "protobuf            4.25.3\n",
      "psutil              5.9.8\n",
      "pure-eval           0.2.2\n",
      "Pygments            2.17.2\n",
      "pyparsing           3.1.2\n",
      "python-dateutil     2.8.2\n",
      "pytz                2024.1\n",
      "pywin32             306\n",
      "PyYAML              6.0.1\n",
      "pyzmq               25.1.2\n",
      "rich                13.7.1\n",
      "scipy               1.12.0\n",
      "seaborn             0.13.2\n",
      "setuptools          69.1.1\n",
      "six                 1.16.0\n",
      "stack-data          0.6.2\n",
      "sympy               1.12\n",
      "tabulate            0.9.0\n",
      "tensorstore         0.1.54\n",
      "toolz               0.12.1\n",
      "tornado             6.4\n",
      "traitlets           5.14.1\n",
      "typeguard           2.13.3\n",
      "typing_extensions   4.10.0\n",
      "tzdata              2024.1\n",
      "wcwidth             0.2.13\n",
      "wheel               0.42.0\n",
      "zipp                3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list requierement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dm_model = Dipole_Moment()\\nkey = jax.random.PRNGKey(0)\\n\\n# Generate train and test datasets.\\nkey, data_key = jax.random.split(key)\\ntrain_data, valid_data = generate_datasets(data_key)\\nparams = dm_model.init(key, train_data['masses'][0:1], train_data['positions'][0:1])\\nmoment=dm_model.apply(params,train_data['masses'][0:1], train_data['positions'][0:1])\\nprint(moment.shape)\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dm_model = Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "params = dm_model.init(key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "moment=dm_model.apply(params,train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "print(moment.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole_moment = model_apply(params,batch['atomic_numbers'] ,batch['positions'])\n",
    "    loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  dipole_moment = model_apply(params,batch['atomic_numbers'],batch['positions'])\n",
    "  loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key,train_data['atomic_numbers'][0:1],train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['positions'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      #print(batch['dipole_moment'].shape)\n",
    "\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_train=3000\n",
    "num_val=200\n",
    "train_data, valid_data = prepare_datasets(filename,key, num_train,num_val)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['positions'][0:1].shape)\\nprint(train_data['atomic_numbers'][0:1].shape)\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['positions'][0:1].shape)\n",
    "print(train_data['atomic_numbers'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  7.643333 valid loss  1.240807\n",
      "epoch    2 train loss  0.617081 valid loss  0.194544\n",
      "epoch    3 train loss  0.061019 valid loss  0.006598\n",
      "epoch    4 train loss  0.004027 valid loss  0.004015\n",
      "epoch    5 train loss  0.003565 valid loss  0.004022\n",
      "epoch    6 train loss  0.003572 valid loss  0.004016\n",
      "epoch    7 train loss  0.003574 valid loss  0.004019\n",
      "epoch    8 train loss  0.003568 valid loss  0.004015\n",
      "epoch    9 train loss  0.003571 valid loss  0.004017\n",
      "epoch   10 train loss  0.003567 valid loss  0.004017\n",
      "epoch   11 train loss  0.003569 valid loss  0.004017\n",
      "epoch   12 train loss  0.003562 valid loss  0.004013\n",
      "epoch   13 train loss  0.003564 valid loss  0.004021\n",
      "epoch   14 train loss  0.003562 valid loss  0.004007\n",
      "epoch   15 train loss  0.003570 valid loss  0.004006\n",
      "epoch   16 train loss  0.003559 valid loss  0.004008\n",
      "epoch   17 train loss  0.003564 valid loss  0.004005\n",
      "epoch   18 train loss  0.003560 valid loss  0.004003\n",
      "epoch   19 train loss  0.003559 valid loss  0.004009\n",
      "epoch   20 train loss  0.003556 valid loss  0.004001\n",
      "epoch   21 train loss  0.003558 valid loss  0.004005\n",
      "epoch   22 train loss  0.003560 valid loss  0.004003\n",
      "epoch   23 train loss  0.003547 valid loss  0.003999\n",
      "epoch   24 train loss  0.003545 valid loss  0.003995\n",
      "epoch   25 train loss  0.003551 valid loss  0.003991\n",
      "epoch   26 train loss  0.003540 valid loss  0.003988\n",
      "epoch   27 train loss  0.003536 valid loss  0.003987\n",
      "epoch   28 train loss  0.003538 valid loss  0.003984\n",
      "epoch   29 train loss  0.003538 valid loss  0.003980\n",
      "epoch   30 train loss  0.003545 valid loss  0.003980\n",
      "epoch   31 train loss  0.003538 valid loss  0.003977\n",
      "epoch   32 train loss  0.003521 valid loss  0.003993\n",
      "epoch   33 train loss  0.003528 valid loss  0.003977\n",
      "epoch   34 train loss  0.003527 valid loss  0.003967\n",
      "epoch   35 train loss  0.003525 valid loss  0.003969\n",
      "epoch   36 train loss  0.003524 valid loss  0.003961\n",
      "epoch   37 train loss  0.003514 valid loss  0.003956\n",
      "epoch   38 train loss  0.003517 valid loss  0.003953\n",
      "epoch   39 train loss  0.003514 valid loss  0.003978\n",
      "epoch   40 train loss  0.003507 valid loss  0.003962\n",
      "epoch   41 train loss  0.003509 valid loss  0.003942\n",
      "epoch   42 train loss  0.003498 valid loss  0.003937\n",
      "epoch   43 train loss  0.003500 valid loss  0.003945\n",
      "epoch   44 train loss  0.003486 valid loss  0.003932\n",
      "epoch   45 train loss  0.003483 valid loss  0.003936\n",
      "epoch   46 train loss  0.003477 valid loss  0.003923\n",
      "epoch   47 train loss  0.003479 valid loss  0.003914\n",
      "epoch   48 train loss  0.003466 valid loss  0.003918\n",
      "epoch   49 train loss  0.003460 valid loss  0.003905\n",
      "epoch   50 train loss  0.003461 valid loss  0.004025\n",
      "epoch   51 train loss  0.003469 valid loss  0.003913\n",
      "epoch   52 train loss  0.003450 valid loss  0.003933\n",
      "epoch   53 train loss  0.003450 valid loss  0.003887\n",
      "epoch   54 train loss  0.003447 valid loss  0.003874\n",
      "epoch   55 train loss  0.003441 valid loss  0.003879\n",
      "epoch   56 train loss  0.003424 valid loss  0.003861\n",
      "epoch   57 train loss  0.003423 valid loss  0.003873\n",
      "epoch   58 train loss  0.003411 valid loss  0.003877\n",
      "epoch   59 train loss  0.003401 valid loss  0.003840\n",
      "epoch   60 train loss  0.003390 valid loss  0.003849\n",
      "epoch   61 train loss  0.003400 valid loss  0.003881\n",
      "epoch   62 train loss  0.003389 valid loss  0.003821\n",
      "epoch   63 train loss  0.003370 valid loss  0.003852\n",
      "epoch   64 train loss  0.003371 valid loss  0.003818\n",
      "epoch   65 train loss  0.003370 valid loss  0.003797\n",
      "epoch   66 train loss  0.003360 valid loss  0.003787\n",
      "epoch   67 train loss  0.003340 valid loss  0.003895\n",
      "epoch   68 train loss  0.003349 valid loss  0.003764\n",
      "epoch   69 train loss  0.003314 valid loss  0.003797\n",
      "epoch   70 train loss  0.003318 valid loss  0.003745\n",
      "epoch   71 train loss  0.003297 valid loss  0.003739\n",
      "epoch   72 train loss  0.003306 valid loss  0.003940\n",
      "epoch   73 train loss  0.003306 valid loss  0.003743\n",
      "epoch   74 train loss  0.003286 valid loss  0.003706\n",
      "epoch   75 train loss  0.003284 valid loss  0.003703\n",
      "epoch   76 train loss  0.003267 valid loss  0.003706\n",
      "epoch   77 train loss  0.003255 valid loss  0.003669\n",
      "epoch   78 train loss  0.003249 valid loss  0.003658\n",
      "epoch   79 train loss  0.003226 valid loss  0.003675\n",
      "epoch   80 train loss  0.003217 valid loss  0.003643\n",
      "epoch   81 train loss  0.003197 valid loss  0.003622\n",
      "epoch   82 train loss  0.003184 valid loss  0.003613\n",
      "epoch   83 train loss  0.003199 valid loss  0.003612\n",
      "epoch   84 train loss  0.003166 valid loss  0.003596\n",
      "epoch   85 train loss  0.003170 valid loss  0.003601\n",
      "epoch   86 train loss  0.003152 valid loss  0.003569\n",
      "epoch   87 train loss  0.003168 valid loss  0.003544\n",
      "epoch   88 train loss  0.003109 valid loss  0.003926\n",
      "epoch   89 train loss  0.003153 valid loss  0.003532\n",
      "epoch   90 train loss  0.003103 valid loss  0.003662\n",
      "epoch   91 train loss  0.003118 valid loss  0.003543\n",
      "epoch   92 train loss  0.003060 valid loss  0.003474\n",
      "epoch   93 train loss  0.003079 valid loss  0.003614\n",
      "epoch   94 train loss  0.003073 valid loss  0.003429\n",
      "epoch   95 train loss  0.003022 valid loss  0.003436\n",
      "epoch   96 train loss  0.003022 valid loss  0.003639\n",
      "epoch   97 train loss  0.003072 valid loss  0.003429\n",
      "epoch   98 train loss  0.002982 valid loss  0.003362\n",
      "epoch   99 train loss  0.002933 valid loss  0.003434\n",
      "epoch  100 train loss  0.002907 valid loss  0.003319\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Dipole_Moment()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[ 1.4647644  -0.30743074  1.7500215 ]\n",
      "mean squared error 0.0022456348\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
