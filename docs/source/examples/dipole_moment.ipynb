{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment of inertia tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "\n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "\n",
    "  # Calculate moment of inertia tensors.\n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "\n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2,\n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "\n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "\n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['masses'][0:1].shape)\\nprint(train_data['positions'][0:1].shape)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['masses'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  1.359933 valid loss  0.650806\n",
      "epoch    2 train loss  0.471154 valid loss  0.361696\n",
      "epoch    3 train loss  0.355795 valid loss  0.330647\n",
      "epoch    4 train loss  0.335975 valid loss  0.313806\n",
      "epoch    5 train loss  0.313707 valid loss  0.307905\n",
      "epoch    6 train loss  0.295819 valid loss  0.261203\n",
      "epoch    7 train loss  0.269274 valid loss  0.236152\n",
      "epoch    8 train loss  0.247977 valid loss  0.230414\n",
      "epoch    9 train loss  0.231734 valid loss  0.205375\n",
      "epoch   10 train loss  0.225083 valid loss  0.209193\n",
      "epoch   11 train loss  0.207602 valid loss  0.188981\n",
      "epoch   12 train loss  0.200761 valid loss  0.185399\n",
      "epoch   13 train loss  0.190793 valid loss  0.175384\n",
      "epoch   14 train loss  0.178643 valid loss  0.169232\n",
      "epoch   15 train loss  0.161587 valid loss  0.144264\n",
      "epoch   16 train loss  0.147181 valid loss  0.133549\n",
      "epoch   17 train loss  0.129426 valid loss  0.109603\n",
      "epoch   18 train loss  0.105608 valid loss  0.088042\n",
      "epoch   19 train loss  0.089911 valid loss  0.065109\n",
      "epoch   20 train loss  0.063822 valid loss  0.046581\n",
      "epoch   21 train loss  0.042836 valid loss  0.039833\n",
      "epoch   22 train loss  0.045359 valid loss  0.037879\n",
      "epoch   23 train loss  0.040164 valid loss  0.048566\n",
      "epoch   24 train loss  0.041613 valid loss  0.038852\n",
      "epoch   25 train loss  0.037659 valid loss  0.036376\n",
      "epoch   26 train loss  0.034417 valid loss  0.038344\n",
      "epoch   27 train loss  0.035188 valid loss  0.030785\n",
      "epoch   28 train loss  0.033791 valid loss  0.031197\n",
      "epoch   29 train loss  0.033894 valid loss  0.027737\n",
      "epoch   30 train loss  0.033661 valid loss  0.030736\n",
      "epoch   31 train loss  0.031063 valid loss  0.025989\n",
      "epoch   32 train loss  0.029492 valid loss  0.026486\n",
      "epoch   33 train loss  0.029381 valid loss  0.024327\n",
      "epoch   34 train loss  0.028494 valid loss  0.023874\n",
      "epoch   35 train loss  0.029745 valid loss  0.028929\n",
      "epoch   36 train loss  0.028127 valid loss  0.023015\n",
      "epoch   37 train loss  0.026591 valid loss  0.024094\n",
      "epoch   38 train loss  0.030150 valid loss  0.028875\n",
      "epoch   39 train loss  0.029794 valid loss  0.023051\n",
      "epoch   40 train loss  0.031116 valid loss  0.028424\n",
      "epoch   41 train loss  0.028212 valid loss  0.021998\n",
      "epoch   42 train loss  0.025479 valid loss  0.021279\n",
      "epoch   43 train loss  0.025208 valid loss  0.024125\n",
      "epoch   44 train loss  0.026061 valid loss  0.021150\n",
      "epoch   45 train loss  0.033841 valid loss  0.057384\n",
      "epoch   46 train loss  0.027158 valid loss  0.020290\n",
      "epoch   47 train loss  0.023987 valid loss  0.019953\n",
      "epoch   48 train loss  0.024759 valid loss  0.024592\n",
      "epoch   49 train loss  0.025928 valid loss  0.024374\n",
      "epoch   50 train loss  0.023460 valid loss  0.018815\n",
      "epoch   51 train loss  0.024572 valid loss  0.019993\n",
      "epoch   52 train loss  0.022887 valid loss  0.018372\n",
      "epoch   53 train loss  0.026181 valid loss  0.025382\n",
      "epoch   54 train loss  0.025671 valid loss  0.021562\n",
      "epoch   55 train loss  0.023084 valid loss  0.017371\n",
      "epoch   56 train loss  0.024710 valid loss  0.019425\n",
      "epoch   57 train loss  0.029084 valid loss  0.029276\n",
      "epoch   58 train loss  0.022432 valid loss  0.015814\n",
      "epoch   59 train loss  0.020231 valid loss  0.016195\n",
      "epoch   60 train loss  0.018062 valid loss  0.014702\n",
      "epoch   61 train loss  0.016403 valid loss  0.013027\n",
      "epoch   62 train loss  0.016155 valid loss  0.012115\n",
      "epoch   63 train loss  0.013847 valid loss  0.010955\n",
      "epoch   64 train loss  0.014790 valid loss  0.011325\n",
      "epoch   65 train loss  0.019167 valid loss  0.010374\n",
      "epoch   66 train loss  0.012386 valid loss  0.007711\n",
      "epoch   67 train loss  0.009181 valid loss  0.006592\n",
      "epoch   68 train loss  0.006980 valid loss  0.005574\n",
      "epoch   69 train loss  0.005331 valid loss  0.004197\n",
      "epoch   70 train loss  0.006010 valid loss  0.013546\n",
      "epoch   71 train loss  0.004674 valid loss  0.004104\n",
      "epoch   72 train loss  0.002438 valid loss  0.001107\n",
      "epoch   73 train loss  0.001398 valid loss  0.000977\n",
      "epoch   74 train loss  0.001085 valid loss  0.000696\n",
      "epoch   75 train loss  0.000974 valid loss  0.000871\n",
      "epoch   76 train loss  0.000811 valid loss  0.000642\n",
      "epoch   77 train loss  0.000723 valid loss  0.000854\n",
      "epoch   78 train loss  0.000840 valid loss  0.001953\n",
      "epoch   79 train loss  0.000751 valid loss  0.000519\n",
      "epoch   80 train loss  0.000636 valid loss  0.000593\n",
      "epoch   81 train loss  0.000609 valid loss  0.000431\n",
      "epoch   82 train loss  0.000456 valid loss  0.000393\n",
      "epoch   83 train loss  0.000414 valid loss  0.000432\n",
      "epoch   84 train loss  0.000414 valid loss  0.000325\n",
      "epoch   85 train loss  0.000374 valid loss  0.000378\n",
      "epoch   86 train loss  0.000338 valid loss  0.000319\n",
      "epoch   87 train loss  0.000391 valid loss  0.000368\n",
      "epoch   88 train loss  0.000297 valid loss  0.000303\n",
      "epoch   89 train loss  0.000289 valid loss  0.000255\n",
      "epoch   90 train loss  0.000279 valid loss  0.000331\n",
      "epoch   91 train loss  0.000330 valid loss  0.000231\n",
      "epoch   92 train loss  0.000239 valid loss  0.000275\n",
      "epoch   93 train loss  0.000244 valid loss  0.000312\n",
      "epoch   94 train loss  0.000205 valid loss  0.000246\n",
      "epoch   95 train loss  0.000211 valid loss  0.000307\n",
      "epoch   96 train loss  0.000207 valid loss  0.000279\n",
      "epoch   97 train loss  0.000195 valid loss  0.000168\n",
      "epoch   98 train loss  0.000177 valid loss  0.000205\n",
      "epoch   99 train loss  0.000346 valid loss  0.000148\n",
      "epoch  100 train loss  0.000156 valid loss  0.000148\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (10, 4)\n",
      "x shape: (10, 1, 4, 1)\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (1, 8) but got shape (1, 1) instead for parameter \"kernel\" in \"/Dense_0/0+\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m masses, positions, target \u001b[38;5;241m=\u001b[39m valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasses\u001b[39m\u001b[38;5;124m'\u001b[39m][i], valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m'\u001b[39m][i], valid_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minertia_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m----> 3\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(target)\n",
      "    \u001b[1;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[46], line 12\u001b[0m, in \u001b[0;36mDipole_Moment.__call__\u001b[1;34m(self, atomic_numbers, positions)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Apply transformations.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43me3x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter Dense layer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m e3x\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mTensorDense(max_degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(x)  \n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\e3x\\nn\\modules.py:241\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Has no pseudotensors.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m   dense \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_dense_for_each_degree(max_degree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias)\n\u001b[0;32m    239\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    240\u001b[0m       [\n\u001b[1;32m--> 241\u001b[0m           \u001b[43mdense\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_degree \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    243\u001b[0m       ],\n\u001b[0;32m    244\u001b[0m       axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    245\u001b[0m   )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape has passed checks even though it is invalid!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\flax\\linen\\linear.py:254\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    261\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[0;32m    262\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[0;32m    263\u001b[0m     )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\jax-gpu\\Lib\\site-packages\\flax\\core\\scope.py:960\u001b[0m, in \u001b[0;36mScope.param\u001b[1;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[0;32m    955\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[1;32m--> 960\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[0;32m    961\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[0;32m    962\u001b[0m       )\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (1, 8) but got shape (1, 1) instead for parameter \"kernel\" in \"/Dense_0/0+\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "filename='Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.5042.R_E_F_D_Q.npz'\n",
    "dataset= np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print('Dipole moment shape array',dataset['D'].shape)\n",
    "print('Dipole moment units', dataset['D_units'])\n",
    "\n",
    "print('Atomic numbers', dataset['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z=jnp.full(16,14)\n",
    "    Z=jnp.append(Z,23)\n",
    "    Z=jnp.expand_dims(Z,axis=0)\n",
    "    Z=jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train=1000\n",
    "num_val=200\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "print(train_data['dipole_moment'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "  #features = 1\n",
    "  #max_degree = 1\n",
    "  @nn.compact\n",
    "  def __call__(self,atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((atomic_numbers[...,None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    #print(\"Initial shape:\", x.shape)\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "    #print(\"x shape:\", x.shape)\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=1)(x) \n",
    "    #print(\"After Dense layer:\", x.shape)\n",
    "    x = e3x.nn.TensorDense(max_degree=1)(x)  \n",
    "    #print(\"After TensorDense layer:\", x.shape)\n",
    "    x=jnp.sum(x, axis=-4) \n",
    "    #print(\"After sum:\", x.shape)\n",
    "    y = x[..., 1, 1:4, 0]\n",
    "    #print(\"After slicing:\", y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package             Version\n",
      "------------------- ----------\n",
      "absl-py             2.1.0\n",
      "ase                 3.23.0\n",
      "asttokens           2.4.1\n",
      "attrs               23.2.0\n",
      "chex                0.1.85\n",
      "colorama            0.4.6\n",
      "comm                0.2.1\n",
      "contextlib2         21.6.0\n",
      "contourpy           1.2.0\n",
      "cycler              0.12.1\n",
      "dataclasses         0.6\n",
      "debugpy             1.8.1\n",
      "decorator           5.1.1\n",
      "dm-haiku            0.0.12\n",
      "e3nn-jax            0.20.6\n",
      "e3x                 1.0.1\n",
      "einops              0.7.0\n",
      "etils               1.7.0\n",
      "exceptiongroup      1.2.0\n",
      "executing           2.0.1\n",
      "flax                0.8.1\n",
      "fonttools           4.49.0\n",
      "fsspec              2024.2.0\n",
      "importlib-metadata  7.0.1\n",
      "importlib_resources 6.1.3\n",
      "ipykernel           6.29.2\n",
      "ipython             8.22.1\n",
      "jax                 0.4.25\n",
      "jax-md              0.2.8\n",
      "jaxlib              0.4.25\n",
      "jaxtyping           0.2.28\n",
      "jedi                0.19.1\n",
      "jmp                 0.0.4\n",
      "jraph               0.0.6.dev0\n",
      "jupyter_client      8.6.0\n",
      "jupyter_core        5.7.1\n",
      "kiwisolver          1.4.5\n",
      "markdown-it-py      3.0.0\n",
      "matplotlib          3.8.3\n",
      "matplotlib-inline   0.1.6\n",
      "mdurl               0.1.2\n",
      "ml_collections      0.1.1\n",
      "ml-dtypes           0.3.2\n",
      "more-itertools      10.2.0\n",
      "mpmath              1.3.0\n",
      "msgpack             1.0.8\n",
      "nest_asyncio        1.6.0\n",
      "numpy               1.26.4\n",
      "opt-einsum          3.3.0\n",
      "optax               0.2.1\n",
      "orbax-checkpoint    0.5.3\n",
      "packaging           23.2\n",
      "pandas              2.2.2\n",
      "parso               0.8.3\n",
      "pickleshare         0.7.5\n",
      "pillow              10.2.0\n",
      "pip                 24.0\n",
      "platformdirs        4.2.0\n",
      "prompt-toolkit      3.0.42\n",
      "protobuf            4.25.3\n",
      "psutil              5.9.8\n",
      "pure-eval           0.2.2\n",
      "Pygments            2.17.2\n",
      "pyparsing           3.1.2\n",
      "python-dateutil     2.8.2\n",
      "pytz                2024.1\n",
      "pywin32             306\n",
      "PyYAML              6.0.1\n",
      "pyzmq               25.1.2\n",
      "rich                13.7.1\n",
      "scipy               1.12.0\n",
      "seaborn             0.13.2\n",
      "setuptools          69.1.1\n",
      "six                 1.16.0\n",
      "stack-data          0.6.2\n",
      "sympy               1.12\n",
      "tabulate            0.9.0\n",
      "tensorstore         0.1.54\n",
      "toolz               0.12.1\n",
      "tornado             6.4\n",
      "traitlets           5.14.1\n",
      "typeguard           2.13.3\n",
      "typing_extensions   4.10.0\n",
      "tzdata              2024.1\n",
      "wcwidth             0.2.13\n",
      "wheel               0.42.0\n",
      "zipp                3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list requierement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dm_model = Dipole_Moment()\\nkey = jax.random.PRNGKey(0)\\n\\n# Generate train and test datasets.\\nkey, data_key = jax.random.split(key)\\ntrain_data, valid_data = generate_datasets(data_key)\\nparams = dm_model.init(key, train_data['masses'][0:1], train_data['positions'][0:1])\\nmoment=dm_model.apply(params,train_data['masses'][0:1], train_data['positions'][0:1])\\nprint(moment.shape)\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dm_model = Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "params = dm_model.init(key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "moment=dm_model.apply(params,train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "print(moment.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole_moment = model_apply(params,batch['atomic_numbers'] ,batch['positions'])\n",
    "    loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  dipole_moment = model_apply(params,batch['atomic_numbers'],batch['positions'])\n",
    "  loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key,train_data['atomic_numbers'][0:1],train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['positions'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  list_train_loss = []\n",
    "  list_val_loss = []\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      #print(batch['dipole_moment'].shape)\n",
    "\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "      \n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "    list_val_loss.append(valid_loss)\n",
    "    list_train_loss.append(train_loss)\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params ,list_train_loss , list_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_train=3000\n",
    "num_val=200\n",
    "train_data, valid_data = prepare_datasets(filename,key, num_train,num_val)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 10000\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['positions'][0:1].shape)\\nprint(train_data['atomic_numbers'][0:1].shape)\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['positions'][0:1].shape)\n",
    "print(train_data['atomic_numbers'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  35.072781 valid loss  28.456921\n",
      "epoch    2 train loss  24.920635 valid loss  20.241196\n",
      "epoch    3 train loss  17.798740 valid loss  14.573094\n",
      "epoch    4 train loss  12.910925 valid loss  10.720650\n",
      "epoch    5 train loss  9.593145 valid loss  8.105575\n",
      "epoch    6 train loss  7.336629 valid loss  6.317753\n",
      "epoch    7 train loss  5.781274 valid loss  5.068070\n",
      "epoch    8 train loss  4.688513 valid loss  4.175456\n",
      "epoch    9 train loss  3.900592 valid loss  3.522735\n",
      "epoch   10 train loss  3.315850 valid loss  3.027928\n",
      "epoch   11 train loss  2.870950 valid loss  2.645716\n",
      "epoch   12 train loss  2.521299 valid loss  2.340809\n",
      "epoch   13 train loss  2.238468 valid loss  2.092879\n",
      "epoch   14 train loss  2.009790 valid loss  1.885348\n",
      "epoch   15 train loss  1.815598 valid loss  1.709185\n",
      "epoch   16 train loss  1.648358 valid loss  1.557613\n",
      "epoch   17 train loss  1.504528 valid loss  1.424069\n",
      "epoch   18 train loss  1.378565 valid loss  1.306026\n",
      "epoch   19 train loss  1.264315 valid loss  1.199716\n",
      "epoch   20 train loss  1.163084 valid loss  1.103741\n",
      "epoch   21 train loss  1.070225 valid loss  1.015890\n",
      "epoch   22 train loss  0.985474 valid loss  0.935203\n",
      "epoch   23 train loss  0.906657 valid loss  0.860566\n",
      "epoch   24 train loss  0.834602 valid loss  0.791166\n",
      "epoch   25 train loss  0.766971 valid loss  0.726371\n",
      "epoch   26 train loss  0.703653 valid loss  0.665694\n",
      "epoch   27 train loss  0.644555 valid loss  0.608783\n",
      "epoch   28 train loss  0.588971 valid loss  0.555135\n",
      "epoch   29 train loss  0.537147 valid loss  0.504660\n",
      "epoch   30 train loss  0.487501 valid loss  0.457183\n",
      "epoch   31 train loss  0.440707 valid loss  0.412577\n",
      "epoch   32 train loss  0.397146 valid loss  0.370749\n",
      "epoch   33 train loss  0.356281 valid loss  0.331364\n",
      "epoch   34 train loss  0.318232 valid loss  0.294869\n",
      "epoch   35 train loss  0.282191 valid loss  0.260744\n",
      "epoch   36 train loss  0.249753 valid loss  0.229172\n",
      "epoch   37 train loss  0.218698 valid loss  0.200151\n",
      "epoch   38 train loss  0.190848 valid loss  0.173882\n",
      "epoch   39 train loss  0.164734 valid loss  0.149826\n",
      "epoch   40 train loss  0.141720 valid loss  0.128163\n",
      "epoch   41 train loss  0.121129 valid loss  0.108806\n",
      "epoch   42 train loss  0.102634 valid loss  0.091680\n",
      "epoch   43 train loss  0.086054 valid loss  0.076663\n",
      "epoch   44 train loss  0.071969 valid loss  0.063622\n",
      "epoch   45 train loss  0.059474 valid loss  0.052591\n",
      "epoch   46 train loss  0.048934 valid loss  0.043053\n",
      "epoch   47 train loss  0.040160 valid loss  0.035181\n",
      "epoch   48 train loss  0.032560 valid loss  0.028500\n",
      "epoch   49 train loss  0.026391 valid loss  0.023134\n",
      "epoch   50 train loss  0.021169 valid loss  0.018779\n",
      "epoch   51 train loss  0.017334 valid loss  0.015187\n",
      "epoch   52 train loss  0.013897 valid loss  0.012428\n",
      "epoch   53 train loss  0.011501 valid loss  0.010267\n",
      "epoch   54 train loss  0.009505 valid loss  0.008658\n",
      "epoch   55 train loss  0.007908 valid loss  0.007373\n",
      "epoch   56 train loss  0.006769 valid loss  0.006403\n",
      "epoch   57 train loss  0.005818 valid loss  0.005724\n",
      "epoch   58 train loss  0.005175 valid loss  0.005189\n",
      "epoch   59 train loss  0.004708 valid loss  0.004831\n",
      "epoch   60 train loss  0.004372 valid loss  0.004552\n",
      "epoch   61 train loss  0.004127 valid loss  0.004375\n",
      "epoch   62 train loss  0.003982 valid loss  0.004252\n",
      "epoch   63 train loss  0.003812 valid loss  0.004158\n",
      "epoch   64 train loss  0.003762 valid loss  0.004103\n",
      "epoch   65 train loss  0.003659 valid loss  0.004069\n",
      "epoch   66 train loss  0.003673 valid loss  0.004045\n",
      "epoch   67 train loss  0.003616 valid loss  0.004031\n",
      "epoch   68 train loss  0.003664 valid loss  0.004023\n",
      "epoch   69 train loss  0.003562 valid loss  0.004016\n",
      "epoch   70 train loss  0.003602 valid loss  0.004015\n",
      "epoch   71 train loss  0.003597 valid loss  0.004015\n",
      "epoch   72 train loss  0.003568 valid loss  0.004015\n",
      "epoch   73 train loss  0.003546 valid loss  0.004016\n",
      "epoch   74 train loss  0.003567 valid loss  0.004016\n",
      "epoch   75 train loss  0.003538 valid loss  0.004017\n",
      "epoch   76 train loss  0.003624 valid loss  0.004017\n",
      "epoch   77 train loss  0.003554 valid loss  0.004018\n",
      "epoch   78 train loss  0.003552 valid loss  0.004019\n",
      "epoch   79 train loss  0.003567 valid loss  0.004018\n",
      "epoch   80 train loss  0.003599 valid loss  0.004018\n",
      "epoch   81 train loss  0.003556 valid loss  0.004018\n",
      "epoch   82 train loss  0.003522 valid loss  0.004019\n",
      "epoch   83 train loss  0.003563 valid loss  0.004019\n",
      "epoch   84 train loss  0.003581 valid loss  0.004019\n",
      "epoch   85 train loss  0.003578 valid loss  0.004019\n",
      "epoch   86 train loss  0.003586 valid loss  0.004019\n",
      "epoch   87 train loss  0.003576 valid loss  0.004019\n",
      "epoch   88 train loss  0.003575 valid loss  0.004018\n",
      "epoch   89 train loss  0.003563 valid loss  0.004018\n",
      "epoch   90 train loss  0.003556 valid loss  0.004018\n",
      "epoch   91 train loss  0.003618 valid loss  0.004018\n",
      "epoch   92 train loss  0.003567 valid loss  0.004018\n",
      "epoch   93 train loss  0.003569 valid loss  0.004018\n",
      "epoch   94 train loss  0.003587 valid loss  0.004018\n",
      "epoch   95 train loss  0.003559 valid loss  0.004018\n",
      "epoch   96 train loss  0.003590 valid loss  0.004018\n",
      "epoch   97 train loss  0.003558 valid loss  0.004018\n",
      "epoch   98 train loss  0.003619 valid loss  0.004019\n",
      "epoch   99 train loss  0.003556 valid loss  0.004019\n",
      "epoch  100 train loss  0.003546 valid loss  0.004019\n",
      "epoch  101 train loss  0.003589 valid loss  0.004019\n",
      "epoch  102 train loss  0.003561 valid loss  0.004018\n",
      "epoch  103 train loss  0.003513 valid loss  0.004018\n",
      "epoch  104 train loss  0.003611 valid loss  0.004018\n",
      "epoch  105 train loss  0.003513 valid loss  0.004018\n",
      "epoch  106 train loss  0.003568 valid loss  0.004018\n",
      "epoch  107 train loss  0.003611 valid loss  0.004018\n",
      "epoch  108 train loss  0.003548 valid loss  0.004018\n",
      "epoch  109 train loss  0.003539 valid loss  0.004018\n",
      "epoch  110 train loss  0.003560 valid loss  0.004018\n",
      "epoch  111 train loss  0.003572 valid loss  0.004018\n",
      "epoch  112 train loss  0.003594 valid loss  0.004018\n",
      "epoch  113 train loss  0.003525 valid loss  0.004018\n",
      "epoch  114 train loss  0.003596 valid loss  0.004018\n",
      "epoch  115 train loss  0.003585 valid loss  0.004018\n",
      "epoch  116 train loss  0.003542 valid loss  0.004018\n",
      "epoch  117 train loss  0.003588 valid loss  0.004018\n",
      "epoch  118 train loss  0.003654 valid loss  0.004018\n",
      "epoch  119 train loss  0.003525 valid loss  0.004018\n",
      "epoch  120 train loss  0.003543 valid loss  0.004018\n",
      "epoch  121 train loss  0.003568 valid loss  0.004018\n",
      "epoch  122 train loss  0.003560 valid loss  0.004018\n",
      "epoch  123 train loss  0.003590 valid loss  0.004018\n",
      "epoch  124 train loss  0.003565 valid loss  0.004018\n",
      "epoch  125 train loss  0.003578 valid loss  0.004018\n",
      "epoch  126 train loss  0.003571 valid loss  0.004018\n",
      "epoch  127 train loss  0.003538 valid loss  0.004018\n",
      "epoch  128 train loss  0.003601 valid loss  0.004017\n",
      "epoch  129 train loss  0.003571 valid loss  0.004017\n",
      "epoch  130 train loss  0.003576 valid loss  0.004017\n",
      "epoch  131 train loss  0.003558 valid loss  0.004018\n",
      "epoch  132 train loss  0.003552 valid loss  0.004018\n",
      "epoch  133 train loss  0.003548 valid loss  0.004018\n",
      "epoch  134 train loss  0.003561 valid loss  0.004018\n",
      "epoch  135 train loss  0.003593 valid loss  0.004018\n",
      "epoch  136 train loss  0.003564 valid loss  0.004019\n",
      "epoch  137 train loss  0.003539 valid loss  0.004018\n",
      "epoch  138 train loss  0.003543 valid loss  0.004018\n",
      "epoch  139 train loss  0.003592 valid loss  0.004018\n",
      "epoch  140 train loss  0.003580 valid loss  0.004018\n",
      "epoch  141 train loss  0.003559 valid loss  0.004018\n",
      "epoch  142 train loss  0.003556 valid loss  0.004017\n",
      "epoch  143 train loss  0.003548 valid loss  0.004017\n",
      "epoch  144 train loss  0.003568 valid loss  0.004017\n",
      "epoch  145 train loss  0.003572 valid loss  0.004017\n",
      "epoch  146 train loss  0.003561 valid loss  0.004016\n",
      "epoch  147 train loss  0.003553 valid loss  0.004017\n",
      "epoch  148 train loss  0.003563 valid loss  0.004017\n",
      "epoch  149 train loss  0.003616 valid loss  0.004017\n",
      "epoch  150 train loss  0.003577 valid loss  0.004016\n",
      "epoch  151 train loss  0.003583 valid loss  0.004016\n",
      "epoch  152 train loss  0.003535 valid loss  0.004016\n",
      "epoch  153 train loss  0.003577 valid loss  0.004017\n",
      "epoch  154 train loss  0.003581 valid loss  0.004016\n",
      "epoch  155 train loss  0.003574 valid loss  0.004017\n",
      "epoch  156 train loss  0.003550 valid loss  0.004017\n",
      "epoch  157 train loss  0.003549 valid loss  0.004017\n",
      "epoch  158 train loss  0.003585 valid loss  0.004016\n",
      "epoch  159 train loss  0.003549 valid loss  0.004016\n",
      "epoch  160 train loss  0.003572 valid loss  0.004015\n",
      "epoch  161 train loss  0.003552 valid loss  0.004016\n",
      "epoch  162 train loss  0.003538 valid loss  0.004015\n",
      "epoch  163 train loss  0.003548 valid loss  0.004015\n",
      "epoch  164 train loss  0.003518 valid loss  0.004015\n",
      "epoch  165 train loss  0.003552 valid loss  0.004015\n",
      "epoch  166 train loss  0.003515 valid loss  0.004015\n",
      "epoch  167 train loss  0.003518 valid loss  0.004015\n",
      "epoch  168 train loss  0.003577 valid loss  0.004015\n",
      "epoch  169 train loss  0.003590 valid loss  0.004015\n",
      "epoch  170 train loss  0.003591 valid loss  0.004015\n",
      "epoch  171 train loss  0.003547 valid loss  0.004015\n",
      "epoch  172 train loss  0.003577 valid loss  0.004016\n",
      "epoch  173 train loss  0.003542 valid loss  0.004015\n",
      "epoch  174 train loss  0.003572 valid loss  0.004016\n",
      "epoch  175 train loss  0.003588 valid loss  0.004016\n",
      "epoch  176 train loss  0.003578 valid loss  0.004016\n",
      "epoch  177 train loss  0.003538 valid loss  0.004015\n",
      "epoch  178 train loss  0.003583 valid loss  0.004015\n",
      "epoch  179 train loss  0.003567 valid loss  0.004015\n",
      "epoch  180 train loss  0.003557 valid loss  0.004015\n",
      "epoch  181 train loss  0.003526 valid loss  0.004015\n",
      "epoch  182 train loss  0.003506 valid loss  0.004015\n",
      "epoch  183 train loss  0.003594 valid loss  0.004015\n",
      "epoch  184 train loss  0.003577 valid loss  0.004015\n",
      "epoch  185 train loss  0.003524 valid loss  0.004015\n",
      "epoch  186 train loss  0.003551 valid loss  0.004015\n",
      "epoch  187 train loss  0.003572 valid loss  0.004015\n",
      "epoch  188 train loss  0.003576 valid loss  0.004015\n",
      "epoch  189 train loss  0.003576 valid loss  0.004015\n",
      "epoch  190 train loss  0.003614 valid loss  0.004015\n",
      "epoch  191 train loss  0.003525 valid loss  0.004015\n",
      "epoch  192 train loss  0.003536 valid loss  0.004015\n",
      "epoch  193 train loss  0.003537 valid loss  0.004015\n",
      "epoch  194 train loss  0.003572 valid loss  0.004015\n",
      "epoch  195 train loss  0.003568 valid loss  0.004015\n",
      "epoch  196 train loss  0.003567 valid loss  0.004015\n",
      "epoch  197 train loss  0.003527 valid loss  0.004015\n",
      "epoch  198 train loss  0.003596 valid loss  0.004015\n",
      "epoch  199 train loss  0.003545 valid loss  0.004014\n",
      "epoch  200 train loss  0.003525 valid loss  0.004014\n",
      "epoch  201 train loss  0.003596 valid loss  0.004014\n",
      "epoch  202 train loss  0.003510 valid loss  0.004014\n",
      "epoch  203 train loss  0.003558 valid loss  0.004014\n",
      "epoch  204 train loss  0.003537 valid loss  0.004014\n",
      "epoch  205 train loss  0.003597 valid loss  0.004014\n",
      "epoch  206 train loss  0.003550 valid loss  0.004013\n",
      "epoch  207 train loss  0.003558 valid loss  0.004013\n",
      "epoch  208 train loss  0.003561 valid loss  0.004013\n",
      "epoch  209 train loss  0.003533 valid loss  0.004013\n",
      "epoch  210 train loss  0.003576 valid loss  0.004013\n",
      "epoch  211 train loss  0.003591 valid loss  0.004013\n",
      "epoch  212 train loss  0.003562 valid loss  0.004013\n",
      "epoch  213 train loss  0.003547 valid loss  0.004013\n",
      "epoch  214 train loss  0.003560 valid loss  0.004014\n",
      "epoch  215 train loss  0.003594 valid loss  0.004014\n",
      "epoch  216 train loss  0.003587 valid loss  0.004014\n",
      "epoch  217 train loss  0.003535 valid loss  0.004014\n",
      "epoch  218 train loss  0.003571 valid loss  0.004014\n",
      "epoch  219 train loss  0.003554 valid loss  0.004014\n",
      "epoch  220 train loss  0.003566 valid loss  0.004014\n",
      "epoch  221 train loss  0.003523 valid loss  0.004014\n",
      "epoch  222 train loss  0.003558 valid loss  0.004014\n",
      "epoch  223 train loss  0.003538 valid loss  0.004013\n",
      "epoch  224 train loss  0.003551 valid loss  0.004013\n",
      "epoch  225 train loss  0.003553 valid loss  0.004013\n",
      "epoch  226 train loss  0.003573 valid loss  0.004013\n",
      "epoch  227 train loss  0.003541 valid loss  0.004013\n",
      "epoch  228 train loss  0.003513 valid loss  0.004013\n",
      "epoch  229 train loss  0.003602 valid loss  0.004013\n",
      "epoch  230 train loss  0.003582 valid loss  0.004013\n",
      "epoch  231 train loss  0.003584 valid loss  0.004013\n",
      "epoch  232 train loss  0.003542 valid loss  0.004013\n",
      "epoch  233 train loss  0.003560 valid loss  0.004013\n",
      "epoch  234 train loss  0.003514 valid loss  0.004013\n",
      "epoch  235 train loss  0.003531 valid loss  0.004013\n",
      "epoch  236 train loss  0.003608 valid loss  0.004012\n",
      "epoch  237 train loss  0.003578 valid loss  0.004012\n",
      "epoch  238 train loss  0.003569 valid loss  0.004012\n",
      "epoch  239 train loss  0.003561 valid loss  0.004012\n",
      "epoch  240 train loss  0.003602 valid loss  0.004011\n",
      "epoch  241 train loss  0.003557 valid loss  0.004012\n",
      "epoch  242 train loss  0.003594 valid loss  0.004012\n",
      "epoch  243 train loss  0.003598 valid loss  0.004012\n",
      "epoch  244 train loss  0.003566 valid loss  0.004011\n",
      "epoch  245 train loss  0.003569 valid loss  0.004011\n",
      "epoch  246 train loss  0.003545 valid loss  0.004012\n",
      "epoch  247 train loss  0.003577 valid loss  0.004011\n",
      "epoch  248 train loss  0.003510 valid loss  0.004011\n",
      "epoch  249 train loss  0.003571 valid loss  0.004011\n",
      "epoch  250 train loss  0.003575 valid loss  0.004011\n",
      "epoch  251 train loss  0.003512 valid loss  0.004011\n",
      "epoch  252 train loss  0.003577 valid loss  0.004011\n",
      "epoch  253 train loss  0.003604 valid loss  0.004011\n",
      "epoch  254 train loss  0.003559 valid loss  0.004011\n",
      "epoch  255 train loss  0.003557 valid loss  0.004011\n",
      "epoch  256 train loss  0.003581 valid loss  0.004011\n",
      "epoch  257 train loss  0.003567 valid loss  0.004011\n",
      "epoch  258 train loss  0.003550 valid loss  0.004011\n",
      "epoch  259 train loss  0.003530 valid loss  0.004011\n",
      "epoch  260 train loss  0.003545 valid loss  0.004011\n",
      "epoch  261 train loss  0.003581 valid loss  0.004011\n",
      "epoch  262 train loss  0.003521 valid loss  0.004011\n",
      "epoch  263 train loss  0.003552 valid loss  0.004011\n",
      "epoch  264 train loss  0.003599 valid loss  0.004011\n",
      "epoch  265 train loss  0.003589 valid loss  0.004011\n",
      "epoch  266 train loss  0.003575 valid loss  0.004011\n",
      "epoch  267 train loss  0.003507 valid loss  0.004011\n",
      "epoch  268 train loss  0.003593 valid loss  0.004010\n",
      "epoch  269 train loss  0.003567 valid loss  0.004010\n",
      "epoch  270 train loss  0.003571 valid loss  0.004010\n",
      "epoch  271 train loss  0.003579 valid loss  0.004010\n",
      "epoch  272 train loss  0.003599 valid loss  0.004010\n",
      "epoch  273 train loss  0.003506 valid loss  0.004010\n",
      "epoch  274 train loss  0.003553 valid loss  0.004010\n",
      "epoch  275 train loss  0.003574 valid loss  0.004010\n",
      "epoch  276 train loss  0.003541 valid loss  0.004010\n",
      "epoch  277 train loss  0.003586 valid loss  0.004010\n",
      "epoch  278 train loss  0.003522 valid loss  0.004010\n",
      "epoch  279 train loss  0.003581 valid loss  0.004010\n",
      "epoch  280 train loss  0.003567 valid loss  0.004010\n",
      "epoch  281 train loss  0.003598 valid loss  0.004010\n",
      "epoch  282 train loss  0.003577 valid loss  0.004010\n",
      "epoch  283 train loss  0.003542 valid loss  0.004009\n",
      "epoch  284 train loss  0.003592 valid loss  0.004009\n",
      "epoch  285 train loss  0.003591 valid loss  0.004010\n",
      "epoch  286 train loss  0.003587 valid loss  0.004010\n",
      "epoch  287 train loss  0.003551 valid loss  0.004010\n",
      "epoch  288 train loss  0.003573 valid loss  0.004010\n",
      "epoch  289 train loss  0.003535 valid loss  0.004009\n",
      "epoch  290 train loss  0.003536 valid loss  0.004009\n",
      "epoch  291 train loss  0.003555 valid loss  0.004009\n",
      "epoch  292 train loss  0.003535 valid loss  0.004008\n",
      "epoch  293 train loss  0.003549 valid loss  0.004008\n",
      "epoch  294 train loss  0.003574 valid loss  0.004008\n",
      "epoch  295 train loss  0.003573 valid loss  0.004008\n",
      "epoch  296 train loss  0.003561 valid loss  0.004008\n",
      "epoch  297 train loss  0.003551 valid loss  0.004009\n",
      "epoch  298 train loss  0.003588 valid loss  0.004008\n",
      "epoch  299 train loss  0.003619 valid loss  0.004008\n",
      "epoch  300 train loss  0.003595 valid loss  0.004009\n",
      "epoch  301 train loss  0.003556 valid loss  0.004009\n",
      "epoch  302 train loss  0.003537 valid loss  0.004008\n",
      "epoch  303 train loss  0.003577 valid loss  0.004009\n",
      "epoch  304 train loss  0.003614 valid loss  0.004008\n",
      "epoch  305 train loss  0.003526 valid loss  0.004008\n",
      "epoch  306 train loss  0.003559 valid loss  0.004008\n",
      "epoch  307 train loss  0.003597 valid loss  0.004008\n",
      "epoch  308 train loss  0.003543 valid loss  0.004008\n",
      "epoch  309 train loss  0.003551 valid loss  0.004009\n",
      "epoch  310 train loss  0.003555 valid loss  0.004009\n",
      "epoch  311 train loss  0.003547 valid loss  0.004009\n",
      "epoch  312 train loss  0.003560 valid loss  0.004008\n",
      "epoch  313 train loss  0.003583 valid loss  0.004008\n",
      "epoch  314 train loss  0.003541 valid loss  0.004007\n",
      "epoch  315 train loss  0.003532 valid loss  0.004007\n",
      "epoch  316 train loss  0.003560 valid loss  0.004007\n",
      "epoch  317 train loss  0.003515 valid loss  0.004007\n",
      "epoch  318 train loss  0.003571 valid loss  0.004006\n",
      "epoch  319 train loss  0.003591 valid loss  0.004007\n",
      "epoch  320 train loss  0.003552 valid loss  0.004007\n",
      "epoch  321 train loss  0.003563 valid loss  0.004007\n",
      "epoch  322 train loss  0.003556 valid loss  0.004007\n",
      "epoch  323 train loss  0.003553 valid loss  0.004007\n",
      "epoch  324 train loss  0.003518 valid loss  0.004006\n",
      "epoch  325 train loss  0.003516 valid loss  0.004006\n",
      "epoch  326 train loss  0.003588 valid loss  0.004006\n",
      "epoch  327 train loss  0.003523 valid loss  0.004006\n",
      "epoch  328 train loss  0.003572 valid loss  0.004006\n",
      "epoch  329 train loss  0.003579 valid loss  0.004006\n",
      "epoch  330 train loss  0.003561 valid loss  0.004005\n",
      "epoch  331 train loss  0.003599 valid loss  0.004005\n",
      "epoch  332 train loss  0.003513 valid loss  0.004006\n",
      "epoch  333 train loss  0.003533 valid loss  0.004005\n",
      "epoch  334 train loss  0.003547 valid loss  0.004005\n",
      "epoch  335 train loss  0.003583 valid loss  0.004006\n",
      "epoch  336 train loss  0.003528 valid loss  0.004006\n",
      "epoch  337 train loss  0.003533 valid loss  0.004006\n",
      "epoch  338 train loss  0.003575 valid loss  0.004005\n",
      "epoch  339 train loss  0.003562 valid loss  0.004005\n",
      "epoch  340 train loss  0.003572 valid loss  0.004005\n",
      "epoch  341 train loss  0.003571 valid loss  0.004005\n",
      "epoch  342 train loss  0.003518 valid loss  0.004005\n",
      "epoch  343 train loss  0.003556 valid loss  0.004005\n",
      "epoch  344 train loss  0.003594 valid loss  0.004005\n",
      "epoch  345 train loss  0.003556 valid loss  0.004005\n",
      "epoch  346 train loss  0.003554 valid loss  0.004005\n",
      "epoch  347 train loss  0.003524 valid loss  0.004005\n",
      "epoch  348 train loss  0.003557 valid loss  0.004005\n",
      "epoch  349 train loss  0.003528 valid loss  0.004005\n",
      "epoch  350 train loss  0.003546 valid loss  0.004005\n",
      "epoch  351 train loss  0.003552 valid loss  0.004005\n",
      "epoch  352 train loss  0.003556 valid loss  0.004005\n",
      "epoch  353 train loss  0.003559 valid loss  0.004005\n",
      "epoch  354 train loss  0.003608 valid loss  0.004005\n",
      "epoch  355 train loss  0.003576 valid loss  0.004005\n",
      "epoch  356 train loss  0.003541 valid loss  0.004004\n",
      "epoch  357 train loss  0.003526 valid loss  0.004003\n",
      "epoch  358 train loss  0.003511 valid loss  0.004002\n",
      "epoch  359 train loss  0.003562 valid loss  0.004002\n",
      "epoch  360 train loss  0.003556 valid loss  0.004002\n",
      "epoch  361 train loss  0.003543 valid loss  0.004002\n",
      "epoch  362 train loss  0.003567 valid loss  0.004002\n",
      "epoch  363 train loss  0.003577 valid loss  0.004002\n",
      "epoch  364 train loss  0.003556 valid loss  0.004002\n",
      "epoch  365 train loss  0.003581 valid loss  0.004002\n",
      "epoch  366 train loss  0.003524 valid loss  0.004003\n",
      "epoch  367 train loss  0.003547 valid loss  0.004004\n",
      "epoch  368 train loss  0.003544 valid loss  0.004004\n",
      "epoch  369 train loss  0.003551 valid loss  0.004003\n",
      "epoch  370 train loss  0.003544 valid loss  0.004002\n",
      "epoch  371 train loss  0.003574 valid loss  0.004002\n",
      "epoch  372 train loss  0.003526 valid loss  0.004002\n",
      "epoch  373 train loss  0.003560 valid loss  0.004002\n",
      "epoch  374 train loss  0.003579 valid loss  0.004002\n",
      "epoch  375 train loss  0.003588 valid loss  0.004002\n",
      "epoch  376 train loss  0.003576 valid loss  0.004002\n",
      "epoch  377 train loss  0.003605 valid loss  0.004002\n",
      "epoch  378 train loss  0.003515 valid loss  0.004002\n",
      "epoch  379 train loss  0.003558 valid loss  0.004002\n",
      "epoch  380 train loss  0.003560 valid loss  0.004002\n",
      "epoch  381 train loss  0.003561 valid loss  0.004002\n",
      "epoch  382 train loss  0.003542 valid loss  0.004001\n",
      "epoch  383 train loss  0.003545 valid loss  0.004001\n",
      "epoch  384 train loss  0.003523 valid loss  0.004001\n",
      "epoch  385 train loss  0.003561 valid loss  0.004001\n",
      "epoch  386 train loss  0.003557 valid loss  0.004001\n",
      "epoch  387 train loss  0.003547 valid loss  0.004001\n",
      "epoch  388 train loss  0.003544 valid loss  0.004001\n",
      "epoch  389 train loss  0.003507 valid loss  0.004001\n",
      "epoch  390 train loss  0.003537 valid loss  0.004001\n",
      "epoch  391 train loss  0.003563 valid loss  0.004001\n",
      "epoch  392 train loss  0.003571 valid loss  0.004001\n",
      "epoch  393 train loss  0.003539 valid loss  0.004001\n",
      "epoch  394 train loss  0.003608 valid loss  0.004001\n",
      "epoch  395 train loss  0.003534 valid loss  0.004001\n",
      "epoch  396 train loss  0.003548 valid loss  0.004001\n",
      "epoch  397 train loss  0.003553 valid loss  0.004000\n",
      "epoch  398 train loss  0.003538 valid loss  0.004000\n",
      "epoch  399 train loss  0.003534 valid loss  0.004000\n",
      "epoch  400 train loss  0.003545 valid loss  0.004000\n",
      "epoch  401 train loss  0.003572 valid loss  0.004000\n",
      "epoch  402 train loss  0.003567 valid loss  0.004000\n",
      "epoch  403 train loss  0.003576 valid loss  0.004000\n",
      "epoch  404 train loss  0.003546 valid loss  0.004000\n",
      "epoch  405 train loss  0.003566 valid loss  0.004000\n",
      "epoch  406 train loss  0.003564 valid loss  0.004000\n",
      "epoch  407 train loss  0.003586 valid loss  0.004000\n",
      "epoch  408 train loss  0.003575 valid loss  0.004000\n",
      "epoch  409 train loss  0.003552 valid loss  0.004000\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Dipole_Moment()\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from typing import List\n",
    "\n",
    "def create_loss_plot(\n",
    "    train_loss: List[np.ndarray],\n",
    "    val_loss: List[np.ndarray],\n",
    "    train_label: str,\n",
    "    val_label: str,\n",
    "    title: str,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a Plotly figure with training and validation loss curves and save it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        train_loss (List[np.ndarray]): List of training loss values.\n",
    "        val_loss (List[np.ndarray]): List of validation loss values.\n",
    "        train_label (str): Label for the training loss curve.\n",
    "        val_label (str): Label for the validation loss curve.\n",
    "        title (str): Title of the plot.\n",
    "        filename (str): Filename to save the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_loss_list = [float(loss) for loss in train_loss]\n",
    "    val_loss_list = [float(loss) for loss in val_loss]\n",
    "\n",
    "    trace_train = go.Scatter(y=train_loss_list, mode=\"lines\", name=train_label)\n",
    "    trace_val = go.Scatter(y=val_loss_list, mode=\"lines\", name=val_label)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(trace_train)\n",
    "    fig.add_trace(trace_val)\n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title=\"Epoch\", yaxis_title=\"Loss\", legend_title=\"Legend\"\n",
    "    )\n",
    "    pio.write_html(fig, filename)\n",
    "\n",
    "\n",
    "create_loss_plot(\n",
    "    list_train_loss,\n",
    "    list_val_loss,\n",
    "    \"Training Loss\",\n",
    "    \"Validation Loss\",\n",
    "    \"Training vs Validation Loss (Train)\",\n",
    "    \"train_vs_val_train_dipole_moment.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[ 1.4647644  -0.30743074  1.7500215 ]\n",
      "mean squared error 0.0022456348\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
