{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment of inertia tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "\n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "\n",
    "  # Calculate moment of inertia tensors.\n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "\n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2,\n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "\n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "\n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['masses'][0:1].shape)\\nprint(train_data['positions'][0:1].shape)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['masses'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 train loss  1.359933 valid loss  0.650806\n",
      "epoch    2 train loss  0.471154 valid loss  0.361696\n",
      "epoch    3 train loss  0.355795 valid loss  0.330647\n",
      "epoch    4 train loss  0.335975 valid loss  0.313806\n",
      "epoch    5 train loss  0.313707 valid loss  0.307905\n",
      "epoch    6 train loss  0.295819 valid loss  0.261203\n",
      "epoch    7 train loss  0.269274 valid loss  0.236152\n",
      "epoch    8 train loss  0.247977 valid loss  0.230414\n",
      "epoch    9 train loss  0.231734 valid loss  0.205375\n",
      "epoch   10 train loss  0.225083 valid loss  0.209193\n",
      "epoch   11 train loss  0.207602 valid loss  0.188981\n",
      "epoch   12 train loss  0.200761 valid loss  0.185399\n",
      "epoch   13 train loss  0.190793 valid loss  0.175384\n",
      "epoch   14 train loss  0.178643 valid loss  0.169232\n",
      "epoch   15 train loss  0.161587 valid loss  0.144264\n",
      "epoch   16 train loss  0.147181 valid loss  0.133549\n",
      "epoch   17 train loss  0.129426 valid loss  0.109603\n",
      "epoch   18 train loss  0.105608 valid loss  0.088042\n",
      "epoch   19 train loss  0.089911 valid loss  0.065109\n",
      "epoch   20 train loss  0.063822 valid loss  0.046581\n",
      "epoch   21 train loss  0.042836 valid loss  0.039833\n",
      "epoch   22 train loss  0.045359 valid loss  0.037879\n",
      "epoch   23 train loss  0.040164 valid loss  0.048566\n",
      "epoch   24 train loss  0.041613 valid loss  0.038852\n",
      "epoch   25 train loss  0.037659 valid loss  0.036376\n",
      "epoch   26 train loss  0.034417 valid loss  0.038344\n",
      "epoch   27 train loss  0.035188 valid loss  0.030785\n",
      "epoch   28 train loss  0.033791 valid loss  0.031197\n",
      "epoch   29 train loss  0.033894 valid loss  0.027737\n",
      "epoch   30 train loss  0.033661 valid loss  0.030736\n",
      "epoch   31 train loss  0.031063 valid loss  0.025989\n",
      "epoch   32 train loss  0.029492 valid loss  0.026486\n",
      "epoch   33 train loss  0.029381 valid loss  0.024327\n",
      "epoch   34 train loss  0.028494 valid loss  0.023874\n",
      "epoch   35 train loss  0.029745 valid loss  0.028929\n",
      "epoch   36 train loss  0.028127 valid loss  0.023015\n",
      "epoch   37 train loss  0.026591 valid loss  0.024094\n",
      "epoch   38 train loss  0.030150 valid loss  0.028875\n",
      "epoch   39 train loss  0.029794 valid loss  0.023051\n",
      "epoch   40 train loss  0.031116 valid loss  0.028424\n",
      "epoch   41 train loss  0.028212 valid loss  0.021998\n",
      "epoch   42 train loss  0.025479 valid loss  0.021279\n",
      "epoch   43 train loss  0.025208 valid loss  0.024125\n",
      "epoch   44 train loss  0.026061 valid loss  0.021150\n",
      "epoch   45 train loss  0.033841 valid loss  0.057384\n",
      "epoch   46 train loss  0.027158 valid loss  0.020290\n",
      "epoch   47 train loss  0.023987 valid loss  0.019953\n",
      "epoch   48 train loss  0.024759 valid loss  0.024592\n",
      "epoch   49 train loss  0.025928 valid loss  0.024374\n",
      "epoch   50 train loss  0.023460 valid loss  0.018815\n",
      "epoch   51 train loss  0.024572 valid loss  0.019993\n",
      "epoch   52 train loss  0.022887 valid loss  0.018372\n",
      "epoch   53 train loss  0.026181 valid loss  0.025382\n",
      "epoch   54 train loss  0.025671 valid loss  0.021562\n",
      "epoch   55 train loss  0.023084 valid loss  0.017371\n",
      "epoch   56 train loss  0.024710 valid loss  0.019425\n",
      "epoch   57 train loss  0.029084 valid loss  0.029276\n",
      "epoch   58 train loss  0.022432 valid loss  0.015814\n",
      "epoch   59 train loss  0.020231 valid loss  0.016195\n",
      "epoch   60 train loss  0.018062 valid loss  0.014702\n",
      "epoch   61 train loss  0.016403 valid loss  0.013027\n",
      "epoch   62 train loss  0.016155 valid loss  0.012115\n",
      "epoch   63 train loss  0.013847 valid loss  0.010955\n",
      "epoch   64 train loss  0.014790 valid loss  0.011325\n",
      "epoch   65 train loss  0.019167 valid loss  0.010374\n",
      "epoch   66 train loss  0.012386 valid loss  0.007711\n",
      "epoch   67 train loss  0.009181 valid loss  0.006592\n",
      "epoch   68 train loss  0.006980 valid loss  0.005574\n",
      "epoch   69 train loss  0.005331 valid loss  0.004197\n",
      "epoch   70 train loss  0.006010 valid loss  0.013546\n",
      "epoch   71 train loss  0.004674 valid loss  0.004104\n",
      "epoch   72 train loss  0.002438 valid loss  0.001107\n",
      "epoch   73 train loss  0.001398 valid loss  0.000977\n",
      "epoch   74 train loss  0.001085 valid loss  0.000696\n",
      "epoch   75 train loss  0.000974 valid loss  0.000871\n",
      "epoch   76 train loss  0.000811 valid loss  0.000642\n",
      "epoch   77 train loss  0.000723 valid loss  0.000854\n",
      "epoch   78 train loss  0.000840 valid loss  0.001953\n",
      "epoch   79 train loss  0.000751 valid loss  0.000519\n",
      "epoch   80 train loss  0.000636 valid loss  0.000593\n",
      "epoch   81 train loss  0.000609 valid loss  0.000431\n",
      "epoch   82 train loss  0.000456 valid loss  0.000393\n",
      "epoch   83 train loss  0.000414 valid loss  0.000432\n",
      "epoch   84 train loss  0.000414 valid loss  0.000325\n",
      "epoch   85 train loss  0.000374 valid loss  0.000378\n",
      "epoch   86 train loss  0.000338 valid loss  0.000319\n",
      "epoch   87 train loss  0.000391 valid loss  0.000368\n",
      "epoch   88 train loss  0.000297 valid loss  0.000303\n",
      "epoch   89 train loss  0.000289 valid loss  0.000255\n",
      "epoch   90 train loss  0.000279 valid loss  0.000331\n",
      "epoch   91 train loss  0.000330 valid loss  0.000231\n",
      "epoch   92 train loss  0.000239 valid loss  0.000275\n",
      "epoch   93 train loss  0.000244 valid loss  0.000312\n",
      "epoch   94 train loss  0.000205 valid loss  0.000246\n",
      "epoch   95 train loss  0.000211 valid loss  0.000307\n",
      "epoch   96 train loss  0.000207 valid loss  0.000279\n",
      "epoch   97 train loss  0.000195 valid loss  0.000168\n",
      "epoch   98 train loss  0.000177 valid loss  0.000205\n",
      "epoch   99 train loss  0.000346 valid loss  0.000148\n",
      "epoch  100 train loss  0.000156 valid loss  0.000148\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "[[ 6.013584    1.6290329  -0.17871115]\n",
      " [ 1.6290329   4.8540945   0.73430276]\n",
      " [-0.17871115  0.73430276  6.1854286 ]]\n",
      "prediction\n",
      "[[ 6.016639    1.6341114  -0.1779103 ]\n",
      " [ 1.634112    4.84913     0.73786885]\n",
      " [-0.17790689  0.73786646  6.182285  ]]\n",
      "mean squared error 1.3572933e-05\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_data.npz\"\n",
    "dataset= np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print('Dipole moment shape array',dataset['D'].shape)\n",
    "print('Dipole moment units', dataset['D_units'])\n",
    "\n",
    "print('Atomic numbers', dataset['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z=jnp.full(16,14)\n",
    "    Z=jnp.append(Z,23)\n",
    "    Z=jnp.expand_dims(Z,axis=0)\n",
    "    Z=jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train=1000\n",
    "num_val=200\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "print(train_data['dipole_moment'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "  #features = 1\n",
    "  #max_degree = 1\n",
    "  @nn.compact\n",
    "  def __call__(self,atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features\n",
    "    '''dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "    print('dst_idx',dst_idx.shape)\n",
    "    dst_idx = jnp.expand_dims(dst_idx, axis=0)  \n",
    "    src_idx = jnp.expand_dims(src_idx, axis=0)  \n",
    "    dst_idx = jnp.repeat(dst_idx, positions.shape[0], axis=0)\n",
    "    src_idx = jnp.repeat(src_idx, positions.shape[0], axis=0)\n",
    "    print('dst_idx after expansion',dst_idx.shape)\n",
    "    positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "    positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "    displacements = positions_src - positions_dst\n",
    "    print('positions',positions.shape)\n",
    "    print('displacements',displacements.shape)\n",
    "    #x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "    Z= e3x.nn.Embed(num_embeddings=23, features=272)(atomic_numbers)\n",
    "    print('atomic_numbers',atomic_numbers.shape)\n",
    "    print('Z',Z.shape)\n",
    "    Z=Z[0,...]\n",
    "    Z=jnp.reshape(Z, (272, 17, 1))\n",
    "    print('Z',Z.shape)\n",
    "    x = jnp.concatenate((Z, displacements), axis=-1) '''\n",
    "\n",
    "    positions -= positions[17,...]\n",
    "    print(positions[17,...])\n",
    "    x = jnp.concatenate((atomic_numbers[...,None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    #print(\"Initial shape:\", x.shape)\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "    #print(\"x shape:\", x.shape)\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=32)(x) \n",
    "    #print(\"After Dense layer:\", x.shape)\n",
    "    x = e3x.nn.TensorDense(features=1,max_degree=1)(x)  \n",
    "    #print(\"After TensorDense layer:\", x.shape)\n",
    "    #x = e3x.nn.Dense(features=16)(x) \n",
    "    #x = e3x.nn.Dense(features=1)(x) \n",
    "    x=jnp.sum(x, axis=-4) \n",
    "    #print(\"After sum:\", x.shape)\n",
    "    #x=jnp.sum(x, axis=0)\n",
    "    #print(\"After second sum:\", x.shape)\n",
    "    y = x[..., 1, 1:4, 0]\n",
    "    #y = x[1, 1:4, 0]\n",
    "    #y=y[None,...]\n",
    "    #print(\"After slicing:\", y.shape)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dm_model = Dipole_Moment()\\nkey = jax.random.PRNGKey(0)\\n\\n# Generate train and test datasets.\\nkey, data_key = jax.random.split(key)\\ntrain_data, valid_data = generate_datasets(data_key)\\nparams = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nmoment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nprint(moment.shape)\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dm_model = Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "params = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "moment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "print(moment.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole_moment = model_apply(params,batch['atomic_numbers'] ,batch['positions'])\n",
    "    loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  dipole_moment = model_apply(params,batch['atomic_numbers'],batch['positions'])\n",
    "  loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key,train_data['atomic_numbers'][0:1],train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['positions'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  list_train_loss = []\n",
    "  list_val_loss = []\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    #print(perms)\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      #print(batch['dipole_moment'].shape)\n",
    "\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "      \n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "    list_val_loss.append(valid_loss)\n",
    "    list_train_loss.append(train_loss)\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params ,list_train_loss , list_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 17)\n",
      "(1, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['atomic_numbers'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_train=3000\n",
    "num_val=200\n",
    "train_data, valid_data = prepare_datasets(filename,key, num_train,num_val)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 200\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['positions'][0:1].shape)\\nprint(train_data['atomic_numbers'][0:1].shape)\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['positions'][0:1].shape)\n",
    "print(train_data['atomic_numbers'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Traced<ShapedArray(float32[17,3])>with<DynamicJaxprTrace(level=1/0)>\n",
      "epoch    1 train loss  491.962830 valid loss  191.331482\n",
      "epoch    2 train loss  142.879074 valid loss  46.414974\n",
      "epoch    3 train loss  29.500174 valid loss  6.130593\n",
      "epoch    4 train loss  2.720644 valid loss  1.121237\n",
      "epoch    5 train loss  3.057379 valid loss  4.690260\n",
      "epoch    6 train loss  6.642517 valid loss  5.940982\n",
      "epoch    7 train loss  7.369856 valid loss  3.901133\n",
      "epoch    8 train loss  3.548134 valid loss  1.770712\n",
      "epoch    9 train loss  1.765145 valid loss  0.895233\n",
      "epoch   10 train loss  0.981283 valid loss  0.968775\n",
      "epoch   11 train loss  0.972482 valid loss  1.262055\n",
      "epoch   12 train loss  1.116208 valid loss  1.359356\n",
      "epoch   13 train loss  1.165659 valid loss  1.264185\n",
      "epoch   14 train loss  1.267178 valid loss  1.047666\n",
      "epoch   15 train loss  0.888810 valid loss  0.902192\n",
      "epoch   16 train loss  0.883614 valid loss  0.856868\n",
      "epoch   17 train loss  0.892286 valid loss  0.852054\n",
      "epoch   18 train loss  0.852058 valid loss  0.853123\n",
      "epoch   19 train loss  0.937395 valid loss  0.852038\n",
      "epoch   20 train loss  0.918059 valid loss  0.861949\n",
      "epoch   21 train loss  0.877296 valid loss  0.896360\n",
      "epoch   22 train loss  0.872180 valid loss  0.932916\n",
      "epoch   23 train loss  0.928681 valid loss  0.933055\n",
      "epoch   24 train loss  0.917721 valid loss  0.887585\n",
      "epoch   25 train loss  0.879190 valid loss  0.854550\n",
      "epoch   26 train loss  0.935225 valid loss  0.852081\n",
      "epoch   27 train loss  0.915077 valid loss  0.855725\n",
      "epoch   28 train loss  0.880191 valid loss  0.869753\n",
      "epoch   29 train loss  0.880219 valid loss  0.879763\n",
      "epoch   30 train loss  0.878995 valid loss  0.881607\n",
      "epoch   31 train loss  0.883173 valid loss  0.885413\n",
      "epoch   32 train loss  0.876157 valid loss  0.883124\n",
      "epoch   33 train loss  0.880775 valid loss  0.875277\n",
      "epoch   34 train loss  0.892013 valid loss  0.867848\n",
      "epoch   35 train loss  0.876534 valid loss  0.869398\n",
      "epoch   36 train loss  0.884761 valid loss  0.876614\n",
      "epoch   37 train loss  0.879660 valid loss  0.871498\n",
      "epoch   38 train loss  0.883677 valid loss  0.872584\n",
      "epoch   39 train loss  0.881087 valid loss  0.882842\n",
      "epoch   40 train loss  0.888452 valid loss  0.865949\n",
      "epoch   41 train loss  0.889419 valid loss  0.875926\n",
      "epoch   42 train loss  0.888115 valid loss  0.883898\n",
      "epoch   43 train loss  0.881122 valid loss  0.880936\n",
      "epoch   44 train loss  0.875439 valid loss  0.881958\n",
      "epoch   45 train loss  0.884195 valid loss  0.894435\n",
      "epoch   46 train loss  0.859164 valid loss  0.907401\n",
      "epoch   47 train loss  0.899306 valid loss  0.916331\n",
      "epoch   48 train loss  0.862385 valid loss  0.906780\n",
      "epoch   49 train loss  0.890927 valid loss  0.901462\n",
      "epoch   50 train loss  0.888434 valid loss  0.875101\n",
      "epoch   51 train loss  0.876344 valid loss  0.893557\n",
      "epoch   52 train loss  0.895055 valid loss  0.879260\n",
      "epoch   53 train loss  0.870589 valid loss  0.857351\n",
      "epoch   54 train loss  0.888255 valid loss  0.852044\n",
      "epoch   55 train loss  0.904474 valid loss  0.852990\n",
      "epoch   56 train loss  0.867213 valid loss  0.860067\n",
      "epoch   57 train loss  0.886099 valid loss  0.877549\n",
      "epoch   58 train loss  0.876787 valid loss  0.913336\n",
      "epoch   59 train loss  0.914028 valid loss  0.891868\n",
      "epoch   60 train loss  0.878490 valid loss  0.865005\n",
      "epoch   61 train loss  0.885971 valid loss  0.864146\n",
      "epoch   62 train loss  0.878902 valid loss  0.868369\n",
      "epoch   63 train loss  0.884401 valid loss  0.861674\n",
      "epoch   64 train loss  0.877939 valid loss  0.858870\n",
      "epoch   65 train loss  0.886201 valid loss  0.870560\n",
      "epoch   66 train loss  0.877585 valid loss  0.885633\n",
      "epoch   67 train loss  0.905396 valid loss  0.914860\n",
      "epoch   68 train loss  0.880637 valid loss  0.888784\n",
      "epoch   69 train loss  0.883910 valid loss  0.876840\n",
      "epoch   70 train loss  0.879489 valid loss  0.863059\n",
      "epoch   71 train loss  0.883821 valid loss  0.868506\n",
      "epoch   72 train loss  0.877757 valid loss  0.893165\n",
      "epoch   73 train loss  0.856693 valid loss  0.941786\n",
      "epoch   74 train loss  0.989479 valid loss  0.863913\n",
      "epoch   75 train loss  0.904682 valid loss  0.851768\n",
      "epoch   76 train loss  0.860067 valid loss  0.851700\n",
      "epoch   77 train loss  0.797290 valid loss  0.853272\n",
      "epoch   78 train loss  0.911814 valid loss  0.851752\n",
      "epoch   79 train loss  0.896304 valid loss  0.859731\n",
      "epoch   80 train loss  0.877656 valid loss  0.898563\n",
      "epoch   81 train loss  0.881929 valid loss  0.960044\n",
      "epoch   82 train loss  0.881149 valid loss  0.948096\n",
      "epoch   83 train loss  0.886494 valid loss  0.932359\n",
      "epoch   84 train loss  0.863645 valid loss  0.902349\n",
      "epoch   85 train loss  0.886240 valid loss  0.881416\n",
      "epoch   86 train loss  0.880544 valid loss  0.854910\n",
      "epoch   87 train loss  0.884808 valid loss  0.852025\n",
      "epoch   88 train loss  0.915400 valid loss  0.872973\n",
      "epoch   89 train loss  0.880359 valid loss  0.912031\n",
      "epoch   90 train loss  0.915633 valid loss  0.890803\n",
      "epoch   91 train loss  0.884299 valid loss  0.855289\n",
      "epoch   92 train loss  0.855570 valid loss  0.852705\n",
      "epoch   93 train loss  0.895157 valid loss  0.851878\n",
      "epoch   94 train loss  0.821942 valid loss  0.851631\n",
      "epoch   95 train loss  0.914830 valid loss  0.855286\n",
      "epoch   96 train loss  0.858385 valid loss  0.860753\n",
      "epoch   97 train loss  0.888911 valid loss  0.872777\n",
      "epoch   98 train loss  0.870609 valid loss  0.914521\n",
      "epoch   99 train loss  0.901880 valid loss  0.937570\n",
      "epoch  100 train loss  0.877044 valid loss  0.904852\n",
      "epoch  101 train loss  0.879786 valid loss  0.879706\n",
      "epoch  102 train loss  0.877069 valid loss  0.860904\n",
      "epoch  103 train loss  0.899638 valid loss  0.851425\n",
      "epoch  104 train loss  0.923352 valid loss  0.873868\n",
      "epoch  105 train loss  0.868387 valid loss  0.944320\n",
      "epoch  106 train loss  0.896783 valid loss  0.985141\n",
      "epoch  107 train loss  0.887572 valid loss  0.929966\n",
      "epoch  108 train loss  0.890218 valid loss  0.882179\n",
      "epoch  109 train loss  0.870920 valid loss  0.852286\n",
      "epoch  110 train loss  0.914295 valid loss  0.851485\n",
      "epoch  111 train loss  0.863574 valid loss  0.856933\n",
      "epoch  112 train loss  0.893477 valid loss  0.856738\n",
      "epoch  113 train loss  0.889484 valid loss  0.888112\n",
      "epoch  114 train loss  0.899454 valid loss  0.875963\n",
      "epoch  115 train loss  0.874235 valid loss  0.852616\n",
      "epoch  116 train loss  0.917092 valid loss  0.852710\n",
      "epoch  117 train loss  0.892694 valid loss  0.886670\n",
      "epoch  118 train loss  0.897382 valid loss  0.914744\n",
      "epoch  119 train loss  0.936271 valid loss  0.851202\n",
      "epoch  120 train loss  0.920322 valid loss  0.855946\n",
      "epoch  121 train loss  0.941783 valid loss  0.870479\n",
      "epoch  122 train loss  0.883535 valid loss  0.944091\n",
      "epoch  123 train loss  0.916123 valid loss  0.914941\n",
      "epoch  124 train loss  0.885430 valid loss  0.864729\n",
      "epoch  125 train loss  0.877204 valid loss  0.851620\n",
      "epoch  126 train loss  0.906215 valid loss  0.856731\n",
      "epoch  127 train loss  0.875403 valid loss  0.872514\n",
      "epoch  128 train loss  0.883479 valid loss  0.879242\n",
      "epoch  129 train loss  0.881643 valid loss  0.872507\n",
      "epoch  130 train loss  0.879984 valid loss  0.886396\n",
      "epoch  131 train loss  0.881247 valid loss  0.905581\n",
      "epoch  132 train loss  0.891755 valid loss  0.883032\n",
      "epoch  133 train loss  0.884805 valid loss  0.852172\n",
      "epoch  134 train loss  0.912323 valid loss  0.851535\n",
      "epoch  135 train loss  0.892493 valid loss  0.863459\n",
      "epoch  136 train loss  0.893209 valid loss  0.930268\n",
      "epoch  137 train loss  0.891639 valid loss  0.929746\n",
      "epoch  138 train loss  0.891326 valid loss  0.877469\n",
      "epoch  139 train loss  0.877873 valid loss  0.864457\n",
      "epoch  140 train loss  0.837485 valid loss  0.851193\n",
      "epoch  141 train loss  0.958685 valid loss  0.851269\n",
      "epoch  142 train loss  0.890168 valid loss  0.882024\n",
      "epoch  143 train loss  0.880099 valid loss  0.896732\n",
      "epoch  144 train loss  0.876684 valid loss  0.912704\n",
      "epoch  145 train loss  0.891385 valid loss  0.896250\n",
      "epoch  146 train loss  0.877339 valid loss  0.857787\n",
      "epoch  147 train loss  0.882792 valid loss  0.851774\n",
      "epoch  148 train loss  0.888441 valid loss  0.858452\n",
      "epoch  149 train loss  0.884080 valid loss  0.888964\n",
      "epoch  150 train loss  0.886137 valid loss  0.877491\n",
      "epoch  151 train loss  0.873231 valid loss  0.895406\n",
      "epoch  152 train loss  0.880873 valid loss  0.908883\n",
      "epoch  153 train loss  0.881726 valid loss  0.900127\n",
      "epoch  154 train loss  0.890984 valid loss  0.907498\n",
      "epoch  155 train loss  0.887975 valid loss  0.867872\n",
      "epoch  156 train loss  0.867676 valid loss  0.854238\n",
      "epoch  157 train loss  0.878347 valid loss  0.850805\n",
      "epoch  158 train loss  0.918503 valid loss  0.863486\n",
      "epoch  159 train loss  0.871972 valid loss  0.940381\n",
      "epoch  160 train loss  0.955516 valid loss  0.863271\n",
      "epoch  161 train loss  0.880491 valid loss  0.857073\n",
      "epoch  162 train loss  0.955593 valid loss  0.851728\n",
      "epoch  163 train loss  0.892924 valid loss  0.903374\n",
      "epoch  164 train loss  0.893321 valid loss  0.897409\n",
      "epoch  165 train loss  0.884046 valid loss  0.851488\n",
      "epoch  166 train loss  0.932310 valid loss  0.856547\n",
      "epoch  167 train loss  0.919037 valid loss  0.864065\n",
      "epoch  168 train loss  0.860521 valid loss  0.987965\n",
      "epoch  169 train loss  0.909046 valid loss  1.053410\n",
      "epoch  170 train loss  1.001804 valid loss  0.872562\n",
      "epoch  171 train loss  0.894406 valid loss  0.860935\n",
      "epoch  172 train loss  0.953334 valid loss  0.850741\n",
      "epoch  173 train loss  0.878111 valid loss  0.878027\n",
      "epoch  174 train loss  0.897271 valid loss  0.906999\n",
      "epoch  175 train loss  0.881073 valid loss  0.883186\n",
      "epoch  176 train loss  0.896849 valid loss  0.851117\n",
      "epoch  177 train loss  0.927171 valid loss  0.869199\n",
      "epoch  178 train loss  0.857984 valid loss  1.025114\n",
      "epoch  179 train loss  0.897487 valid loss  1.061424\n",
      "epoch  180 train loss  0.963430 valid loss  0.940378\n",
      "epoch  181 train loss  0.852021 valid loss  0.871795\n",
      "epoch  182 train loss  0.876568 valid loss  0.861117\n",
      "epoch  183 train loss  0.876869 valid loss  0.851313\n",
      "epoch  184 train loss  0.902089 valid loss  0.868436\n",
      "epoch  185 train loss  0.899074 valid loss  0.895227\n",
      "epoch  186 train loss  0.873596 valid loss  0.881415\n",
      "epoch  187 train loss  0.879502 valid loss  0.868560\n",
      "epoch  188 train loss  0.875770 valid loss  0.850608\n",
      "epoch  189 train loss  0.915874 valid loss  0.859270\n",
      "epoch  190 train loss  0.881586 valid loss  0.895741\n",
      "epoch  191 train loss  0.900891 valid loss  0.851818\n",
      "epoch  192 train loss  0.898768 valid loss  0.850336\n",
      "epoch  193 train loss  0.821180 valid loss  0.850519\n",
      "epoch  194 train loss  0.890216 valid loss  0.850297\n",
      "epoch  195 train loss  0.888021 valid loss  0.866902\n",
      "epoch  196 train loss  0.882437 valid loss  0.864056\n",
      "epoch  197 train loss  0.873564 valid loss  0.858555\n",
      "epoch  198 train loss  0.886446 valid loss  0.868477\n",
      "epoch  199 train loss  0.878189 valid loss  0.851834\n",
      "epoch  200 train loss  0.921536 valid loss  0.852458\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Dipole_Moment()\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from typing import List\n",
    "\n",
    "def create_loss_plot(\n",
    "    train_loss: List[np.ndarray],\n",
    "    val_loss: List[np.ndarray],\n",
    "    train_label: str,\n",
    "    val_label: str,\n",
    "    title: str,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a Plotly figure with training and validation loss curves and save it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        train_loss (List[np.ndarray]): List of training loss values.\n",
    "        val_loss (List[np.ndarray]): List of validation loss values.\n",
    "        train_label (str): Label for the training loss curve.\n",
    "        val_label (str): Label for the validation loss curve.\n",
    "        title (str): Title of the plot.\n",
    "        filename (str): Filename to save the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_loss_list = [float(loss) for loss in train_loss]\n",
    "    val_loss_list = [float(loss) for loss in val_loss]\n",
    "\n",
    "    trace_train = go.Scatter(y=train_loss_list, mode=\"lines\", name=train_label)\n",
    "    trace_val = go.Scatter(y=val_loss_list, mode=\"lines\", name=val_label)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(trace_train)\n",
    "    fig.add_trace(trace_val)\n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title=\"Epoch\", yaxis_title=\"Loss\", legend_title=\"Legend\"\n",
    "    )\n",
    "    pio.write_html(fig, filename)\n",
    "\n",
    "\n",
    "create_loss_plot(\n",
    "    list_train_loss,\n",
    "    list_val_loss,\n",
    "    \"Training Loss\",\n",
    "    \"Validation Loss\",\n",
    "    \"Training vs Validation Loss (Train)\",\n",
    "    \"train_vs_val_train_dipole_moment.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[-13.63463    -5.4318485  -2.9032514]\n",
      "mean squared error 92.00165\n",
      "positions \n",
      " [[ 1.3739698  -0.36267418  1.7607656 ]\n",
      " [ 1.0720738   0.6241141  -0.73667634]\n",
      " [-0.72627133 -1.6513554   2.6751056 ]\n",
      " [ 0.36511922 -0.6054491   4.4396477 ]\n",
      " [-1.0101337  -0.31764832  0.65454173]\n",
      " [ 2.8003755  -0.15685865  4.326635  ]\n",
      " [ 1.8958619  -2.5351772   3.4812946 ]\n",
      " [ 0.08705649  2.1099863   1.0994254 ]\n",
      " [ 0.6552997  -3.044066    1.1402597 ]\n",
      " [ 0.6292777  -1.7157842  -0.89606315]\n",
      " [-0.9160637   0.91140497  2.9305716 ]\n",
      " [ 1.3739641   1.775827    3.3651717 ]\n",
      " [ 2.5825186   2.0538971   0.9288267 ]\n",
      " [ 2.7610474  -2.0349882   0.22516835]\n",
      " [ 3.5957031  -1.5967042   2.3485258 ]\n",
      " [ 3.7129374   0.07741954  0.03268949]\n",
      " [ 3.9070163   0.6479711   2.2983937 ]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[-13.63463    -5.4318485  -2.9032514]\n",
      "mean squared error 92.00165\n",
      "positions \n",
      " [[-2.5330465  -1.0106453  -0.5376282 ]\n",
      " [-2.8349423  -0.023857   -3.03507   ]\n",
      " [-4.6332874  -2.2993264   0.37671185]\n",
      " [-3.541897   -1.2534201   2.141254  ]\n",
      " [-4.91715    -0.96561944 -1.643852  ]\n",
      " [-1.1066408  -0.8048297   2.0282412 ]\n",
      " [-2.0111544  -3.1831484   1.1829009 ]\n",
      " [-3.8199599   1.4620152  -1.1989683 ]\n",
      " [-3.2517166  -3.692037   -1.158134  ]\n",
      " [-3.2777386  -2.3637552  -3.1944568 ]\n",
      " [-4.82308     0.26343387  0.6321778 ]\n",
      " [-2.5330522   1.127856    1.066778  ]\n",
      " [-1.3244977   1.405926   -1.369567  ]\n",
      " [-1.1459689  -2.6829593  -2.0732255 ]\n",
      " [-0.31131315 -2.2446754   0.05013204]\n",
      " [-0.19407892 -0.5705516  -2.2657042 ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "positions-=positions[17,...]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0.]\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[-13.634623   -5.4318485  -2.9032514]\n",
      "mean squared error 92.00158\n",
      "positions \n",
      " [[ 9.7466957e+01 -1.0106453e+00 -5.3762817e-01]\n",
      " [ 9.7165054e+01 -2.3856997e-02 -3.0350699e+00]\n",
      " [ 9.5366714e+01 -2.2993264e+00  3.7671185e-01]\n",
      " [ 9.6458099e+01 -1.2534201e+00  2.1412539e+00]\n",
      " [ 9.5082848e+01 -9.6561944e-01 -1.6438520e+00]\n",
      " [ 9.8893356e+01 -8.0482972e-01  2.0282412e+00]\n",
      " [ 9.7988846e+01 -3.1831484e+00  1.1829009e+00]\n",
      " [ 9.6180038e+01  1.4620152e+00 -1.1989683e+00]\n",
      " [ 9.6748283e+01 -3.6920371e+00 -1.1581340e+00]\n",
      " [ 9.6722260e+01 -2.3637552e+00 -3.1944568e+00]\n",
      " [ 9.5176918e+01  2.6343387e-01  6.3217783e-01]\n",
      " [ 9.7466949e+01  1.1278560e+00  1.0667779e+00]\n",
      " [ 9.8675499e+01  1.4059260e+00 -1.3695670e+00]\n",
      " [ 9.8854034e+01 -2.6829593e+00 -2.0732255e+00]\n",
      " [ 9.9688690e+01 -2.2446754e+00  5.0132036e-02]\n",
      " [ 9.9805923e+01 -5.7055157e-01 -2.2657042e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "positions-=positions[17,...]\n",
    "positions+=jnp.array([100,0,0])\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        #print('Embed',x.shape)\n",
    "        #print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                #print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                #print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            #print('Residual',x.shape)\n",
    "\n",
    "        # 5. Predict atomic energies with an ordinary dense layer.\n",
    "        #element_bias = self.param(\n",
    "        #    \"element_bias\",\n",
    "        #    lambda rng, shape: jnp.zeros(shape),\n",
    "        #    (self.max_atomic_number + 1),\n",
    "        #)\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 9, 1)\n",
    "        #print('After dense:',x.shape)\n",
    "        x=jnp.sum(x, axis=-4) \n",
    "        #print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        #print('After slicing:' ,x.shape)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "\n",
    "        # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "        # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        \n",
    "        dipole = self.dipole_moment(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "dm_model = MP_Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "num_train=10\n",
    "num_val=2\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "params = dm_model.init(key,\n",
    "    atomic_numbers=train_data['atomic_numbers'][0],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "moment = dm_model.apply(\n",
    "            params,\n",
    "            atomic_numbers=train_data[\"atomic_numbers\"][0],\n",
    "            positions=train_data[\"positions\"][0],\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=None,\n",
    "            batch_size=1,\n",
    "        )\n",
    "print(moment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update', 'batch_size'))\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole = model_apply(\n",
    "      params,\n",
    "      atomic_numbers=batch['atomic_numbers'],\n",
    "      positions=batch['positions'],\n",
    "      dst_idx=batch['dst_idx'],\n",
    "      src_idx=batch['src_idx'],\n",
    "      batch_segments=batch['batch_segments'],\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "      dipole_prediction=dipole,\n",
    "      dipole_target=batch['dipole_moment']\n",
    "    )\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'batch_size'))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "  dipole = model_apply(\n",
    "    params,\n",
    "    atomic_numbers=batch['atomic_numbers'],\n",
    "    positions=batch['positions'],\n",
    "    dst_idx=batch['dst_idx'],\n",
    "    src_idx=batch['src_idx'],\n",
    "    batch_segments=batch['batch_segments'],\n",
    "    batch_size=batch_size\n",
    "  )\n",
    "  loss = mean_squared_loss(\n",
    "    energy_prediction=dipole,\n",
    "    energy_target=batch['dipole_moment']\n",
    "  )\n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(train_data['atomic_numbers']))\n",
    "  params = model.init(init_key,\n",
    "    atomic_numbers=train_data['atomic_numbers'],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Batches for the validation set need to be prepared only once.\n",
    "  key, shuffle_key = jax.random.split(key)\n",
    "  valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Prepare batches.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "    # Loop over train batches.\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_batches):\n",
    "      params, opt_state, loss= train_step(\n",
    "        model_apply=model.apply,\n",
    "        optimizer_update=optimizer.update,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        opt_state=opt_state,\n",
    "        params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    valid_loss = 0.0\n",
    "    for i, batch in enumerate(valid_batches):\n",
    "      loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        params=params\n",
    "      )\n",
    "      valid_loss += (loss - valid_loss)/(i+1)\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "    print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
