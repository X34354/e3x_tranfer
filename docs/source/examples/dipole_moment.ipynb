{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment of inertia tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "\n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "\n",
    "  # Calculate moment of inertia tensors.\n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "\n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2,\n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "\n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "\n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['masses'][0:1].shape)\\nprint(train_data['positions'][0:1].shape)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['masses'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'key, train_key = jax.random.split(key)\\nmodel = Model()\\nparams = train_model(\\n  key=train_key,\\n  model=model,\\n  train_data=train_data,\\n  valid_data=valid_data,\\n  num_epochs=num_epochs,\\n  learning_rate=learning_rate,\\n  batch_size=batch_size,\\n)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i = 0\\nmasses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\\nprediction = model.apply(params, masses, positions)\\n\\nprint('target')\\nprint(target)\\nprint('prediction')\\nprint(prediction)\\nprint('mean squared error', jnp.mean((prediction-target)**2))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "R_units\n",
      "z\n",
      "E\n",
      "E_units\n",
      "F\n",
      "F_units\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "name\n",
      "README\n",
      "theory\n",
      "Dipole moment shape array (5042, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [23 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14]\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_data.npz\"\n",
    "dataset= np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print('Dipole moment shape array',dataset['D'].shape)\n",
    "print('Dipole moment units', dataset['D_units'])\n",
    "\n",
    "print('Atomic numbers', dataset['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "    Z=jnp.full(16,14)\n",
    "    Z=jnp.append(Z,23)\n",
    "    Z=jnp.expand_dims(Z,axis=0)\n",
    "    Z=jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train=3000\n",
    "num_val=2000\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "print(train_data['dipole_moment'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "    # features = 1\n",
    "    # max_degree = 1\n",
    "    @nn.compact\n",
    "    def __call__(self,atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "        # 1. Initialize features\n",
    "        '''dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "        print('dst_idx',dst_idx.shape)\n",
    "        dst_idx = jnp.expand_dims(dst_idx, axis=0)  \n",
    "        src_idx = jnp.expand_dims(src_idx, axis=0)  \n",
    "        dst_idx = jnp.repeat(dst_idx, positions.shape[0], axis=0)\n",
    "        src_idx = jnp.repeat(src_idx, positions.shape[0], axis=0)\n",
    "        print('dst_idx after expansion',dst_idx.shape)\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst\n",
    "        print('positions',positions.shape)\n",
    "        print('displacements',displacements.shape)\n",
    "        #x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "        Z= e3x.nn.Embed(num_embeddings=23, features=272)(atomic_numbers)\n",
    "        print('atomic_numbers',atomic_numbers.shape)\n",
    "        print('Z',Z.shape)\n",
    "        Z=Z[0,...]\n",
    "        Z=jnp.reshape(Z, (272, 17, 1))\n",
    "        print('Z',Z.shape)\n",
    "        x = jnp.concatenate((Z, displacements), axis=-1) '''\n",
    "        positions -= positions[0, ...]\n",
    "        print('aaaaaa')\n",
    "        x = jnp.concatenate(\n",
    "            (atomic_numbers[..., None], positions), axis=-1\n",
    "        )  # Shape (..., N, 4).\n",
    "        # print(\"Initial shape:\", x.shape)\n",
    "        x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "        \n",
    "        # print(\"x shape:\", x.shape)\n",
    "        # 2. Apply transformations.\n",
    "        # First Dense block with residual connection\n",
    "        residual = e3x.nn.Dense(features=256)(x)\n",
    "        x = nn.relu(residual)\n",
    "        x = e3x.nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x + residual  # Add residual connection\n",
    "\n",
    "        # Layer Normalization\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Second Dense block\n",
    "        residual = e3x.nn.Dense(features=128)(x)\n",
    "        x = nn.relu(residual)\n",
    "        x = e3x.nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = x + residual  # Add residual connection\n",
    "\n",
    "        # Layer Normalization\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Adding more complexity with TensorDense and additional layers\n",
    "        x = e3x.nn.TensorDense(features=64, max_degree=2)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = e3x.nn.TensorDense(features=32, max_degree=2)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=1, max_degree=1)(x)\n",
    "        # print(\"After TensorDense layer:\", x.shape)\n",
    "        # x = e3x.nn.Dense(features=16)(x)\n",
    "        # x = e3x.nn.Dense(features=1)(x)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        # print(\"After sum:\", x.shape)\n",
    "        # x=jnp.sum(x, axis=0)\n",
    "        # print(\"After second sum:\", x.shape)\n",
    "        y = x[..., 1, 1:4, 0]\n",
    "        # y = x[1, 1:4, 0]\n",
    "        # y=y[None,...]\n",
    "        # print(\"After slicing:\", y.shape)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dm_model = Dipole_Moment()\\nkey = jax.random.PRNGKey(0)\\n\\n# Generate train and test datasets.\\nkey, data_key = jax.random.split(key)\\ntrain_data, valid_data = generate_datasets(data_key)\\nparams = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nmoment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nprint(moment.shape)\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dm_model = Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "params = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "moment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "print(moment.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "\n",
    "    def loss_fn(params):\n",
    "        print(batch[\"positions\"].shape)\n",
    "        dipole_moment = model_apply(params, batch[\"atomic_numbers\"], batch[\"positions\"])\n",
    "        loss = mean_squared_loss(dipole_moment, batch[\"dipole_moment\"])\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  dipole_moment = model_apply(params,batch['atomic_numbers'],batch['positions'])\n",
    "  loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key,train_data['atomic_numbers'][0:1],train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['positions'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  list_train_loss = []\n",
    "  list_val_loss = []\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    #print(perms)\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      \n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "      \n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "    list_val_loss.append(valid_loss)\n",
    "    list_train_loss.append(train_loss)\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params ,list_train_loss , list_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 17)\n",
      "(1, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['atomic_numbers'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_train=3000\n",
    "num_val=2000\n",
    "train_data, valid_data = prepare_datasets(filename,key, num_train,num_val)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['positions'][0:1].shape)\\nprint(train_data['atomic_numbers'][0:1].shape)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['positions'][0:1].shape)\n",
    "print(train_data['atomic_numbers'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa\n",
      "(512, 17, 3)\n",
      "aaaaaa\n",
      "aaaaaa\n",
      "epoch    1 train loss  1.764485 valid loss  0.809700\n",
      "epoch    2 train loss  0.599803 valid loss  0.418900\n",
      "epoch    3 train loss  0.291038 valid loss  0.320077\n",
      "epoch    4 train loss  0.249948 valid loss  0.149052\n",
      "epoch    5 train loss  0.108494 valid loss  0.060611\n",
      "epoch    6 train loss  0.058921 valid loss  0.052531\n",
      "epoch    7 train loss  0.046835 valid loss  0.039849\n",
      "epoch    8 train loss  0.038926 valid loss  0.018981\n",
      "epoch    9 train loss  0.037068 valid loss  0.016277\n",
      "epoch   10 train loss  0.029506 valid loss  0.010412\n",
      "epoch   11 train loss  0.023983 valid loss  0.022705\n",
      "epoch   12 train loss  0.022923 valid loss  0.019069\n",
      "epoch   13 train loss  0.015926 valid loss  0.009802\n",
      "epoch   14 train loss  0.011696 valid loss  0.010478\n",
      "epoch   15 train loss  0.011893 valid loss  0.007667\n",
      "epoch   16 train loss  0.010234 valid loss  0.007870\n",
      "epoch   17 train loss  0.011396 valid loss  0.007045\n",
      "epoch   18 train loss  0.011317 valid loss  0.009663\n",
      "epoch   19 train loss  0.010457 valid loss  0.007681\n",
      "epoch   20 train loss  0.011444 valid loss  0.008024\n",
      "epoch   21 train loss  0.012562 valid loss  0.007864\n",
      "epoch   22 train loss  0.010767 valid loss  0.012767\n",
      "epoch   23 train loss  0.010893 valid loss  0.011783\n",
      "epoch   24 train loss  0.010531 valid loss  0.014335\n",
      "epoch   25 train loss  0.011620 valid loss  0.012334\n",
      "epoch   26 train loss  0.010405 valid loss  0.008408\n",
      "epoch   27 train loss  0.009207 valid loss  0.007940\n",
      "epoch   28 train loss  0.009607 valid loss  0.007844\n",
      "epoch   29 train loss  0.011561 valid loss  0.007577\n",
      "epoch   30 train loss  0.011528 valid loss  0.008545\n",
      "epoch   31 train loss  0.010803 valid loss  0.008101\n",
      "epoch   32 train loss  0.010703 valid loss  0.008726\n",
      "epoch   33 train loss  0.012549 valid loss  0.010226\n",
      "epoch   34 train loss  0.010394 valid loss  0.008315\n",
      "epoch   35 train loss  0.009915 valid loss  0.011776\n",
      "epoch   36 train loss  0.011414 valid loss  0.008273\n",
      "epoch   37 train loss  0.011578 valid loss  0.007503\n",
      "epoch   38 train loss  0.012779 valid loss  0.007490\n",
      "epoch   39 train loss  0.012595 valid loss  0.008305\n",
      "epoch   40 train loss  0.010019 valid loss  0.007741\n",
      "epoch   41 train loss  0.013641 valid loss  0.007505\n",
      "epoch   42 train loss  0.014162 valid loss  0.007296\n",
      "epoch   43 train loss  0.010814 valid loss  0.007669\n",
      "epoch   44 train loss  0.010961 valid loss  0.007644\n",
      "epoch   45 train loss  0.009846 valid loss  0.007653\n",
      "epoch   46 train loss  0.010738 valid loss  0.011483\n",
      "epoch   47 train loss  0.011227 valid loss  0.011101\n",
      "epoch   48 train loss  0.014806 valid loss  0.007684\n",
      "epoch   49 train loss  0.010306 valid loss  0.007070\n",
      "epoch   50 train loss  0.009494 valid loss  0.007191\n",
      "epoch   51 train loss  0.010937 valid loss  0.007239\n",
      "epoch   52 train loss  0.014873 valid loss  0.007666\n",
      "epoch   53 train loss  0.013132 valid loss  0.007858\n",
      "epoch   54 train loss  0.010347 valid loss  0.007985\n",
      "epoch   55 train loss  0.014731 valid loss  0.011521\n",
      "epoch   56 train loss  0.012572 valid loss  0.011113\n",
      "epoch   57 train loss  0.018232 valid loss  0.024232\n",
      "epoch   58 train loss  0.020033 valid loss  0.014528\n",
      "epoch   59 train loss  0.012486 valid loss  0.010849\n",
      "epoch   60 train loss  0.011529 valid loss  0.010518\n",
      "epoch   61 train loss  0.011219 valid loss  0.010924\n",
      "epoch   62 train loss  0.009957 valid loss  0.007238\n",
      "epoch   63 train loss  0.011766 valid loss  0.007903\n",
      "epoch   64 train loss  0.010346 valid loss  0.008918\n",
      "epoch   65 train loss  0.010604 valid loss  0.008633\n",
      "epoch   66 train loss  0.012641 valid loss  0.009676\n",
      "epoch   67 train loss  0.009897 valid loss  0.009103\n",
      "epoch   68 train loss  0.008692 valid loss  0.009209\n",
      "epoch   69 train loss  0.010776 valid loss  0.007471\n",
      "epoch   70 train loss  0.009721 valid loss  0.007593\n",
      "epoch   71 train loss  0.010599 valid loss  0.008225\n",
      "epoch   72 train loss  0.010074 valid loss  0.009012\n",
      "epoch   73 train loss  0.011368 valid loss  0.008886\n",
      "epoch   74 train loss  0.009607 valid loss  0.007854\n",
      "epoch   75 train loss  0.010614 valid loss  0.011539\n",
      "epoch   76 train loss  0.012283 valid loss  0.007760\n",
      "epoch   77 train loss  0.010671 valid loss  0.006907\n",
      "epoch   78 train loss  0.011111 valid loss  0.007246\n",
      "epoch   79 train loss  0.010067 valid loss  0.015611\n",
      "epoch   80 train loss  0.012312 valid loss  0.007577\n",
      "epoch   81 train loss  0.009798 valid loss  0.008614\n",
      "epoch   82 train loss  0.010862 valid loss  0.008133\n",
      "epoch   83 train loss  0.015012 valid loss  0.007655\n",
      "epoch   84 train loss  0.010602 valid loss  0.012100\n",
      "epoch   85 train loss  0.012997 valid loss  0.009018\n",
      "epoch   86 train loss  0.010009 valid loss  0.007903\n",
      "epoch   87 train loss  0.010201 valid loss  0.009190\n",
      "epoch   88 train loss  0.010192 valid loss  0.014084\n",
      "epoch   89 train loss  0.016726 valid loss  0.007375\n",
      "epoch   90 train loss  0.012624 valid loss  0.008127\n",
      "epoch   91 train loss  0.013317 valid loss  0.007124\n",
      "epoch   92 train loss  0.012774 valid loss  0.014495\n",
      "epoch   93 train loss  0.014067 valid loss  0.016633\n",
      "epoch   94 train loss  0.022465 valid loss  0.016294\n",
      "epoch   95 train loss  0.015504 valid loss  0.018202\n",
      "epoch   96 train loss  0.013181 valid loss  0.013112\n",
      "epoch   97 train loss  0.011771 valid loss  0.012433\n",
      "epoch   98 train loss  0.012271 valid loss  0.009171\n",
      "epoch   99 train loss  0.010858 valid loss  0.010646\n",
      "epoch  100 train loss  0.012447 valid loss  0.013219\n",
      "epoch  101 train loss  0.015253 valid loss  0.009142\n",
      "epoch  102 train loss  0.011674 valid loss  0.009895\n",
      "epoch  103 train loss  0.011387 valid loss  0.008867\n",
      "epoch  104 train loss  0.010774 valid loss  0.008399\n",
      "epoch  105 train loss  0.009529 valid loss  0.007698\n",
      "epoch  106 train loss  0.009195 valid loss  0.007648\n",
      "epoch  107 train loss  0.010660 valid loss  0.008577\n",
      "epoch  108 train loss  0.008754 valid loss  0.008985\n",
      "epoch  109 train loss  0.011960 valid loss  0.008158\n",
      "epoch  110 train loss  0.010516 valid loss  0.007576\n",
      "epoch  111 train loss  0.010732 valid loss  0.008536\n",
      "epoch  112 train loss  0.012732 valid loss  0.017760\n",
      "epoch  113 train loss  0.012686 valid loss  0.015205\n",
      "epoch  114 train loss  0.012126 valid loss  0.009887\n",
      "epoch  115 train loss  0.012716 valid loss  0.009805\n",
      "epoch  116 train loss  0.011655 valid loss  0.007576\n",
      "epoch  117 train loss  0.009837 valid loss  0.007096\n",
      "epoch  118 train loss  0.011978 valid loss  0.008537\n",
      "epoch  119 train loss  0.011728 valid loss  0.007162\n",
      "epoch  120 train loss  0.011477 valid loss  0.009157\n",
      "epoch  121 train loss  0.012561 valid loss  0.008091\n",
      "epoch  122 train loss  0.011445 valid loss  0.008006\n",
      "epoch  123 train loss  0.011425 valid loss  0.007605\n",
      "epoch  124 train loss  0.012164 valid loss  0.008967\n",
      "epoch  125 train loss  0.012617 valid loss  0.008954\n",
      "epoch  126 train loss  0.012326 valid loss  0.011986\n",
      "epoch  127 train loss  0.012334 valid loss  0.009244\n",
      "epoch  128 train loss  0.009108 valid loss  0.008981\n",
      "epoch  129 train loss  0.009967 valid loss  0.007982\n",
      "epoch  130 train loss  0.012845 valid loss  0.008071\n",
      "epoch  131 train loss  0.012533 valid loss  0.008223\n",
      "epoch  132 train loss  0.011815 valid loss  0.007549\n",
      "epoch  133 train loss  0.010544 valid loss  0.007363\n",
      "epoch  134 train loss  0.010496 valid loss  0.008236\n",
      "epoch  135 train loss  0.011064 valid loss  0.009561\n",
      "epoch  136 train loss  0.014933 valid loss  0.017737\n",
      "epoch  137 train loss  0.014157 valid loss  0.009504\n",
      "epoch  138 train loss  0.011696 valid loss  0.011868\n",
      "epoch  139 train loss  0.013199 valid loss  0.009183\n",
      "epoch  140 train loss  0.015648 valid loss  0.013243\n",
      "epoch  141 train loss  0.013429 valid loss  0.012508\n",
      "epoch  142 train loss  0.013591 valid loss  0.013257\n",
      "epoch  143 train loss  0.012070 valid loss  0.011520\n",
      "epoch  144 train loss  0.013447 valid loss  0.013692\n",
      "epoch  145 train loss  0.013032 valid loss  0.009813\n",
      "epoch  146 train loss  0.014943 valid loss  0.007778\n",
      "epoch  147 train loss  0.014508 valid loss  0.007508\n",
      "epoch  148 train loss  0.014273 valid loss  0.007221\n",
      "epoch  149 train loss  0.011015 valid loss  0.007586\n",
      "epoch  150 train loss  0.011545 valid loss  0.008799\n",
      "epoch  151 train loss  0.011097 valid loss  0.008406\n",
      "epoch  152 train loss  0.010330 valid loss  0.008832\n",
      "epoch  153 train loss  0.010131 valid loss  0.007852\n",
      "epoch  154 train loss  0.009728 valid loss  0.008175\n",
      "epoch  155 train loss  0.009935 valid loss  0.008079\n",
      "epoch  156 train loss  0.008691 valid loss  0.007941\n",
      "epoch  157 train loss  0.010147 valid loss  0.011745\n",
      "epoch  158 train loss  0.012103 valid loss  0.014067\n",
      "epoch  159 train loss  0.012672 valid loss  0.012853\n",
      "epoch  160 train loss  0.013151 valid loss  0.014637\n",
      "epoch  161 train loss  0.010868 valid loss  0.012747\n",
      "epoch  162 train loss  0.013937 valid loss  0.012724\n",
      "epoch  163 train loss  0.015016 valid loss  0.013600\n",
      "epoch  164 train loss  0.012869 valid loss  0.009810\n",
      "epoch  165 train loss  0.012248 valid loss  0.009008\n",
      "epoch  166 train loss  0.012893 valid loss  0.008499\n",
      "epoch  167 train loss  0.016037 valid loss  0.008131\n",
      "epoch  168 train loss  0.015668 valid loss  0.010705\n",
      "epoch  169 train loss  0.013282 valid loss  0.017412\n",
      "epoch  170 train loss  0.014428 valid loss  0.011113\n",
      "epoch  171 train loss  0.015391 valid loss  0.012675\n",
      "epoch  172 train loss  0.013760 valid loss  0.008573\n",
      "epoch  173 train loss  0.011037 valid loss  0.009498\n",
      "epoch  174 train loss  0.010916 valid loss  0.008499\n",
      "epoch  175 train loss  0.009632 valid loss  0.007900\n",
      "epoch  176 train loss  0.008637 valid loss  0.008108\n",
      "epoch  177 train loss  0.008795 valid loss  0.008401\n",
      "epoch  178 train loss  0.010506 valid loss  0.008183\n",
      "epoch  179 train loss  0.008668 valid loss  0.007644\n",
      "epoch  180 train loss  0.011220 valid loss  0.007756\n",
      "epoch  181 train loss  0.009592 valid loss  0.007739\n",
      "epoch  182 train loss  0.009684 valid loss  0.007910\n",
      "epoch  183 train loss  0.011200 valid loss  0.007988\n",
      "epoch  184 train loss  0.009556 valid loss  0.012180\n",
      "epoch  185 train loss  0.010797 valid loss  0.009406\n",
      "epoch  186 train loss  0.009184 valid loss  0.009333\n",
      "epoch  187 train loss  0.010995 valid loss  0.010952\n",
      "epoch  188 train loss  0.013057 valid loss  0.012441\n",
      "epoch  189 train loss  0.018876 valid loss  0.015190\n",
      "epoch  190 train loss  0.014188 valid loss  0.010151\n",
      "epoch  191 train loss  0.011700 valid loss  0.009105\n",
      "epoch  192 train loss  0.013508 valid loss  0.007867\n",
      "epoch  193 train loss  0.011603 valid loss  0.008580\n",
      "epoch  194 train loss  0.009797 valid loss  0.008484\n",
      "epoch  195 train loss  0.012721 valid loss  0.012505\n",
      "epoch  196 train loss  0.013475 valid loss  0.008031\n",
      "epoch  197 train loss  0.011050 valid loss  0.007285\n",
      "epoch  198 train loss  0.009909 valid loss  0.008235\n",
      "epoch  199 train loss  0.010319 valid loss  0.007285\n",
      "epoch  200 train loss  0.011090 valid loss  0.007553\n",
      "epoch  201 train loss  0.009865 valid loss  0.008140\n",
      "epoch  202 train loss  0.010282 valid loss  0.008279\n",
      "epoch  203 train loss  0.011208 valid loss  0.009284\n",
      "epoch  204 train loss  0.010784 valid loss  0.010786\n",
      "epoch  205 train loss  0.010906 valid loss  0.012742\n",
      "epoch  206 train loss  0.016589 valid loss  0.018764\n",
      "epoch  207 train loss  0.017706 valid loss  0.017450\n",
      "epoch  208 train loss  0.016518 valid loss  0.025200\n",
      "epoch  209 train loss  0.019374 valid loss  0.024310\n",
      "epoch  210 train loss  0.018185 valid loss  0.024995\n",
      "epoch  211 train loss  0.015954 valid loss  0.014255\n",
      "epoch  212 train loss  0.014238 valid loss  0.008202\n",
      "epoch  213 train loss  0.010040 valid loss  0.007476\n",
      "epoch  214 train loss  0.009231 valid loss  0.007185\n",
      "epoch  215 train loss  0.008848 valid loss  0.007391\n",
      "epoch  216 train loss  0.009839 valid loss  0.007988\n",
      "epoch  217 train loss  0.010255 valid loss  0.008233\n",
      "epoch  218 train loss  0.011713 valid loss  0.009784\n",
      "epoch  219 train loss  0.013310 valid loss  0.007845\n",
      "epoch  220 train loss  0.010037 valid loss  0.008770\n",
      "epoch  221 train loss  0.011055 valid loss  0.007905\n",
      "epoch  222 train loss  0.015361 valid loss  0.007708\n",
      "epoch  223 train loss  0.012203 valid loss  0.008828\n",
      "epoch  224 train loss  0.014283 valid loss  0.013236\n",
      "epoch  225 train loss  0.014842 valid loss  0.008855\n",
      "epoch  226 train loss  0.010614 valid loss  0.009924\n",
      "epoch  227 train loss  0.012587 valid loss  0.009735\n",
      "epoch  228 train loss  0.017104 valid loss  0.013907\n",
      "epoch  229 train loss  0.016712 valid loss  0.021560\n",
      "epoch  230 train loss  0.016339 valid loss  0.012602\n",
      "epoch  231 train loss  0.014129 valid loss  0.009174\n",
      "epoch  232 train loss  0.011854 valid loss  0.007682\n",
      "epoch  233 train loss  0.008891 valid loss  0.010495\n",
      "epoch  234 train loss  0.010282 valid loss  0.008886\n",
      "epoch  235 train loss  0.009141 valid loss  0.007993\n",
      "epoch  236 train loss  0.010955 valid loss  0.009928\n",
      "epoch  237 train loss  0.009772 valid loss  0.009870\n",
      "epoch  238 train loss  0.012401 valid loss  0.010014\n",
      "epoch  239 train loss  0.011533 valid loss  0.007284\n",
      "epoch  240 train loss  0.009781 valid loss  0.007308\n",
      "epoch  241 train loss  0.011853 valid loss  0.012839\n",
      "epoch  242 train loss  0.012545 valid loss  0.012615\n",
      "epoch  243 train loss  0.014580 valid loss  0.013104\n",
      "epoch  244 train loss  0.013570 valid loss  0.011331\n",
      "epoch  245 train loss  0.011528 valid loss  0.012738\n",
      "epoch  246 train loss  0.012499 valid loss  0.012667\n",
      "epoch  247 train loss  0.013343 valid loss  0.011627\n",
      "epoch  248 train loss  0.010626 valid loss  0.009014\n",
      "epoch  249 train loss  0.020233 valid loss  0.012486\n",
      "epoch  250 train loss  0.011475 valid loss  0.008402\n",
      "epoch  251 train loss  0.011484 valid loss  0.012750\n",
      "epoch  252 train loss  0.012724 valid loss  0.015012\n",
      "epoch  253 train loss  0.013413 valid loss  0.011271\n",
      "epoch  254 train loss  0.012148 valid loss  0.012575\n",
      "epoch  255 train loss  0.013745 valid loss  0.008893\n",
      "epoch  256 train loss  0.011132 valid loss  0.010425\n",
      "epoch  257 train loss  0.010905 valid loss  0.011581\n",
      "epoch  258 train loss  0.010630 valid loss  0.009785\n",
      "epoch  259 train loss  0.013546 valid loss  0.007488\n",
      "epoch  260 train loss  0.010731 valid loss  0.006707\n",
      "epoch  261 train loss  0.010227 valid loss  0.007678\n",
      "epoch  262 train loss  0.010888 valid loss  0.009565\n",
      "epoch  263 train loss  0.009406 valid loss  0.008096\n",
      "epoch  264 train loss  0.011848 valid loss  0.007546\n",
      "epoch  265 train loss  0.012315 valid loss  0.008928\n",
      "epoch  266 train loss  0.013787 valid loss  0.007760\n",
      "epoch  267 train loss  0.014151 valid loss  0.007342\n",
      "epoch  268 train loss  0.013024 valid loss  0.007381\n",
      "epoch  269 train loss  0.010759 valid loss  0.009992\n",
      "epoch  270 train loss  0.014435 valid loss  0.010089\n",
      "epoch  271 train loss  0.016679 valid loss  0.006653\n",
      "epoch  272 train loss  0.012284 valid loss  0.008845\n",
      "epoch  273 train loss  0.010584 valid loss  0.008085\n",
      "epoch  274 train loss  0.010901 valid loss  0.009157\n",
      "epoch  275 train loss  0.011125 valid loss  0.006973\n",
      "epoch  276 train loss  0.010997 valid loss  0.008050\n",
      "epoch  277 train loss  0.010903 valid loss  0.009249\n",
      "epoch  278 train loss  0.009929 valid loss  0.009228\n",
      "epoch  279 train loss  0.010902 valid loss  0.008101\n",
      "epoch  280 train loss  0.010074 valid loss  0.008786\n",
      "epoch  281 train loss  0.010932 valid loss  0.007732\n",
      "epoch  282 train loss  0.009653 valid loss  0.007702\n",
      "epoch  283 train loss  0.009192 valid loss  0.007123\n",
      "epoch  284 train loss  0.008844 valid loss  0.007957\n",
      "epoch  285 train loss  0.010702 valid loss  0.011282\n",
      "epoch  286 train loss  0.011642 valid loss  0.011448\n",
      "epoch  287 train loss  0.010919 valid loss  0.007729\n",
      "epoch  288 train loss  0.009194 valid loss  0.007446\n",
      "epoch  289 train loss  0.011923 valid loss  0.007530\n",
      "epoch  290 train loss  0.011881 valid loss  0.008226\n",
      "epoch  291 train loss  0.011838 valid loss  0.008079\n",
      "epoch  292 train loss  0.011305 valid loss  0.007612\n",
      "epoch  293 train loss  0.009897 valid loss  0.007713\n",
      "epoch  294 train loss  0.009132 valid loss  0.010686\n",
      "epoch  295 train loss  0.010401 valid loss  0.007820\n",
      "epoch  296 train loss  0.009422 valid loss  0.007655\n",
      "epoch  297 train loss  0.007725 valid loss  0.008105\n",
      "epoch  298 train loss  0.008890 valid loss  0.007833\n",
      "epoch  299 train loss  0.011881 valid loss  0.009203\n",
      "epoch  300 train loss  0.012207 valid loss  0.009084\n",
      "epoch  301 train loss  0.012994 valid loss  0.008057\n",
      "epoch  302 train loss  0.010542 valid loss  0.011118\n",
      "epoch  303 train loss  0.010254 valid loss  0.007802\n",
      "epoch  304 train loss  0.010132 valid loss  0.007703\n",
      "epoch  305 train loss  0.010033 valid loss  0.009628\n",
      "epoch  306 train loss  0.010916 valid loss  0.010362\n",
      "epoch  307 train loss  0.013361 valid loss  0.015610\n",
      "epoch  308 train loss  0.012532 valid loss  0.011585\n",
      "epoch  309 train loss  0.012111 valid loss  0.008526\n",
      "epoch  310 train loss  0.010744 valid loss  0.008743\n",
      "epoch  311 train loss  0.010127 valid loss  0.009499\n",
      "epoch  312 train loss  0.009582 valid loss  0.008188\n",
      "epoch  313 train loss  0.008744 valid loss  0.008132\n",
      "epoch  314 train loss  0.009300 valid loss  0.008835\n",
      "epoch  315 train loss  0.010599 valid loss  0.007560\n",
      "epoch  316 train loss  0.009183 valid loss  0.007160\n",
      "epoch  317 train loss  0.010790 valid loss  0.011241\n",
      "epoch  318 train loss  0.011427 valid loss  0.013187\n",
      "epoch  319 train loss  0.012627 valid loss  0.009249\n",
      "epoch  320 train loss  0.010140 valid loss  0.009678\n",
      "epoch  321 train loss  0.010718 valid loss  0.008934\n",
      "epoch  322 train loss  0.010475 valid loss  0.010419\n",
      "epoch  323 train loss  0.011092 valid loss  0.009127\n",
      "epoch  324 train loss  0.011531 valid loss  0.009417\n",
      "epoch  325 train loss  0.011233 valid loss  0.007795\n",
      "epoch  326 train loss  0.012594 valid loss  0.014925\n",
      "epoch  327 train loss  0.013163 valid loss  0.014557\n",
      "epoch  328 train loss  0.011317 valid loss  0.012894\n",
      "epoch  329 train loss  0.012322 valid loss  0.011868\n",
      "epoch  330 train loss  0.012890 valid loss  0.013163\n",
      "epoch  331 train loss  0.012006 valid loss  0.011007\n",
      "epoch  332 train loss  0.011651 valid loss  0.007651\n",
      "epoch  333 train loss  0.013446 valid loss  0.019839\n",
      "epoch  334 train loss  0.014180 valid loss  0.014272\n",
      "epoch  335 train loss  0.014667 valid loss  0.015136\n",
      "epoch  336 train loss  0.015644 valid loss  0.009761\n",
      "epoch  337 train loss  0.013794 valid loss  0.011315\n",
      "epoch  338 train loss  0.010376 valid loss  0.008487\n",
      "epoch  339 train loss  0.012621 valid loss  0.009335\n",
      "epoch  340 train loss  0.012023 valid loss  0.007634\n",
      "epoch  341 train loss  0.010104 valid loss  0.009843\n",
      "epoch  342 train loss  0.011319 valid loss  0.007513\n",
      "epoch  343 train loss  0.010096 valid loss  0.007670\n",
      "epoch  344 train loss  0.009844 valid loss  0.009934\n",
      "epoch  345 train loss  0.010427 valid loss  0.011874\n",
      "epoch  346 train loss  0.012115 valid loss  0.015062\n",
      "epoch  347 train loss  0.012589 valid loss  0.014038\n",
      "epoch  348 train loss  0.012377 valid loss  0.008745\n",
      "epoch  349 train loss  0.011055 valid loss  0.009106\n",
      "epoch  350 train loss  0.009268 valid loss  0.007942\n",
      "epoch  351 train loss  0.010912 valid loss  0.008176\n",
      "epoch  352 train loss  0.013441 valid loss  0.008145\n",
      "epoch  353 train loss  0.011406 valid loss  0.007537\n",
      "epoch  354 train loss  0.010414 valid loss  0.011231\n",
      "epoch  355 train loss  0.011871 valid loss  0.013682\n",
      "epoch  356 train loss  0.014807 valid loss  0.008824\n",
      "epoch  357 train loss  0.010091 valid loss  0.007801\n",
      "epoch  358 train loss  0.010695 valid loss  0.008139\n",
      "epoch  359 train loss  0.009574 valid loss  0.007513\n",
      "epoch  360 train loss  0.009969 valid loss  0.008007\n",
      "epoch  361 train loss  0.009831 valid loss  0.007358\n",
      "epoch  362 train loss  0.014746 valid loss  0.008560\n",
      "epoch  363 train loss  0.012013 valid loss  0.007429\n",
      "epoch  364 train loss  0.011997 valid loss  0.009138\n",
      "epoch  365 train loss  0.011699 valid loss  0.016777\n",
      "epoch  366 train loss  0.012330 valid loss  0.007933\n",
      "epoch  367 train loss  0.010368 valid loss  0.011709\n",
      "epoch  368 train loss  0.012170 valid loss  0.009948\n",
      "epoch  369 train loss  0.011240 valid loss  0.009642\n",
      "epoch  370 train loss  0.010942 valid loss  0.013780\n",
      "epoch  371 train loss  0.013698 valid loss  0.023104\n",
      "epoch  372 train loss  0.016811 valid loss  0.015009\n",
      "epoch  373 train loss  0.014507 valid loss  0.013888\n",
      "epoch  374 train loss  0.013288 valid loss  0.008665\n",
      "epoch  375 train loss  0.010891 valid loss  0.008747\n",
      "epoch  376 train loss  0.011058 valid loss  0.008502\n",
      "epoch  377 train loss  0.009976 valid loss  0.008939\n",
      "epoch  378 train loss  0.012130 valid loss  0.009510\n",
      "epoch  379 train loss  0.010909 valid loss  0.008664\n",
      "epoch  380 train loss  0.009825 valid loss  0.007853\n",
      "epoch  381 train loss  0.009392 valid loss  0.009236\n",
      "epoch  382 train loss  0.010181 valid loss  0.013917\n",
      "epoch  383 train loss  0.011926 valid loss  0.009394\n",
      "epoch  384 train loss  0.010238 valid loss  0.009722\n",
      "epoch  385 train loss  0.010025 valid loss  0.008221\n",
      "epoch  386 train loss  0.010452 valid loss  0.009590\n",
      "epoch  387 train loss  0.009115 valid loss  0.010313\n",
      "epoch  388 train loss  0.012817 valid loss  0.014972\n",
      "epoch  389 train loss  0.011159 valid loss  0.012896\n",
      "epoch  390 train loss  0.020854 valid loss  0.010825\n",
      "epoch  391 train loss  0.011146 valid loss  0.011194\n",
      "epoch  392 train loss  0.013114 valid loss  0.007992\n",
      "epoch  393 train loss  0.011865 valid loss  0.009719\n",
      "epoch  394 train loss  0.010582 valid loss  0.007419\n",
      "epoch  395 train loss  0.009436 valid loss  0.008281\n",
      "epoch  396 train loss  0.009409 valid loss  0.007500\n",
      "epoch  397 train loss  0.009155 valid loss  0.008142\n",
      "epoch  398 train loss  0.010829 valid loss  0.011332\n",
      "epoch  399 train loss  0.012189 valid loss  0.008540\n",
      "epoch  400 train loss  0.010586 valid loss  0.008109\n",
      "epoch  401 train loss  0.010118 valid loss  0.007720\n",
      "epoch  402 train loss  0.009702 valid loss  0.007442\n",
      "epoch  403 train loss  0.009802 valid loss  0.007724\n",
      "epoch  404 train loss  0.009519 valid loss  0.009275\n",
      "epoch  405 train loss  0.011333 valid loss  0.007573\n",
      "epoch  406 train loss  0.010974 valid loss  0.007931\n",
      "epoch  407 train loss  0.011982 valid loss  0.011448\n",
      "epoch  408 train loss  0.012527 valid loss  0.009883\n",
      "epoch  409 train loss  0.011573 valid loss  0.010050\n",
      "epoch  410 train loss  0.010719 valid loss  0.014417\n",
      "epoch  411 train loss  0.013071 valid loss  0.014913\n",
      "epoch  412 train loss  0.013327 valid loss  0.008492\n",
      "epoch  413 train loss  0.008576 valid loss  0.007540\n",
      "epoch  414 train loss  0.010057 valid loss  0.009622\n",
      "epoch  415 train loss  0.010975 valid loss  0.009354\n",
      "epoch  416 train loss  0.012681 valid loss  0.008109\n",
      "epoch  417 train loss  0.013356 valid loss  0.007755\n",
      "epoch  418 train loss  0.015392 valid loss  0.007491\n",
      "epoch  419 train loss  0.018099 valid loss  0.009000\n",
      "epoch  420 train loss  0.014411 valid loss  0.008729\n",
      "epoch  421 train loss  0.009640 valid loss  0.010301\n",
      "epoch  422 train loss  0.011692 valid loss  0.011234\n",
      "epoch  423 train loss  0.011350 valid loss  0.007623\n",
      "epoch  424 train loss  0.008702 valid loss  0.008145\n",
      "epoch  425 train loss  0.009685 valid loss  0.008166\n",
      "epoch  426 train loss  0.013773 valid loss  0.009037\n",
      "epoch  427 train loss  0.014753 valid loss  0.008337\n",
      "epoch  428 train loss  0.010510 valid loss  0.008435\n",
      "epoch  429 train loss  0.010612 valid loss  0.007238\n",
      "epoch  430 train loss  0.013219 valid loss  0.007162\n",
      "epoch  431 train loss  0.012270 valid loss  0.008826\n",
      "epoch  432 train loss  0.009374 valid loss  0.007743\n",
      "epoch  433 train loss  0.009522 valid loss  0.010794\n",
      "epoch  434 train loss  0.010942 valid loss  0.009667\n",
      "epoch  435 train loss  0.011183 valid loss  0.007515\n",
      "epoch  436 train loss  0.010064 valid loss  0.006863\n",
      "epoch  437 train loss  0.009452 valid loss  0.007169\n",
      "epoch  438 train loss  0.010659 valid loss  0.007624\n",
      "epoch  439 train loss  0.010802 valid loss  0.009902\n",
      "epoch  440 train loss  0.010923 valid loss  0.008574\n",
      "epoch  441 train loss  0.008699 valid loss  0.008935\n",
      "epoch  442 train loss  0.010376 valid loss  0.010406\n",
      "epoch  443 train loss  0.010783 valid loss  0.008574\n",
      "epoch  444 train loss  0.011157 valid loss  0.008043\n",
      "epoch  445 train loss  0.011168 valid loss  0.009364\n",
      "epoch  446 train loss  0.009414 valid loss  0.007012\n",
      "epoch  447 train loss  0.010749 valid loss  0.007486\n",
      "epoch  448 train loss  0.011586 valid loss  0.007404\n",
      "epoch  449 train loss  0.008882 valid loss  0.007999\n",
      "epoch  450 train loss  0.010335 valid loss  0.009014\n",
      "epoch  451 train loss  0.012382 valid loss  0.014969\n",
      "epoch  452 train loss  0.013094 valid loss  0.012266\n",
      "epoch  453 train loss  0.011137 valid loss  0.007712\n",
      "epoch  454 train loss  0.013431 valid loss  0.008483\n",
      "epoch  455 train loss  0.013664 valid loss  0.012199\n",
      "epoch  456 train loss  0.013902 valid loss  0.032529\n",
      "epoch  457 train loss  0.024724 valid loss  0.025860\n",
      "epoch  458 train loss  0.014207 valid loss  0.008526\n",
      "epoch  459 train loss  0.010691 valid loss  0.007605\n",
      "epoch  460 train loss  0.011391 valid loss  0.009217\n",
      "epoch  461 train loss  0.009959 valid loss  0.007908\n",
      "epoch  462 train loss  0.008143 valid loss  0.007677\n",
      "epoch  463 train loss  0.010488 valid loss  0.010611\n",
      "epoch  464 train loss  0.014200 valid loss  0.020517\n",
      "epoch  465 train loss  0.014905 valid loss  0.013509\n",
      "epoch  466 train loss  0.012170 valid loss  0.008604\n",
      "epoch  467 train loss  0.011277 valid loss  0.007735\n",
      "epoch  468 train loss  0.009990 valid loss  0.008912\n",
      "epoch  469 train loss  0.008821 valid loss  0.007829\n",
      "epoch  470 train loss  0.009411 valid loss  0.008501\n",
      "epoch  471 train loss  0.010522 valid loss  0.007746\n",
      "epoch  472 train loss  0.012291 valid loss  0.008982\n",
      "epoch  473 train loss  0.014182 valid loss  0.006932\n",
      "epoch  474 train loss  0.009352 valid loss  0.006952\n",
      "epoch  475 train loss  0.009412 valid loss  0.007656\n",
      "epoch  476 train loss  0.008760 valid loss  0.007408\n",
      "epoch  477 train loss  0.009362 valid loss  0.008555\n",
      "epoch  478 train loss  0.010496 valid loss  0.011411\n",
      "epoch  479 train loss  0.009902 valid loss  0.010068\n",
      "epoch  480 train loss  0.010275 valid loss  0.008244\n",
      "epoch  481 train loss  0.010028 valid loss  0.008015\n",
      "epoch  482 train loss  0.010019 valid loss  0.008167\n",
      "epoch  483 train loss  0.012283 valid loss  0.009651\n",
      "epoch  484 train loss  0.016018 valid loss  0.008644\n",
      "epoch  485 train loss  0.011001 valid loss  0.009360\n",
      "epoch  486 train loss  0.012653 valid loss  0.013824\n",
      "epoch  487 train loss  0.012229 valid loss  0.015565\n",
      "epoch  488 train loss  0.012433 valid loss  0.012837\n",
      "epoch  489 train loss  0.012518 valid loss  0.007979\n",
      "epoch  490 train loss  0.015970 valid loss  0.009405\n",
      "epoch  491 train loss  0.018540 valid loss  0.014099\n",
      "epoch  492 train loss  0.017197 valid loss  0.012877\n",
      "epoch  493 train loss  0.013103 valid loss  0.008446\n",
      "epoch  494 train loss  0.010107 valid loss  0.007232\n",
      "epoch  495 train loss  0.010196 valid loss  0.008433\n",
      "epoch  496 train loss  0.014719 valid loss  0.011127\n",
      "epoch  497 train loss  0.013874 valid loss  0.014876\n",
      "epoch  498 train loss  0.012652 valid loss  0.008404\n",
      "epoch  499 train loss  0.010113 valid loss  0.009641\n",
      "epoch  500 train loss  0.012377 valid loss  0.011978\n"
     ]
    }
   ],
   "source": [
    "key, train_key = jax.random.split(key)\n",
    "model = Dipole_Moment()\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from typing import List\n",
    "\n",
    "def create_loss_plot(\n",
    "    train_loss: List[np.ndarray],\n",
    "    val_loss: List[np.ndarray],\n",
    "    train_label: str,\n",
    "    val_label: str,\n",
    "    title: str,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a Plotly figure with training and validation loss curves and save it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        train_loss (List[np.ndarray]): List of training loss values.\n",
    "        val_loss (List[np.ndarray]): List of validation loss values.\n",
    "        train_label (str): Label for the training loss curve.\n",
    "        val_label (str): Label for the validation loss curve.\n",
    "        title (str): Title of the plot.\n",
    "        filename (str): Filename to save the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_loss_list = [float(loss) for loss in train_loss]\n",
    "    val_loss_list = [float(loss) for loss in val_loss]\n",
    "\n",
    "    trace_train = go.Scatter(y=train_loss_list, mode=\"lines\", name=train_label)\n",
    "    trace_val = go.Scatter(y=val_loss_list, mode=\"lines\", name=val_label)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(trace_train)\n",
    "    fig.add_trace(trace_val)\n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title=\"Epoch\", yaxis_title=\"Loss\", legend_title=\"Legend\"\n",
    "    )\n",
    "    pio.write_html(fig, filename)\n",
    "    #fig.show()\n",
    "\n",
    "create_loss_plot(\n",
    "    list_train_loss,\n",
    "    list_val_loss,\n",
    "    \"Training Loss\",\n",
    "    \"Validation Loss\",\n",
    "    \"Training vs Validation Loss (Train)\",\n",
    "    \"train_vs_val_train_dipole_moment.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[ 1.2799603  -0.17976862  1.5490792 ]\n",
      "mean squared error 0.032999963\n",
      "positions \n",
      " [[ 1.3739698  -0.36267418  1.7607656 ]\n",
      " [ 1.0720738   0.6241141  -0.73667634]\n",
      " [-0.72627133 -1.6513554   2.6751056 ]\n",
      " [ 0.36511922 -0.6054491   4.4396477 ]\n",
      " [-1.0101337  -0.31764832  0.65454173]\n",
      " [ 2.8003755  -0.15685865  4.326635  ]\n",
      " [ 1.8958619  -2.5351772   3.4812946 ]\n",
      " [ 0.08705649  2.1099863   1.0994254 ]\n",
      " [ 0.6552997  -3.044066    1.1402597 ]\n",
      " [ 0.6292777  -1.7157842  -0.89606315]\n",
      " [-0.9160637   0.91140497  2.9305716 ]\n",
      " [ 1.3739641   1.775827    3.3651717 ]\n",
      " [ 2.5825186   2.0538971   0.9288267 ]\n",
      " [ 2.7610474  -2.0349882   0.22516835]\n",
      " [ 3.5957031  -1.5967042   2.3485258 ]\n",
      " [ 3.7129374   0.07741954  0.03268949]\n",
      " [ 3.9070163   0.6479711   2.2983937 ]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[ 1.2799603  -0.17976862  1.5490792 ]\n",
      "mean squared error 0.032999963\n",
      "positions \n",
      " [[ 1.3739698  -0.36267418  1.7607656 ]\n",
      " [ 1.0720738   0.6241141  -0.73667634]\n",
      " [-0.72627133 -1.6513554   2.6751056 ]\n",
      " [ 0.36511922 -0.6054491   4.4396477 ]\n",
      " [-1.0101337  -0.31764832  0.65454173]\n",
      " [ 2.8003755  -0.15685865  4.326635  ]\n",
      " [ 1.8958619  -2.5351772   3.4812946 ]\n",
      " [ 0.08705649  2.1099863   1.0994254 ]\n",
      " [ 0.6552997  -3.044066    1.1402597 ]\n",
      " [ 0.6292777  -1.7157842  -0.89606315]\n",
      " [-0.9160637   0.91140497  2.9305716 ]\n",
      " [ 1.3739641   1.775827    3.3651717 ]\n",
      " [ 2.5825186   2.0538971   0.9288267 ]\n",
      " [ 2.7610474  -2.0349882   0.22516835]\n",
      " [ 3.5957031  -1.5967042   2.3485258 ]\n",
      " [ 3.7129374   0.07741954  0.03268949]\n",
      " [ 3.9070163   0.6479711   2.2983937 ]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions[0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa\n",
      "target\n",
      "[ 1.5010834  -0.35230795  1.6916788 ]\n",
      "prediction\n",
      "[ 1.2836092  -0.18835199  1.5632188 ]\n",
      "mean squared error 0.030226184\n",
      "positions \n",
      " [[100.       100.       100.      ]\n",
      " [ 99.698105 100.986786  97.502556]\n",
      " [ 97.89976   98.71132  100.91434 ]\n",
      " [ 98.99115   99.757225 102.67888 ]\n",
      " [ 97.6159   100.04503   98.893776]\n",
      " [101.42641  100.20582  102.56587 ]\n",
      " [100.52189   97.8275   101.72053 ]\n",
      " [ 98.71309  102.472664  99.33866 ]\n",
      " [ 99.28133   97.31861   99.37949 ]\n",
      " [ 99.25531   98.64689   97.34317 ]\n",
      " [ 97.70997  101.27408  101.16981 ]\n",
      " [ 99.99999  102.138504 101.60441 ]\n",
      " [101.20855  102.41657   99.16806 ]\n",
      " [101.38708   98.32768   98.4644  ]\n",
      " [102.22173   98.76597  100.58776 ]\n",
      " [102.33897  100.440094  98.27193 ]\n",
      " [102.53304  101.01064  100.53763 ]]\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "positions-=positions[0,...]\n",
    "positions+=jnp.array([100,100,100])\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        #print('Embed',x.shape)\n",
    "        #print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                #print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                #print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            #print('Residual',x.shape)\n",
    "\n",
    "        # 5. Predict atomic energies with an ordinary dense layer.\n",
    "        #element_bias = self.param(\n",
    "        #    \"element_bias\",\n",
    "        #    lambda rng, shape: jnp.zeros(shape),\n",
    "        #    (self.max_atomic_number + 1),\n",
    "        #)\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 9, 1)\n",
    "        #print('After dense:',x.shape)\n",
    "        x=jnp.sum(x, axis=-4) \n",
    "        #print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        #print('After slicing:' ,x.shape)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "\n",
    "        # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "        # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        \n",
    "        dipole = self.dipole_moment(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "dm_model = MP_Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "num_train=10\n",
    "num_val=2\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "params = dm_model.init(key,\n",
    "    atomic_numbers=train_data['atomic_numbers'][0],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "moment = dm_model.apply(\n",
    "            params,\n",
    "            atomic_numbers=train_data[\"atomic_numbers\"][0],\n",
    "            positions=train_data[\"positions\"][0],\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=None,\n",
    "            batch_size=1,\n",
    "        )\n",
    "print(moment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update', 'batch_size'))\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole = model_apply(\n",
    "      params,\n",
    "      atomic_numbers=batch['atomic_numbers'],\n",
    "      positions=batch['positions'],\n",
    "      dst_idx=batch['dst_idx'],\n",
    "      src_idx=batch['src_idx'],\n",
    "      batch_segments=batch['batch_segments'],\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "      dipole_prediction=dipole,\n",
    "      dipole_target=batch['dipole_moment']\n",
    "    )\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'batch_size'))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "  dipole = model_apply(\n",
    "    params,\n",
    "    atomic_numbers=batch['atomic_numbers'],\n",
    "    positions=batch['positions'],\n",
    "    dst_idx=batch['dst_idx'],\n",
    "    src_idx=batch['src_idx'],\n",
    "    batch_segments=batch['batch_segments'],\n",
    "    batch_size=batch_size\n",
    "  )\n",
    "  loss = mean_squared_loss(\n",
    "    energy_prediction=dipole,\n",
    "    energy_target=batch['dipole_moment']\n",
    "  )\n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(train_data['atomic_numbers']))\n",
    "  params = model.init(init_key,\n",
    "    atomic_numbers=train_data['atomic_numbers'],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Batches for the validation set need to be prepared only once.\n",
    "  key, shuffle_key = jax.random.split(key)\n",
    "  valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Prepare batches.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "    # Loop over train batches.\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_batches):\n",
    "      params, opt_state, loss= train_step(\n",
    "        model_apply=model.apply,\n",
    "        optimizer_update=optimizer.update,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        opt_state=opt_state,\n",
    "        params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    valid_loss = 0.0\n",
    "    for i, batch in enumerate(valid_batches):\n",
    "      loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        params=params\n",
    "      )\n",
    "      valid_loss += (loss - valid_loss)/(i+1)\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "    print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
