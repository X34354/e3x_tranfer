{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import e3x\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "# Disable future warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment of inertia tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moment_of_inertia_tensor(masses, positions):\n",
    "  diag = jnp.sum(positions**2, axis=-1)[..., None, None]*jnp.eye(3)\n",
    "  outer = positions[..., None, :] * positions[..., :, None]\n",
    "  return jnp.sum(masses[..., None, None] * (diag - outer), axis=-3)\n",
    "\n",
    "def generate_datasets(key, num_train=1000, num_valid=100, num_points=10, min_mass=0.0, max_mass=1.0, stdev=1.0):\n",
    "  # Generate random keys.\n",
    "  train_position_key, train_masses_key, valid_position_key, valid_masses_key = jax.random.split(key, num=4)\n",
    "\n",
    "  # Draw random point masses with random positions.\n",
    "  train_positions = stdev * jax.random.normal(train_position_key,  shape=(num_train, num_points, 3))\n",
    "  train_masses = jax.random.uniform(train_masses_key, shape=(num_train, num_points), minval=min_mass, maxval=max_mass)\n",
    "  valid_positions = stdev * jax.random.normal(valid_position_key,  shape=(num_valid, num_points, 3))\n",
    "  valid_masses = jax.random.uniform(valid_masses_key, shape=(num_valid, num_points), minval=min_mass, maxval=max_mass)\n",
    "\n",
    "  # Calculate moment of inertia tensors.\n",
    "  train_inertia_tensor = calculate_moment_of_inertia_tensor(train_masses, train_positions)\n",
    "  valid_inertia_tensor = calculate_moment_of_inertia_tensor(valid_masses, valid_positions)\n",
    "\n",
    "  # Return final train and validation datasets.\n",
    "  train_data = dict(positions=train_positions, masses=train_masses, inertia_tensor=train_inertia_tensor)\n",
    "  valid_data = dict(positions=valid_positions, masses=valid_masses, inertia_tensor=valid_inertia_tensor)\n",
    "  return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  features = 8\n",
    "  max_degree = 1\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, masses, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "    # 1. Initialize features.\n",
    "    x = jnp.concatenate((masses[..., None], positions), axis=-1) # Shape (..., N, 4).\n",
    "    x = x[..., None, :, None]  # Shape (..., N, 1, 4, 1).\n",
    "\n",
    "    # 2. Apply transformations.\n",
    "    x = e3x.nn.Dense(features=self.features)(x)  # Shape (..., N, 1, 4, features).\n",
    "    x = e3x.nn.TensorDense(max_degree=self.max_degree)(x)  # Shape (..., N, 2, (max_degree+1)**2, features).\n",
    "    x = e3x.nn.TensorDense(  # Shape (..., N, 2, 9, 1).\n",
    "        features=1,\n",
    "        max_degree=2,\n",
    "    )(x)\n",
    "    # Try it: Zero-out irrep of degree 1 to only produce symmetric output tensors.\n",
    "    # x = x.at[..., :, 1:4, :].set(0)\n",
    "\n",
    "    # 3. Collect even irreps from feature channel 0 and sum over contributions from individual points.\n",
    "    x = jnp.sum(x[..., 0, :, 0], axis=-2)  # Shape (..., (max_degree+1)**2).\n",
    "\n",
    "    # 4. Convert output irreps to 3x3 matrix and return.\n",
    "    cg = e3x.so3.clebsch_gordan(max_degree1=1, max_degree2=1, max_degree3=2)  # Shape (4, 4, 9).\n",
    "    y = jnp.einsum('...l,nml->...nm', x, cg[1:, 1:, :])  # Shape (..., 3, 3).\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(prediction, target):\n",
    "  return jnp.mean(optax.l2_loss(prediction, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "    loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  inertia_tensor = model_apply(params, batch['masses'], batch['positions'])\n",
    "  loss = mean_squared_loss(inertia_tensor, batch['inertia_tensor'])\n",
    "  return loss\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  params = model.init(init_key, train_data['masses'][0:1], train_data['positions'][0:1])\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Determine the number of training steps per epoch.\n",
    "  train_size = len(train_data['masses'])\n",
    "  steps_per_epoch = train_size//batch_size\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perms = jax.random.permutation(shuffle_key, train_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Loop over all batches.\n",
    "    train_loss = 0.0  # For keeping a running average of the loss.\n",
    "    for i, perm in enumerate(perms):\n",
    "      batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "      params, opt_state, loss = train_step(\n",
    "          model_apply=model.apply,\n",
    "          optimizer_update=optimizer.update,\n",
    "          batch=batch,\n",
    "          opt_state=opt_state,\n",
    "          params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on the test set after each training epoch.\n",
    "    valid_loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=valid_data,\n",
    "        params=params\n",
    "    )\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch {epoch : 4d} train loss {train_loss : 8.6f} valid loss {valid_loss : 8.6f}\")\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.002\n",
    "num_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(train_data['masses'][0:1].shape)\\nprint(train_data['positions'][0:1].shape)\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(train_data['masses'][0:1].shape)\n",
    "print(train_data['positions'][0:1].shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'key, train_key = jax.random.split(key)\\nmodel = Model()\\nparams = train_model(\\n  key=train_key,\\n  model=model,\\n  train_data=train_data,\\n  valid_data=valid_data,\\n  num_epochs=num_epochs,\\n  learning_rate=learning_rate,\\n  batch_size=batch_size,\\n)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''key, train_key = jax.random.split(key)\n",
    "model = Model()\n",
    "params = train_model(\n",
    "  key=train_key,\n",
    "  model=model,\n",
    "  train_data=train_data,\n",
    "  valid_data=valid_data,\n",
    "  num_epochs=num_epochs,\n",
    "  learning_rate=learning_rate,\n",
    "  batch_size=batch_size,\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i = 0\\nmasses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\\nprediction = model.apply(params, masses, positions)\\n\\nprint('target')\\nprint(target)\\nprint('prediction')\\nprint(prediction)\\nprint('mean squared error', jnp.mean((prediction-target)**2))\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''i = 0\n",
    "masses, positions, target = valid_data['masses'][i], valid_data['positions'][i], valid_data['inertia_tensor'][i]\n",
    "prediction = model.apply(params, masses, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "R\n",
      "z\n",
      "E\n",
      "F\n",
      "F_units\n",
      "e_unit\n",
      "r_unit\n",
      "name\n",
      "theory\n",
      "D\n",
      "D_units\n",
      "Q\n",
      "README\n",
      "F_min\n",
      "F_max\n",
      "F_mean\n",
      "F_var\n",
      "E_min\n",
      "E_max\n",
      "E_mean\n",
      "E_var\n",
      "md5\n",
      "Dipole moment shape array (2580, 3)\n",
      "Dipole moment units eAng\n",
      "Atomic numbers [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "filename = \"Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.2580.R_E_F_D_Q.npz\"\n",
    "dataset= np.load(filename)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "print('Dipole moment shape array',dataset['D'].shape)\n",
    "print('Dipole moment units', dataset['D_units'])\n",
    "#print('R units', dataset['R_units'])\n",
    "print('Atomic numbers', dataset['z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(filename, key, num_train, num_valid):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"E\"])\n",
    "\n",
    "    Z = jnp.full(1, 23)\n",
    "    Z = jnp.append(Z, jnp.full(16, 14))\n",
    "    Z=jnp.expand_dims(Z,axis=0)\n",
    "    Z=jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][train_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][train_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][train_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        #energy=jnp.asarray(dataset[\"E\"][valid_choice, 0] - mean_energy),\n",
    "        #forces=jnp.asarray(dataset[\"F\"][valid_choice]),\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        dipole_moment= jnp.asarray(dataset[\"D\"][valid_choice]),\n",
    "        # atomic_numbers=jnp.asarray(z_hack),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "    return train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "num_train=2000\n",
    "num_val=500\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "print(train_data['dipole_moment'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "    # features = 1\n",
    "    # max_degree = 1\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "        # 1. Initialize features\n",
    "        \"\"\"dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "        print('dst_idx',dst_idx.shape)\n",
    "        dst_idx = jnp.expand_dims(dst_idx, axis=0)\n",
    "        src_idx = jnp.expand_dims(src_idx, axis=0)\n",
    "        dst_idx = jnp.repeat(dst_idx, positions.shape[0], axis=0)\n",
    "        src_idx = jnp.repeat(src_idx, positions.shape[0], axis=0)\n",
    "        print('dst_idx after expansion',dst_idx.shape)\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst\n",
    "        print('positions',positions.shape)\n",
    "        print('displacements',displacements.shape)\n",
    "        #x = e3x.nn.Embed(num_embeddings=self.max_atomic_number + 1, features=self.features)(atomic_numbers)\n",
    "        Z= e3x.nn.Embed(num_embeddings=23, features=272)(atomic_numbers)\n",
    "        print('atomic_numbers',atomic_numbers.shape)\n",
    "        print('Z',Z.shape)\n",
    "        Z=Z[0,...]\n",
    "        Z=jnp.reshape(Z, (272, 17, 1))\n",
    "        print('Z',Z.shape)\n",
    "        x = jnp.concatenate((Z, displacements), axis=-1)\"\"\"\n",
    "\n",
    "        positions -= positions[0, ...]\n",
    "\n",
    "        x = jnp.concatenate(\n",
    "            (atomic_numbers[..., None], positions), axis=-1\n",
    "        )  # Shape (..., N, 4).\n",
    "        # print(\"Initial shape:\", x.shape)\n",
    "        x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "        # print(\"x shape:\", x.shape)\n",
    "        # 2. Apply transformations.\n",
    "        x = e3x.nn.Dense(features=32)(x)\n",
    "        # print(\"After Dense layer:\", x.shape)\n",
    "        x = e3x.nn.TensorDense(features=1, max_degree=1)(x)\n",
    "        # print(\"After TensorDense layer:\", x.shape)\n",
    "        # x = e3x.nn.Dense(features=16)(x)\n",
    "        # x = e3x.nn.Dense(features=1)(x)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        # print(\"After sum:\", x.shape)\n",
    "        # x=jnp.sum(x, axis=0)\n",
    "        # print(\"After second sum:\", x.shape)\n",
    "        y = x[..., 1, 1:4, 0]\n",
    "        # y = x[1, 1:4, 0]\n",
    "        # y=y[None,...]\n",
    "        # print(\"After slicing:\", y.shape)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "    # features = 1\n",
    "    # max_degree = 1\n",
    "    @nn.compact\n",
    "    def __call__(self,atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "        # 1. Initialize features\n",
    "        positions -= positions[0, ...]\n",
    "        x = jnp.concatenate(\n",
    "            (atomic_numbers[..., None], positions), axis=-1\n",
    "        )  # Shape (..., N, 4).\n",
    "        # print(\"Initial shape:\", x.shape)\n",
    "        x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "\n",
    "        # print(\"x shape:\", x.shape)\n",
    "\n",
    "        x = e3x.nn.Dense(features=512)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        x = nn.LayerNorm()(x)\n",
    "        # Second Dense block\n",
    "\n",
    "        x = e3x.nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "\n",
    "        # Layer Normalization\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Adding more complexity with TensorDense and additional layers\n",
    "        x = e3x.nn.TensorDense(features=64, max_degree=2)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = e3x.nn.TensorDense(features=32, max_degree=2)(x)\n",
    "        x = nn.relu(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=1, max_degree=1)(x)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        y = x[..., 1, 1:4, 0]\n",
    "\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "    # features = 1\n",
    "    # max_degree = 1\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "        # 1. Initialize features\n",
    "        #positions -= positions[0, ...]\n",
    "        positions = positions - positions[:, 0:1, :]\n",
    "        x = jnp.concatenate(\n",
    "            (atomic_numbers[..., None], positions), axis=-1\n",
    "        )  # Shape (..., N, 4).\n",
    "        x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "\n",
    "        # Incremento de complejidad con más capas densas\n",
    "        x = e3x.nn.Dense(features=1024)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.Dense(features=512)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.Dense(features=256)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Más capas densas\n",
    "        x = e3x.nn.Dense(features=128)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Capas TensorDense adicionales para más complejidad\n",
    "        x = e3x.nn.TensorDense(features=64, max_degree=2)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=32, max_degree=2)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=16, max_degree=2)(x)\n",
    "        x = e3x.nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Capa final\n",
    "        x = e3x.nn.TensorDense(features=1, max_degree=1)(x)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        y = x[..., 1, 1:4, 0]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dipole_Moment(nn.Module):\n",
    "    # features = 1\n",
    "    # max_degree = 1\n",
    "    @nn.compact\n",
    "    def __call__(self, atomic_numbers, positions):  # Shapes (..., N) and (..., N, 3).\n",
    "        # 1. Initialize features\n",
    "        positions -= positions[0, ...]\n",
    "        x = jnp.concatenate(\n",
    "            (atomic_numbers[..., None], positions), axis=-1\n",
    "        )  # Shape (..., N, 4).\n",
    "        x = x[..., None, :, None]  # Shape (..., N, 1, 3, 1).\n",
    "\n",
    "        # Añadir bloque Transformer para relaciones más profundas\n",
    "        x = nn.DenseGeneral(features=(512,), axis=-1)(x)\n",
    "        x = nn.MultiHeadDotProductAttention(num_heads=8)(x, x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "        x = nn.DenseGeneral(features=(256,), axis=-1)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Capas Dense adicionales\n",
    "        x = e3x.nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Capas TensorDense adicionales para más complejidad\n",
    "        x = e3x.nn.TensorDense(features=64, max_degree=1)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=32, max_degree=1)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        x = e3x.nn.TensorDense(features=16, max_degree=1)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.LayerNorm()(x)\n",
    "\n",
    "        # Capa final\n",
    "        x = e3x.nn.TensorDense(features=1, max_degree=1)(x)\n",
    "        x = jnp.sum(x, axis=-4)\n",
    "        y = x[..., 1, 1:4, 0]\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dm_model = Dipole_Moment()\\nkey = jax.random.PRNGKey(0)\\n\\n# Generate train and test datasets.\\nkey, data_key = jax.random.split(key)\\ntrain_data, valid_data = generate_datasets(data_key)\\nparams = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nmoment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\\nprint(moment.shape)\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dm_model = Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "train_data, valid_data = generate_datasets(data_key)\n",
    "params = dm_model.init(key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "moment=dm_model.apply(params,train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "print(moment.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update'))\n",
    "def train_step(model_apply, optimizer_update, batch, opt_state, params):\n",
    "\n",
    "    def loss_fn(params):\n",
    "        print(batch[\"positions\"].shape)\n",
    "        dipole_moment = model_apply(params, batch[\"atomic_numbers\"], batch[\"positions\"])\n",
    "        loss = mean_squared_loss(dipole_moment, batch[\"dipole_moment\"])\n",
    "        return loss\n",
    "\n",
    "    loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply',))\n",
    "def eval_step(model_apply, batch, params):\n",
    "  dipole_moment = model_apply(params,batch['atomic_numbers'],batch['positions'])\n",
    "  loss = mean_squared_loss(dipole_moment, batch['dipole_moment'])\n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "    # Initialize model parameters and optimizer state.\n",
    "    key, init_key = jax.random.split(key)\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    params = model.init(init_key, train_data['atomic_numbers'][0:1], train_data['positions'][0:1])\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    # Initialize variables for tracking the best model and loss\n",
    "    best_params = None\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    # Determine the number of training steps per epoch.\n",
    "    train_size = len(train_data['positions'])\n",
    "    steps_per_epoch = train_size // batch_size\n",
    "    \n",
    "    # Train for 'num_epochs' epochs.\n",
    "    list_train_loss = []\n",
    "    list_val_loss = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Draw random permutations for fetching batches from the train data.\n",
    "        key, shuffle_key = jax.random.split(key)\n",
    "        perms = jax.random.permutation(shuffle_key, train_size)\n",
    "        perms = perms[:steps_per_epoch * batch_size]  # Skip the last batch (if incomplete).\n",
    "        perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "        \n",
    "        # Loop over all batches.\n",
    "        train_loss = 0.0  # For keeping a running average of the loss.\n",
    "        \n",
    "        for i, perm in enumerate(perms):\n",
    "            batch = {k: v[perm, ...] for k, v in train_data.items()}\n",
    "            \n",
    "            params, opt_state, loss = train_step(\n",
    "                model_apply=model.apply,\n",
    "                optimizer_update=optimizer.update,\n",
    "                batch=batch,\n",
    "                opt_state=opt_state,\n",
    "                params=params\n",
    "            )\n",
    "            train_loss += (loss - train_loss) / (i + 1)\n",
    "        \n",
    "        # Evaluate on the validation set after each training epoch.\n",
    "        valid_loss = eval_step(\n",
    "            model_apply=model.apply,\n",
    "            batch=valid_data,\n",
    "            params=params\n",
    "        )\n",
    "        list_val_loss.append(valid_loss)\n",
    "        list_train_loss.append(train_loss)\n",
    "        \n",
    "        # Check if the current validation loss is the best we've seen so far.\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_params = params\n",
    "        \n",
    "        # Print progress.\n",
    "        print(f\"epoch {epoch:4d} train loss {train_loss:8.6f} valid loss {valid_loss:8.6f}\")\n",
    "    \n",
    "    # Return the best model parameters and the lists of train and validation losses.\n",
    "    return best_params, list_train_loss, list_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PRNGKey for random number generation.\n",
    "key = jax.random.PRNGKey(0)\n",
    "num_train=2000\n",
    "num_val=500\n",
    "train_data, valid_data = prepare_datasets(filename,key, num_train,num_val)\n",
    "\n",
    "# Define training hyperparameters.\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2000\n",
    "batch_size = 512\n",
    "key, train_key = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 17, 3)\n",
      "epoch    1 train loss 6.460361 valid loss 13.729761\n",
      "epoch    2 train loss 19.997490 valid loss 4.872768\n",
      "epoch    3 train loss 8.838713 valid loss 11.893063\n",
      "epoch    4 train loss 8.943563 valid loss 11.990310\n",
      "epoch    5 train loss 7.855114 valid loss 9.153582\n",
      "epoch    6 train loss 22.836384 valid loss 10.301203\n",
      "epoch    7 train loss 9.614491 valid loss 10.841181\n",
      "epoch    8 train loss 9.858075 valid loss 44.769146\n",
      "epoch    9 train loss 19.239082 valid loss 5.107082\n",
      "epoch   10 train loss 8.340735 valid loss 5.596352\n",
      "epoch   11 train loss 4.511841 valid loss 2.455589\n",
      "epoch   12 train loss 7.290314 valid loss 9.737181\n",
      "epoch   13 train loss 7.296909 valid loss 3.426876\n",
      "epoch   14 train loss 4.550248 valid loss 8.417161\n",
      "epoch   15 train loss 5.250689 valid loss 3.791478\n",
      "epoch   16 train loss 3.263251 valid loss 3.767081\n",
      "epoch   17 train loss 3.987390 valid loss 5.382708\n",
      "epoch   18 train loss 3.394279 valid loss 2.151001\n",
      "epoch   19 train loss 2.034329 valid loss 4.759785\n",
      "epoch   20 train loss 4.244462 valid loss 2.306168\n",
      "epoch   21 train loss 2.244559 valid loss 3.272382\n",
      "epoch   22 train loss 3.103659 valid loss 3.143887\n",
      "epoch   23 train loss 2.775330 valid loss 2.339470\n",
      "epoch   24 train loss 2.115280 valid loss 4.618588\n",
      "epoch   25 train loss 2.791596 valid loss 1.798229\n",
      "epoch   26 train loss 1.473312 valid loss 1.724951\n",
      "epoch   27 train loss 1.510065 valid loss 1.805302\n",
      "epoch   28 train loss 1.522307 valid loss 1.249938\n",
      "epoch   29 train loss 1.075984 valid loss 1.077617\n",
      "epoch   30 train loss 1.601454 valid loss 2.140011\n",
      "epoch   31 train loss 1.975146 valid loss 1.048550\n",
      "epoch   32 train loss 1.317053 valid loss 0.895688\n",
      "epoch   33 train loss 1.257103 valid loss 1.437720\n",
      "epoch   34 train loss 2.024931 valid loss 0.882919\n",
      "epoch   35 train loss 0.933420 valid loss 0.758673\n",
      "epoch   36 train loss 0.755234 valid loss 1.212986\n",
      "epoch   37 train loss 0.954592 valid loss 1.117918\n",
      "epoch   38 train loss 1.157229 valid loss 1.494987\n",
      "epoch   39 train loss 1.899130 valid loss 1.660610\n",
      "epoch   40 train loss 1.591142 valid loss 1.694421\n",
      "epoch   41 train loss 0.948464 valid loss 0.876973\n",
      "epoch   42 train loss 0.972664 valid loss 1.814084\n",
      "epoch   43 train loss 1.238515 valid loss 0.981421\n",
      "epoch   44 train loss 0.982189 valid loss 0.817848\n",
      "epoch   45 train loss 0.918868 valid loss 0.912804\n",
      "epoch   46 train loss 1.192639 valid loss 0.718767\n",
      "epoch   47 train loss 0.722613 valid loss 0.895345\n",
      "epoch   48 train loss 0.703147 valid loss 0.449722\n",
      "epoch   49 train loss 0.565596 valid loss 0.485392\n",
      "epoch   50 train loss 0.619484 valid loss 0.706972\n",
      "epoch   51 train loss 0.627386 valid loss 0.694737\n",
      "epoch   52 train loss 1.322950 valid loss 0.996811\n",
      "epoch   53 train loss 0.985201 valid loss 0.716316\n",
      "epoch   54 train loss 1.270244 valid loss 1.497734\n",
      "epoch   55 train loss 1.238711 valid loss 0.546392\n",
      "epoch   56 train loss 0.651082 valid loss 0.551016\n",
      "epoch   57 train loss 0.617390 valid loss 0.663258\n",
      "epoch   58 train loss 0.648900 valid loss 1.948237\n",
      "epoch   59 train loss 1.360747 valid loss 0.773836\n",
      "epoch   60 train loss 0.596332 valid loss 0.663165\n",
      "epoch   61 train loss 0.494755 valid loss 0.462675\n",
      "epoch   62 train loss 0.513402 valid loss 0.470604\n",
      "epoch   63 train loss 0.555335 valid loss 0.315725\n",
      "epoch   64 train loss 0.362770 valid loss 0.556954\n",
      "epoch   65 train loss 0.488635 valid loss 0.584972\n",
      "epoch   66 train loss 0.456105 valid loss 0.237083\n",
      "epoch   67 train loss 0.291176 valid loss 0.260031\n",
      "epoch   68 train loss 0.281947 valid loss 0.403763\n",
      "epoch   69 train loss 0.567858 valid loss 0.261034\n",
      "epoch   70 train loss 0.460255 valid loss 0.295310\n",
      "epoch   71 train loss 0.479770 valid loss 0.282152\n",
      "epoch   72 train loss 0.307381 valid loss 0.215128\n",
      "epoch   73 train loss 0.425883 valid loss 0.356958\n",
      "epoch   74 train loss 0.490997 valid loss 0.339788\n",
      "epoch   75 train loss 0.278976 valid loss 0.298007\n",
      "epoch   76 train loss 0.341813 valid loss 0.338804\n",
      "epoch   77 train loss 0.406170 valid loss 0.492427\n",
      "epoch   78 train loss 0.372617 valid loss 0.402896\n",
      "epoch   79 train loss 0.274563 valid loss 0.165369\n",
      "epoch   80 train loss 0.149044 valid loss 0.181347\n",
      "epoch   81 train loss 0.188945 valid loss 0.267798\n",
      "epoch   82 train loss 0.220596 valid loss 0.174613\n",
      "epoch   83 train loss 0.188003 valid loss 0.124274\n",
      "epoch   84 train loss 0.173315 valid loss 0.125089\n",
      "epoch   85 train loss 0.141411 valid loss 0.098641\n",
      "epoch   86 train loss 0.131210 valid loss 0.158333\n",
      "epoch   87 train loss 0.124433 valid loss 0.180600\n",
      "epoch   88 train loss 0.157632 valid loss 0.179192\n",
      "epoch   89 train loss 0.143199 valid loss 0.089037\n",
      "epoch   90 train loss 0.104298 valid loss 0.156804\n",
      "epoch   91 train loss 0.146311 valid loss 0.219504\n",
      "epoch   92 train loss 0.183130 valid loss 0.160842\n",
      "epoch   93 train loss 0.150455 valid loss 0.106561\n",
      "epoch   94 train loss 0.119749 valid loss 0.151374\n",
      "epoch   95 train loss 0.143578 valid loss 0.091612\n",
      "epoch   96 train loss 0.113418 valid loss 0.162961\n",
      "epoch   97 train loss 0.127988 valid loss 0.183760\n",
      "epoch   98 train loss 0.149932 valid loss 0.098713\n",
      "epoch   99 train loss 0.107972 valid loss 0.118271\n",
      "epoch  100 train loss 0.151937 valid loss 0.123397\n",
      "epoch  101 train loss 0.133773 valid loss 0.140129\n",
      "epoch  102 train loss 0.125143 valid loss 0.074603\n",
      "epoch  103 train loss 0.106432 valid loss 0.094531\n",
      "epoch  104 train loss 0.099162 valid loss 0.256040\n",
      "epoch  105 train loss 0.213737 valid loss 0.151658\n",
      "epoch  106 train loss 0.114942 valid loss 0.124959\n",
      "epoch  107 train loss 0.119254 valid loss 0.138199\n",
      "epoch  108 train loss 0.116028 valid loss 0.075426\n",
      "epoch  109 train loss 0.093288 valid loss 0.088134\n",
      "epoch  110 train loss 0.102731 valid loss 0.087234\n",
      "epoch  111 train loss 0.103875 valid loss 0.113500\n",
      "epoch  112 train loss 0.092441 valid loss 0.076013\n",
      "epoch  113 train loss 0.098633 valid loss 0.081352\n",
      "epoch  114 train loss 0.100753 valid loss 0.103085\n",
      "epoch  115 train loss 0.094593 valid loss 0.078200\n",
      "epoch  116 train loss 0.093518 valid loss 0.091363\n",
      "epoch  117 train loss 0.089651 valid loss 0.165450\n",
      "epoch  118 train loss 0.146081 valid loss 0.075792\n",
      "epoch  119 train loss 0.099241 valid loss 0.081189\n",
      "epoch  120 train loss 0.127561 valid loss 0.279224\n",
      "epoch  121 train loss 0.170955 valid loss 0.089385\n",
      "epoch  122 train loss 0.108966 valid loss 0.119543\n",
      "epoch  123 train loss 0.153386 valid loss 0.109096\n",
      "epoch  124 train loss 0.109323 valid loss 0.143395\n",
      "epoch  125 train loss 0.123356 valid loss 0.108873\n",
      "epoch  126 train loss 0.108762 valid loss 0.067658\n",
      "epoch  127 train loss 0.099231 valid loss 0.106121\n",
      "epoch  128 train loss 0.102787 valid loss 0.101942\n",
      "epoch  129 train loss 0.095336 valid loss 0.101910\n",
      "epoch  130 train loss 0.089192 valid loss 0.080444\n",
      "epoch  131 train loss 0.098776 valid loss 0.079855\n",
      "epoch  132 train loss 0.092176 valid loss 0.079217\n",
      "epoch  133 train loss 0.067579 valid loss 0.058768\n",
      "epoch  134 train loss 0.059187 valid loss 0.051308\n",
      "epoch  135 train loss 0.091840 valid loss 0.088393\n",
      "epoch  136 train loss 0.101907 valid loss 0.161767\n",
      "epoch  137 train loss 0.136368 valid loss 0.107794\n",
      "epoch  138 train loss 0.117297 valid loss 0.127648\n",
      "epoch  139 train loss 0.095550 valid loss 0.062699\n",
      "epoch  140 train loss 0.076368 valid loss 0.100749\n",
      "epoch  141 train loss 0.094726 valid loss 0.129803\n",
      "epoch  142 train loss 0.094500 valid loss 0.097541\n",
      "epoch  143 train loss 0.079421 valid loss 0.127854\n",
      "epoch  144 train loss 0.139385 valid loss 0.104666\n",
      "epoch  145 train loss 0.080441 valid loss 0.061805\n",
      "epoch  146 train loss 0.061139 valid loss 0.109788\n",
      "epoch  147 train loss 0.093670 valid loss 0.088521\n",
      "epoch  148 train loss 0.093916 valid loss 0.075580\n",
      "epoch  149 train loss 0.092528 valid loss 0.089358\n",
      "epoch  150 train loss 0.081791 valid loss 0.154360\n",
      "epoch  151 train loss 0.147888 valid loss 0.063335\n",
      "epoch  152 train loss 0.067976 valid loss 0.056804\n",
      "epoch  153 train loss 0.080149 valid loss 0.069710\n",
      "epoch  154 train loss 0.075825 valid loss 0.058564\n",
      "epoch  155 train loss 0.065200 valid loss 0.054640\n",
      "epoch  156 train loss 0.053254 valid loss 0.168829\n",
      "epoch  157 train loss 0.104128 valid loss 0.085216\n",
      "epoch  158 train loss 0.083917 valid loss 0.112837\n",
      "epoch  159 train loss 0.078826 valid loss 0.133184\n",
      "epoch  160 train loss 0.087471 valid loss 0.065288\n",
      "epoch  161 train loss 0.100008 valid loss 0.052566\n",
      "epoch  162 train loss 0.075964 valid loss 0.078139\n",
      "epoch  163 train loss 0.095032 valid loss 0.115464\n",
      "epoch  164 train loss 0.083453 valid loss 0.136578\n",
      "epoch  165 train loss 0.094954 valid loss 0.077573\n",
      "epoch  166 train loss 0.058921 valid loss 0.080908\n",
      "epoch  167 train loss 0.055640 valid loss 0.088045\n",
      "epoch  168 train loss 0.062524 valid loss 0.045625\n",
      "epoch  169 train loss 0.051797 valid loss 0.047982\n",
      "epoch  170 train loss 0.056021 valid loss 0.051961\n",
      "epoch  171 train loss 0.052692 valid loss 0.042806\n",
      "epoch  172 train loss 0.051586 valid loss 0.054004\n",
      "epoch  173 train loss 0.056956 valid loss 0.067630\n",
      "epoch  174 train loss 0.059441 valid loss 0.050159\n",
      "epoch  175 train loss 0.048650 valid loss 0.050589\n",
      "epoch  176 train loss 0.059155 valid loss 0.066419\n",
      "epoch  177 train loss 0.077467 valid loss 0.077106\n",
      "epoch  178 train loss 0.068667 valid loss 0.064240\n",
      "epoch  179 train loss 0.063560 valid loss 0.115497\n",
      "epoch  180 train loss 0.077896 valid loss 0.076606\n",
      "epoch  181 train loss 0.064526 valid loss 0.068183\n",
      "epoch  182 train loss 0.068502 valid loss 0.094976\n",
      "epoch  183 train loss 0.078523 valid loss 0.077433\n",
      "epoch  184 train loss 0.065551 valid loss 0.047762\n",
      "epoch  185 train loss 0.063808 valid loss 0.078315\n",
      "epoch  186 train loss 0.081278 valid loss 0.067400\n",
      "epoch  187 train loss 0.063824 valid loss 0.049669\n",
      "epoch  188 train loss 0.070600 valid loss 0.080054\n",
      "epoch  189 train loss 0.074145 valid loss 0.040726\n",
      "epoch  190 train loss 0.049745 valid loss 0.070222\n",
      "epoch  191 train loss 0.059958 valid loss 0.054040\n",
      "epoch  192 train loss 0.072187 valid loss 0.055993\n",
      "epoch  193 train loss 0.055167 valid loss 0.054009\n",
      "epoch  194 train loss 0.050679 valid loss 0.059209\n",
      "epoch  195 train loss 0.054943 valid loss 0.041216\n",
      "epoch  196 train loss 0.057347 valid loss 0.078517\n",
      "epoch  197 train loss 0.085890 valid loss 0.057656\n",
      "epoch  198 train loss 0.053876 valid loss 0.056437\n",
      "epoch  199 train loss 0.064763 valid loss 0.085961\n",
      "epoch  200 train loss 0.065132 valid loss 0.057298\n",
      "epoch  201 train loss 0.070656 valid loss 0.136853\n",
      "epoch  202 train loss 0.095683 valid loss 0.068757\n",
      "epoch  203 train loss 0.069083 valid loss 0.096749\n",
      "epoch  204 train loss 0.079806 valid loss 0.062339\n",
      "epoch  205 train loss 0.055917 valid loss 0.064330\n",
      "epoch  206 train loss 0.067151 valid loss 0.055321\n",
      "epoch  207 train loss 0.065441 valid loss 0.098026\n",
      "epoch  208 train loss 0.086767 valid loss 0.061908\n",
      "epoch  209 train loss 0.155124 valid loss 0.221336\n",
      "epoch  210 train loss 0.164543 valid loss 0.173871\n",
      "epoch  211 train loss 0.244449 valid loss 0.153761\n",
      "epoch  212 train loss 0.116652 valid loss 0.073726\n",
      "epoch  213 train loss 0.063866 valid loss 0.084414\n",
      "epoch  214 train loss 0.108938 valid loss 0.130166\n",
      "epoch  215 train loss 0.090849 valid loss 0.076319\n",
      "epoch  216 train loss 0.055232 valid loss 0.048624\n",
      "epoch  217 train loss 0.055793 valid loss 0.061367\n",
      "epoch  218 train loss 0.079506 valid loss 0.108011\n",
      "epoch  219 train loss 0.077810 valid loss 0.054862\n",
      "epoch  220 train loss 0.047748 valid loss 0.052954\n",
      "epoch  221 train loss 0.050289 valid loss 0.049795\n",
      "epoch  222 train loss 0.046598 valid loss 0.031145\n",
      "epoch  223 train loss 0.034048 valid loss 0.034928\n",
      "epoch  224 train loss 0.042386 valid loss 0.056586\n",
      "epoch  225 train loss 0.041556 valid loss 0.040776\n",
      "epoch  226 train loss 0.038072 valid loss 0.039014\n",
      "epoch  227 train loss 0.033330 valid loss 0.049809\n",
      "epoch  228 train loss 0.034984 valid loss 0.041921\n",
      "epoch  229 train loss 0.041688 valid loss 0.039845\n",
      "epoch  230 train loss 0.043047 valid loss 0.076810\n",
      "epoch  231 train loss 0.081992 valid loss 0.039916\n",
      "epoch  232 train loss 0.035733 valid loss 0.050712\n",
      "epoch  233 train loss 0.052329 valid loss 0.054032\n",
      "epoch  234 train loss 0.055653 valid loss 0.053819\n",
      "epoch  235 train loss 0.051225 valid loss 0.038501\n",
      "epoch  236 train loss 0.040113 valid loss 0.057259\n",
      "epoch  237 train loss 0.068669 valid loss 0.025447\n",
      "epoch  238 train loss 0.044200 valid loss 0.041215\n",
      "epoch  239 train loss 0.042141 valid loss 0.026635\n",
      "epoch  240 train loss 0.031955 valid loss 0.040820\n",
      "epoch  241 train loss 0.050166 valid loss 0.046032\n",
      "epoch  242 train loss 0.051560 valid loss 0.099710\n",
      "epoch  243 train loss 0.055288 valid loss 0.046426\n",
      "epoch  244 train loss 0.044422 valid loss 0.045798\n",
      "epoch  245 train loss 0.047523 valid loss 0.038922\n",
      "epoch  246 train loss 0.054978 valid loss 0.062490\n",
      "epoch  247 train loss 0.039970 valid loss 0.061300\n",
      "epoch  248 train loss 0.044512 valid loss 0.030679\n",
      "epoch  249 train loss 0.033372 valid loss 0.024869\n",
      "epoch  250 train loss 0.038386 valid loss 0.033378\n",
      "epoch  251 train loss 0.033775 valid loss 0.033751\n",
      "epoch  252 train loss 0.037242 valid loss 0.040613\n",
      "epoch  253 train loss 0.039483 valid loss 0.053759\n",
      "epoch  254 train loss 0.057708 valid loss 0.070602\n",
      "epoch  255 train loss 0.058929 valid loss 0.029606\n",
      "epoch  256 train loss 0.042284 valid loss 0.051816\n",
      "epoch  257 train loss 0.042500 valid loss 0.038869\n",
      "epoch  258 train loss 0.042561 valid loss 0.074430\n",
      "epoch  259 train loss 0.050584 valid loss 0.046538\n",
      "epoch  260 train loss 0.053784 valid loss 0.039123\n",
      "epoch  261 train loss 0.045424 valid loss 0.048504\n",
      "epoch  262 train loss 0.040452 valid loss 0.043989\n",
      "epoch  263 train loss 0.048635 valid loss 0.050808\n",
      "epoch  264 train loss 0.035939 valid loss 0.052803\n",
      "epoch  265 train loss 0.047224 valid loss 0.043602\n",
      "epoch  266 train loss 0.053178 valid loss 0.050491\n",
      "epoch  267 train loss 0.055289 valid loss 0.039428\n",
      "epoch  268 train loss 0.047514 valid loss 0.116495\n",
      "epoch  269 train loss 0.067512 valid loss 0.026133\n",
      "epoch  270 train loss 0.037520 valid loss 0.035303\n",
      "epoch  271 train loss 0.052587 valid loss 0.024319\n",
      "epoch  272 train loss 0.037821 valid loss 0.038153\n",
      "epoch  273 train loss 0.041701 valid loss 0.022028\n",
      "epoch  274 train loss 0.042764 valid loss 0.066723\n",
      "epoch  275 train loss 0.048118 valid loss 0.049295\n",
      "epoch  276 train loss 0.042453 valid loss 0.062999\n",
      "epoch  277 train loss 0.058864 valid loss 0.022398\n",
      "epoch  278 train loss 0.038681 valid loss 0.028992\n",
      "epoch  279 train loss 0.039789 valid loss 0.055019\n",
      "epoch  280 train loss 0.041424 valid loss 0.042653\n",
      "epoch  281 train loss 0.039787 valid loss 0.041362\n",
      "epoch  282 train loss 0.035412 valid loss 0.034158\n",
      "epoch  283 train loss 0.040289 valid loss 0.052887\n",
      "epoch  284 train loss 0.049347 valid loss 0.054758\n",
      "epoch  285 train loss 0.039292 valid loss 0.025287\n",
      "epoch  286 train loss 0.026677 valid loss 0.025952\n",
      "epoch  287 train loss 0.034537 valid loss 0.048599\n",
      "epoch  288 train loss 0.047973 valid loss 0.030957\n",
      "epoch  289 train loss 0.038401 valid loss 0.022609\n",
      "epoch  290 train loss 0.027612 valid loss 0.034890\n",
      "epoch  291 train loss 0.033425 valid loss 0.041092\n",
      "epoch  292 train loss 0.040994 valid loss 0.024986\n",
      "epoch  293 train loss 0.041207 valid loss 0.035745\n",
      "epoch  294 train loss 0.038016 valid loss 0.052425\n",
      "epoch  295 train loss 0.043711 valid loss 0.048722\n",
      "epoch  296 train loss 0.042626 valid loss 0.026718\n",
      "epoch  297 train loss 0.030063 valid loss 0.048760\n",
      "epoch  298 train loss 0.039044 valid loss 0.044499\n",
      "epoch  299 train loss 0.032846 valid loss 0.029032\n",
      "epoch  300 train loss 0.041639 valid loss 0.053298\n",
      "epoch  301 train loss 0.037860 valid loss 0.017078\n",
      "epoch  302 train loss 0.019225 valid loss 0.032330\n",
      "epoch  303 train loss 0.036445 valid loss 0.052873\n",
      "epoch  304 train loss 0.048996 valid loss 0.032843\n",
      "epoch  305 train loss 0.026842 valid loss 0.036642\n",
      "epoch  306 train loss 0.032924 valid loss 0.036606\n",
      "epoch  307 train loss 0.030542 valid loss 0.026604\n",
      "epoch  308 train loss 0.022318 valid loss 0.022243\n",
      "epoch  309 train loss 0.021037 valid loss 0.030504\n",
      "epoch  310 train loss 0.033858 valid loss 0.047405\n",
      "epoch  311 train loss 0.040322 valid loss 0.026252\n",
      "epoch  312 train loss 0.032420 valid loss 0.029686\n",
      "epoch  313 train loss 0.033789 valid loss 0.039529\n",
      "epoch  314 train loss 0.033828 valid loss 0.037586\n",
      "epoch  315 train loss 0.033745 valid loss 0.029148\n",
      "epoch  316 train loss 0.032634 valid loss 0.026151\n",
      "epoch  317 train loss 0.026631 valid loss 0.028638\n",
      "epoch  318 train loss 0.026381 valid loss 0.018748\n",
      "epoch  319 train loss 0.026200 valid loss 0.029194\n",
      "epoch  320 train loss 0.031844 valid loss 0.032310\n",
      "epoch  321 train loss 0.034514 valid loss 0.028976\n",
      "epoch  322 train loss 0.026834 valid loss 0.029750\n",
      "epoch  323 train loss 0.028127 valid loss 0.020042\n",
      "epoch  324 train loss 0.021179 valid loss 0.020142\n",
      "epoch  325 train loss 0.021347 valid loss 0.019875\n",
      "epoch  326 train loss 0.023256 valid loss 0.023774\n",
      "epoch  327 train loss 0.023030 valid loss 0.026222\n",
      "epoch  328 train loss 0.024031 valid loss 0.018514\n",
      "epoch  329 train loss 0.021655 valid loss 0.022553\n",
      "epoch  330 train loss 0.030917 valid loss 0.020749\n",
      "epoch  331 train loss 0.034393 valid loss 0.038561\n",
      "epoch  332 train loss 0.035693 valid loss 0.023368\n",
      "epoch  333 train loss 0.021368 valid loss 0.018230\n",
      "epoch  334 train loss 0.026950 valid loss 0.019325\n",
      "epoch  335 train loss 0.021308 valid loss 0.018641\n",
      "epoch  336 train loss 0.024262 valid loss 0.022207\n",
      "epoch  337 train loss 0.025156 valid loss 0.027199\n",
      "epoch  338 train loss 0.026573 valid loss 0.024656\n",
      "epoch  339 train loss 0.023917 valid loss 0.024860\n",
      "epoch  340 train loss 0.024929 valid loss 0.017669\n",
      "epoch  341 train loss 0.022751 valid loss 0.022356\n",
      "epoch  342 train loss 0.028586 valid loss 0.030775\n",
      "epoch  343 train loss 0.023506 valid loss 0.027245\n",
      "epoch  344 train loss 0.038816 valid loss 0.034179\n",
      "epoch  345 train loss 0.028412 valid loss 0.022480\n",
      "epoch  346 train loss 0.019440 valid loss 0.023566\n",
      "epoch  347 train loss 0.029570 valid loss 0.017964\n",
      "epoch  348 train loss 0.023450 valid loss 0.022798\n",
      "epoch  349 train loss 0.020090 valid loss 0.016448\n",
      "epoch  350 train loss 0.020995 valid loss 0.022971\n",
      "epoch  351 train loss 0.028747 valid loss 0.043689\n",
      "epoch  352 train loss 0.030909 valid loss 0.048644\n",
      "epoch  353 train loss 0.029651 valid loss 0.022012\n",
      "epoch  354 train loss 0.023352 valid loss 0.029245\n",
      "epoch  355 train loss 0.026371 valid loss 0.017194\n",
      "epoch  356 train loss 0.021871 valid loss 0.029145\n",
      "epoch  357 train loss 0.023085 valid loss 0.020099\n",
      "epoch  358 train loss 0.022709 valid loss 0.022813\n",
      "epoch  359 train loss 0.022336 valid loss 0.025665\n",
      "epoch  360 train loss 0.023162 valid loss 0.020932\n",
      "epoch  361 train loss 0.019804 valid loss 0.015835\n",
      "epoch  362 train loss 0.027103 valid loss 0.021025\n",
      "epoch  363 train loss 0.022540 valid loss 0.043356\n",
      "epoch  364 train loss 0.031226 valid loss 0.023357\n",
      "epoch  365 train loss 0.022060 valid loss 0.020226\n",
      "epoch  366 train loss 0.020398 valid loss 0.024282\n",
      "epoch  367 train loss 0.027074 valid loss 0.023573\n",
      "epoch  368 train loss 0.026083 valid loss 0.037895\n",
      "epoch  369 train loss 0.027556 valid loss 0.028925\n",
      "epoch  370 train loss 0.033340 valid loss 0.029000\n",
      "epoch  371 train loss 0.035221 valid loss 0.021618\n",
      "epoch  372 train loss 0.020189 valid loss 0.022754\n",
      "epoch  373 train loss 0.025446 valid loss 0.023661\n",
      "epoch  374 train loss 0.024830 valid loss 0.017260\n",
      "epoch  375 train loss 0.026141 valid loss 0.024652\n",
      "epoch  376 train loss 0.025796 valid loss 0.020743\n",
      "epoch  377 train loss 0.022138 valid loss 0.017413\n",
      "epoch  378 train loss 0.019473 valid loss 0.024420\n",
      "epoch  379 train loss 0.025013 valid loss 0.015927\n",
      "epoch  380 train loss 0.025026 valid loss 0.050310\n",
      "epoch  381 train loss 0.060607 valid loss 0.030513\n",
      "epoch  382 train loss 0.042734 valid loss 0.088130\n",
      "epoch  383 train loss 0.072335 valid loss 0.050083\n",
      "epoch  384 train loss 0.059438 valid loss 0.078871\n",
      "epoch  385 train loss 0.051611 valid loss 0.090167\n",
      "epoch  386 train loss 0.075746 valid loss 0.067188\n",
      "epoch  387 train loss 0.050986 valid loss 0.030646\n",
      "epoch  388 train loss 0.034695 valid loss 0.044785\n",
      "epoch  389 train loss 0.036581 valid loss 0.025010\n",
      "epoch  390 train loss 0.024229 valid loss 0.025300\n",
      "epoch  391 train loss 0.025448 valid loss 0.029545\n",
      "epoch  392 train loss 0.026512 valid loss 0.029632\n",
      "epoch  393 train loss 0.027728 valid loss 0.033450\n",
      "epoch  394 train loss 0.032978 valid loss 0.028245\n",
      "epoch  395 train loss 0.024443 valid loss 0.033449\n",
      "epoch  396 train loss 0.030669 valid loss 0.034836\n",
      "epoch  397 train loss 0.035342 valid loss 0.031484\n",
      "epoch  398 train loss 0.031027 valid loss 0.020763\n",
      "epoch  399 train loss 0.030222 valid loss 0.036443\n",
      "epoch  400 train loss 0.034622 valid loss 0.035043\n",
      "epoch  401 train loss 0.037959 valid loss 0.039806\n",
      "epoch  402 train loss 0.069839 valid loss 0.049584\n",
      "epoch  403 train loss 0.041040 valid loss 0.024401\n",
      "epoch  404 train loss 0.023103 valid loss 0.023305\n",
      "epoch  405 train loss 0.035325 valid loss 0.034815\n",
      "epoch  406 train loss 0.028534 valid loss 0.020019\n",
      "epoch  407 train loss 0.030079 valid loss 0.029924\n",
      "epoch  408 train loss 0.026664 valid loss 0.030607\n",
      "epoch  409 train loss 0.026537 valid loss 0.021076\n",
      "epoch  410 train loss 0.021345 valid loss 0.019570\n",
      "epoch  411 train loss 0.020854 valid loss 0.022436\n",
      "epoch  412 train loss 0.021665 valid loss 0.021039\n",
      "epoch  413 train loss 0.019652 valid loss 0.029671\n",
      "epoch  414 train loss 0.026021 valid loss 0.027343\n",
      "epoch  415 train loss 0.025312 valid loss 0.022399\n",
      "epoch  416 train loss 0.031834 valid loss 0.027454\n",
      "epoch  417 train loss 0.025365 valid loss 0.026945\n",
      "epoch  418 train loss 0.026026 valid loss 0.025124\n",
      "epoch  419 train loss 0.023661 valid loss 0.019653\n",
      "epoch  420 train loss 0.023435 valid loss 0.028124\n",
      "epoch  421 train loss 0.027520 valid loss 0.020989\n",
      "epoch  422 train loss 0.018866 valid loss 0.018006\n",
      "epoch  423 train loss 0.019264 valid loss 0.016859\n",
      "epoch  424 train loss 0.019416 valid loss 0.023910\n",
      "epoch  425 train loss 0.020739 valid loss 0.036152\n",
      "epoch  426 train loss 0.027953 valid loss 0.023262\n",
      "epoch  427 train loss 0.024181 valid loss 0.036610\n",
      "epoch  428 train loss 0.023079 valid loss 0.023905\n",
      "epoch  429 train loss 0.021279 valid loss 0.023667\n",
      "epoch  430 train loss 0.019632 valid loss 0.022253\n",
      "epoch  431 train loss 0.022436 valid loss 0.042727\n",
      "epoch  432 train loss 0.036436 valid loss 0.019344\n",
      "epoch  433 train loss 0.019515 valid loss 0.027812\n",
      "epoch  434 train loss 0.021497 valid loss 0.018932\n",
      "epoch  435 train loss 0.022705 valid loss 0.019238\n",
      "epoch  436 train loss 0.020159 valid loss 0.015683\n",
      "epoch  437 train loss 0.018433 valid loss 0.021410\n",
      "epoch  438 train loss 0.020783 valid loss 0.027342\n",
      "epoch  439 train loss 0.022234 valid loss 0.036615\n",
      "epoch  440 train loss 0.024887 valid loss 0.020051\n",
      "epoch  441 train loss 0.017412 valid loss 0.024034\n",
      "epoch  442 train loss 0.020362 valid loss 0.017210\n",
      "epoch  443 train loss 0.021592 valid loss 0.028873\n",
      "epoch  444 train loss 0.027508 valid loss 0.020340\n",
      "epoch  445 train loss 0.021392 valid loss 0.018200\n",
      "epoch  446 train loss 0.022611 valid loss 0.021869\n",
      "epoch  447 train loss 0.019497 valid loss 0.018844\n",
      "epoch  448 train loss 0.020412 valid loss 0.023001\n",
      "epoch  449 train loss 0.023681 valid loss 0.027710\n",
      "epoch  450 train loss 0.020381 valid loss 0.021668\n",
      "epoch  451 train loss 0.016882 valid loss 0.026356\n",
      "epoch  452 train loss 0.027634 valid loss 0.023219\n",
      "epoch  453 train loss 0.029756 valid loss 0.023653\n",
      "epoch  454 train loss 0.021774 valid loss 0.045976\n",
      "epoch  455 train loss 0.024925 valid loss 0.026689\n",
      "epoch  456 train loss 0.025449 valid loss 0.021968\n",
      "epoch  457 train loss 0.022337 valid loss 0.016019\n",
      "epoch  458 train loss 0.020671 valid loss 0.022816\n",
      "epoch  459 train loss 0.022921 valid loss 0.022338\n",
      "epoch  460 train loss 0.027168 valid loss 0.013916\n",
      "epoch  461 train loss 0.016639 valid loss 0.011928\n",
      "epoch  462 train loss 0.015608 valid loss 0.017758\n",
      "epoch  463 train loss 0.028525 valid loss 0.029601\n",
      "epoch  464 train loss 0.022265 valid loss 0.034494\n",
      "epoch  465 train loss 0.024540 valid loss 0.014536\n",
      "epoch  466 train loss 0.020082 valid loss 0.022792\n",
      "epoch  467 train loss 0.021604 valid loss 0.025999\n",
      "epoch  468 train loss 0.023620 valid loss 0.017005\n",
      "epoch  469 train loss 0.017488 valid loss 0.017794\n",
      "epoch  470 train loss 0.017466 valid loss 0.019387\n",
      "epoch  471 train loss 0.023202 valid loss 0.015690\n",
      "epoch  472 train loss 0.017044 valid loss 0.030342\n",
      "epoch  473 train loss 0.022401 valid loss 0.016751\n",
      "epoch  474 train loss 0.017425 valid loss 0.021796\n",
      "epoch  475 train loss 0.017851 valid loss 0.020094\n",
      "epoch  476 train loss 0.022898 valid loss 0.033922\n",
      "epoch  477 train loss 0.021935 valid loss 0.015213\n",
      "epoch  478 train loss 0.014606 valid loss 0.012704\n",
      "epoch  479 train loss 0.020279 valid loss 0.013377\n",
      "epoch  480 train loss 0.013428 valid loss 0.045961\n",
      "epoch  481 train loss 0.026917 valid loss 0.019807\n",
      "epoch  482 train loss 0.016498 valid loss 0.037212\n",
      "epoch  483 train loss 0.024669 valid loss 0.022138\n",
      "epoch  484 train loss 0.028603 valid loss 0.017803\n",
      "epoch  485 train loss 0.020883 valid loss 0.023268\n",
      "epoch  486 train loss 0.023775 valid loss 0.021493\n",
      "epoch  487 train loss 0.022534 valid loss 0.016510\n",
      "epoch  488 train loss 0.018127 valid loss 0.018916\n",
      "epoch  489 train loss 0.014217 valid loss 0.015884\n",
      "epoch  490 train loss 0.014759 valid loss 0.019708\n",
      "epoch  491 train loss 0.019008 valid loss 0.019166\n",
      "epoch  492 train loss 0.019711 valid loss 0.015163\n",
      "epoch  493 train loss 0.016868 valid loss 0.017705\n",
      "epoch  494 train loss 0.017250 valid loss 0.012793\n",
      "epoch  495 train loss 0.012390 valid loss 0.028559\n",
      "epoch  496 train loss 0.021916 valid loss 0.016858\n",
      "epoch  497 train loss 0.020340 valid loss 0.024003\n",
      "epoch  498 train loss 0.020252 valid loss 0.014698\n",
      "epoch  499 train loss 0.017944 valid loss 0.019826\n",
      "epoch  500 train loss 0.015961 valid loss 0.024075\n",
      "epoch  501 train loss 0.020064 valid loss 0.023394\n",
      "epoch  502 train loss 0.023385 valid loss 0.014022\n",
      "epoch  503 train loss 0.017228 valid loss 0.024453\n",
      "epoch  504 train loss 0.016742 valid loss 0.017963\n",
      "epoch  505 train loss 0.016248 valid loss 0.015740\n",
      "epoch  506 train loss 0.015170 valid loss 0.013962\n",
      "epoch  507 train loss 0.013764 valid loss 0.011960\n",
      "epoch  508 train loss 0.013108 valid loss 0.012607\n",
      "epoch  509 train loss 0.012522 valid loss 0.023054\n",
      "epoch  510 train loss 0.015797 valid loss 0.013698\n",
      "epoch  511 train loss 0.013612 valid loss 0.014664\n",
      "epoch  512 train loss 0.014158 valid loss 0.013001\n",
      "epoch  513 train loss 0.013297 valid loss 0.025886\n",
      "epoch  514 train loss 0.019994 valid loss 0.025694\n",
      "epoch  515 train loss 0.017907 valid loss 0.013404\n",
      "epoch  516 train loss 0.014459 valid loss 0.016381\n",
      "epoch  517 train loss 0.014302 valid loss 0.012611\n",
      "epoch  518 train loss 0.018113 valid loss 0.017876\n",
      "epoch  519 train loss 0.015763 valid loss 0.016095\n",
      "epoch  520 train loss 0.014268 valid loss 0.015432\n",
      "epoch  521 train loss 0.015231 valid loss 0.016322\n",
      "epoch  522 train loss 0.018384 valid loss 0.013588\n",
      "epoch  523 train loss 0.013482 valid loss 0.013612\n",
      "epoch  524 train loss 0.013265 valid loss 0.016317\n",
      "epoch  525 train loss 0.015806 valid loss 0.017973\n",
      "epoch  526 train loss 0.019750 valid loss 0.017238\n",
      "epoch  527 train loss 0.017311 valid loss 0.015664\n",
      "epoch  528 train loss 0.014661 valid loss 0.014598\n",
      "epoch  529 train loss 0.012504 valid loss 0.018420\n",
      "epoch  530 train loss 0.015577 valid loss 0.012506\n",
      "epoch  531 train loss 0.012551 valid loss 0.016867\n",
      "epoch  532 train loss 0.017646 valid loss 0.016932\n",
      "epoch  533 train loss 0.013056 valid loss 0.022385\n",
      "epoch  534 train loss 0.016402 valid loss 0.013258\n",
      "epoch  535 train loss 0.013766 valid loss 0.012286\n",
      "epoch  536 train loss 0.013964 valid loss 0.011613\n",
      "epoch  537 train loss 0.011224 valid loss 0.016865\n",
      "epoch  538 train loss 0.019481 valid loss 0.023527\n",
      "epoch  539 train loss 0.018413 valid loss 0.019063\n",
      "epoch  540 train loss 0.018015 valid loss 0.016642\n",
      "epoch  541 train loss 0.015415 valid loss 0.013301\n",
      "epoch  542 train loss 0.012635 valid loss 0.018386\n",
      "epoch  543 train loss 0.015298 valid loss 0.011491\n",
      "epoch  544 train loss 0.014474 valid loss 0.017596\n",
      "epoch  545 train loss 0.015458 valid loss 0.015014\n",
      "epoch  546 train loss 0.014597 valid loss 0.014750\n",
      "epoch  547 train loss 0.012128 valid loss 0.015680\n",
      "epoch  548 train loss 0.017042 valid loss 0.021022\n",
      "epoch  549 train loss 0.018602 valid loss 0.016570\n",
      "epoch  550 train loss 0.014730 valid loss 0.015161\n",
      "epoch  551 train loss 0.017382 valid loss 0.014515\n",
      "epoch  552 train loss 0.013995 valid loss 0.014015\n",
      "epoch  553 train loss 0.013914 valid loss 0.028830\n",
      "epoch  554 train loss 0.019234 valid loss 0.009667\n",
      "epoch  555 train loss 0.013132 valid loss 0.016887\n",
      "epoch  556 train loss 0.021680 valid loss 0.018047\n",
      "epoch  557 train loss 0.017224 valid loss 0.015135\n",
      "epoch  558 train loss 0.013035 valid loss 0.013269\n",
      "epoch  559 train loss 0.014153 valid loss 0.013030\n",
      "epoch  560 train loss 0.014092 valid loss 0.018442\n",
      "epoch  561 train loss 0.015019 valid loss 0.012493\n",
      "epoch  562 train loss 0.014744 valid loss 0.013011\n",
      "epoch  563 train loss 0.012559 valid loss 0.011131\n",
      "epoch  564 train loss 0.012584 valid loss 0.017974\n",
      "epoch  565 train loss 0.016376 valid loss 0.026640\n",
      "epoch  566 train loss 0.019288 valid loss 0.018078\n",
      "epoch  567 train loss 0.016410 valid loss 0.014776\n",
      "epoch  568 train loss 0.013359 valid loss 0.015231\n",
      "epoch  569 train loss 0.014289 valid loss 0.017352\n",
      "epoch  570 train loss 0.013346 valid loss 0.013536\n",
      "epoch  571 train loss 0.014943 valid loss 0.012008\n",
      "epoch  572 train loss 0.012698 valid loss 0.013425\n",
      "epoch  573 train loss 0.012506 valid loss 0.011014\n",
      "epoch  574 train loss 0.012882 valid loss 0.012219\n",
      "epoch  575 train loss 0.011839 valid loss 0.012068\n",
      "epoch  576 train loss 0.012683 valid loss 0.011466\n",
      "epoch  577 train loss 0.014488 valid loss 0.014261\n",
      "epoch  578 train loss 0.014528 valid loss 0.012571\n",
      "epoch  579 train loss 0.010788 valid loss 0.009679\n",
      "epoch  580 train loss 0.011322 valid loss 0.012929\n",
      "epoch  581 train loss 0.011964 valid loss 0.016089\n",
      "epoch  582 train loss 0.013347 valid loss 0.012539\n",
      "epoch  583 train loss 0.013137 valid loss 0.014536\n",
      "epoch  584 train loss 0.014271 valid loss 0.016737\n",
      "epoch  585 train loss 0.012430 valid loss 0.010651\n",
      "epoch  586 train loss 0.011976 valid loss 0.020525\n",
      "epoch  587 train loss 0.016870 valid loss 0.014812\n",
      "epoch  588 train loss 0.013496 valid loss 0.014505\n",
      "epoch  589 train loss 0.013876 valid loss 0.011458\n",
      "epoch  590 train loss 0.014810 valid loss 0.011535\n",
      "epoch  591 train loss 0.012214 valid loss 0.011261\n",
      "epoch  592 train loss 0.011315 valid loss 0.012525\n",
      "epoch  593 train loss 0.015371 valid loss 0.014113\n",
      "epoch  594 train loss 0.013386 valid loss 0.017636\n",
      "epoch  595 train loss 0.014864 valid loss 0.011432\n",
      "epoch  596 train loss 0.010164 valid loss 0.012474\n",
      "epoch  597 train loss 0.014434 valid loss 0.012468\n",
      "epoch  598 train loss 0.016275 valid loss 0.012037\n",
      "epoch  599 train loss 0.014255 valid loss 0.013095\n",
      "epoch  600 train loss 0.011456 valid loss 0.013795\n",
      "epoch  601 train loss 0.012343 valid loss 0.010257\n",
      "epoch  602 train loss 0.009891 valid loss 0.008897\n",
      "epoch  603 train loss 0.012907 valid loss 0.018054\n",
      "epoch  604 train loss 0.013550 valid loss 0.013599\n",
      "epoch  605 train loss 0.011759 valid loss 0.010437\n",
      "epoch  606 train loss 0.014708 valid loss 0.010049\n",
      "epoch  607 train loss 0.011237 valid loss 0.009990\n",
      "epoch  608 train loss 0.009577 valid loss 0.010418\n",
      "epoch  609 train loss 0.012516 valid loss 0.015601\n",
      "epoch  610 train loss 0.013475 valid loss 0.009873\n",
      "epoch  611 train loss 0.011872 valid loss 0.012223\n",
      "epoch  612 train loss 0.015330 valid loss 0.013983\n",
      "epoch  613 train loss 0.013540 valid loss 0.013696\n",
      "epoch  614 train loss 0.013220 valid loss 0.010053\n",
      "epoch  615 train loss 0.012494 valid loss 0.012991\n",
      "epoch  616 train loss 0.011776 valid loss 0.012019\n",
      "epoch  617 train loss 0.010904 valid loss 0.022779\n",
      "epoch  618 train loss 0.017274 valid loss 0.009992\n",
      "epoch  619 train loss 0.009848 valid loss 0.018666\n",
      "epoch  620 train loss 0.014827 valid loss 0.009657\n",
      "epoch  621 train loss 0.008701 valid loss 0.008507\n",
      "epoch  622 train loss 0.009763 valid loss 0.016972\n",
      "epoch  623 train loss 0.012286 valid loss 0.016988\n",
      "epoch  624 train loss 0.013048 valid loss 0.011722\n",
      "epoch  625 train loss 0.010463 valid loss 0.012006\n",
      "epoch  626 train loss 0.015473 valid loss 0.014184\n",
      "epoch  627 train loss 0.017535 valid loss 0.014143\n",
      "epoch  628 train loss 0.011948 valid loss 0.010942\n",
      "epoch  629 train loss 0.011822 valid loss 0.009539\n",
      "epoch  630 train loss 0.010167 valid loss 0.009975\n",
      "epoch  631 train loss 0.010477 valid loss 0.016419\n",
      "epoch  632 train loss 0.012920 valid loss 0.016189\n",
      "epoch  633 train loss 0.011414 valid loss 0.009259\n",
      "epoch  634 train loss 0.009627 valid loss 0.007857\n",
      "epoch  635 train loss 0.009025 valid loss 0.010195\n",
      "epoch  636 train loss 0.012907 valid loss 0.013482\n",
      "epoch  637 train loss 0.010628 valid loss 0.009938\n",
      "epoch  638 train loss 0.013297 valid loss 0.012579\n",
      "epoch  639 train loss 0.009735 valid loss 0.013773\n",
      "epoch  640 train loss 0.012632 valid loss 0.015249\n",
      "epoch  641 train loss 0.011646 valid loss 0.015914\n",
      "epoch  642 train loss 0.013349 valid loss 0.014210\n",
      "epoch  643 train loss 0.011009 valid loss 0.015648\n",
      "epoch  644 train loss 0.011138 valid loss 0.011283\n",
      "epoch  645 train loss 0.011513 valid loss 0.010234\n",
      "epoch  646 train loss 0.011441 valid loss 0.009069\n",
      "epoch  647 train loss 0.011206 valid loss 0.009340\n",
      "epoch  648 train loss 0.011043 valid loss 0.016455\n",
      "epoch  649 train loss 0.012581 valid loss 0.009042\n",
      "epoch  650 train loss 0.010421 valid loss 0.015712\n",
      "epoch  651 train loss 0.012556 valid loss 0.013226\n",
      "epoch  652 train loss 0.012861 valid loss 0.012325\n",
      "epoch  653 train loss 0.009946 valid loss 0.010277\n",
      "epoch  654 train loss 0.010326 valid loss 0.010175\n",
      "epoch  655 train loss 0.010250 valid loss 0.010804\n",
      "epoch  656 train loss 0.009919 valid loss 0.008855\n",
      "epoch  657 train loss 0.011583 valid loss 0.013288\n",
      "epoch  658 train loss 0.013660 valid loss 0.013245\n",
      "epoch  659 train loss 0.011068 valid loss 0.013196\n",
      "epoch  660 train loss 0.011145 valid loss 0.011032\n",
      "epoch  661 train loss 0.013972 valid loss 0.011511\n",
      "epoch  662 train loss 0.011643 valid loss 0.013865\n",
      "epoch  663 train loss 0.013599 valid loss 0.009974\n",
      "epoch  664 train loss 0.011008 valid loss 0.009424\n",
      "epoch  665 train loss 0.010476 valid loss 0.010279\n",
      "epoch  666 train loss 0.011307 valid loss 0.013424\n",
      "epoch  667 train loss 0.011432 valid loss 0.012732\n",
      "epoch  668 train loss 0.015019 valid loss 0.014562\n",
      "epoch  669 train loss 0.013082 valid loss 0.011895\n",
      "epoch  670 train loss 0.015847 valid loss 0.013068\n",
      "epoch  671 train loss 0.012369 valid loss 0.009870\n",
      "epoch  672 train loss 0.012307 valid loss 0.010652\n",
      "epoch  673 train loss 0.011412 valid loss 0.011703\n",
      "epoch  674 train loss 0.012612 valid loss 0.011152\n",
      "epoch  675 train loss 0.014458 valid loss 0.015263\n",
      "epoch  676 train loss 0.011781 valid loss 0.011684\n",
      "epoch  677 train loss 0.010345 valid loss 0.012210\n",
      "epoch  678 train loss 0.010898 valid loss 0.014742\n",
      "epoch  679 train loss 0.015163 valid loss 0.010138\n",
      "epoch  680 train loss 0.010525 valid loss 0.011104\n",
      "epoch  681 train loss 0.010261 valid loss 0.017575\n",
      "epoch  682 train loss 0.013377 valid loss 0.015681\n",
      "epoch  683 train loss 0.011057 valid loss 0.010660\n",
      "epoch  684 train loss 0.014397 valid loss 0.013132\n",
      "epoch  685 train loss 0.012390 valid loss 0.011015\n",
      "epoch  686 train loss 0.012438 valid loss 0.008815\n",
      "epoch  687 train loss 0.010268 valid loss 0.012961\n",
      "epoch  688 train loss 0.011925 valid loss 0.014341\n",
      "epoch  689 train loss 0.012383 valid loss 0.011528\n",
      "epoch  690 train loss 0.012279 valid loss 0.009016\n",
      "epoch  691 train loss 0.011070 valid loss 0.009998\n",
      "epoch  692 train loss 0.010746 valid loss 0.011694\n",
      "epoch  693 train loss 0.009710 valid loss 0.010528\n",
      "epoch  694 train loss 0.011004 valid loss 0.016222\n",
      "epoch  695 train loss 0.011515 valid loss 0.010130\n",
      "epoch  696 train loss 0.010865 valid loss 0.009141\n",
      "epoch  697 train loss 0.014203 valid loss 0.011632\n",
      "epoch  698 train loss 0.012764 valid loss 0.011725\n",
      "epoch  699 train loss 0.011212 valid loss 0.010262\n",
      "epoch  700 train loss 0.010045 valid loss 0.010932\n",
      "epoch  701 train loss 0.011854 valid loss 0.009153\n",
      "epoch  702 train loss 0.009976 valid loss 0.013236\n",
      "epoch  703 train loss 0.011940 valid loss 0.009144\n",
      "epoch  704 train loss 0.013092 valid loss 0.021669\n",
      "epoch  705 train loss 0.016258 valid loss 0.017296\n",
      "epoch  706 train loss 0.013433 valid loss 0.012128\n",
      "epoch  707 train loss 0.011515 valid loss 0.009798\n",
      "epoch  708 train loss 0.010408 valid loss 0.010099\n",
      "epoch  709 train loss 0.010291 valid loss 0.008639\n",
      "epoch  710 train loss 0.009449 valid loss 0.014877\n",
      "epoch  711 train loss 0.013414 valid loss 0.010531\n",
      "epoch  712 train loss 0.010346 valid loss 0.011508\n",
      "epoch  713 train loss 0.013495 valid loss 0.016498\n",
      "epoch  714 train loss 0.014126 valid loss 0.011366\n",
      "epoch  715 train loss 0.010855 valid loss 0.016299\n",
      "epoch  716 train loss 0.012377 valid loss 0.013853\n",
      "epoch  717 train loss 0.016459 valid loss 0.012621\n",
      "epoch  718 train loss 0.011735 valid loss 0.012689\n",
      "epoch  719 train loss 0.013774 valid loss 0.019573\n",
      "epoch  720 train loss 0.013956 valid loss 0.011812\n",
      "epoch  721 train loss 0.011373 valid loss 0.016768\n",
      "epoch  722 train loss 0.013700 valid loss 0.011232\n",
      "epoch  723 train loss 0.011634 valid loss 0.011440\n",
      "epoch  724 train loss 0.011537 valid loss 0.011695\n",
      "epoch  725 train loss 0.011167 valid loss 0.009044\n",
      "epoch  726 train loss 0.011246 valid loss 0.009711\n",
      "epoch  727 train loss 0.011366 valid loss 0.016481\n",
      "epoch  728 train loss 0.014072 valid loss 0.012985\n",
      "epoch  729 train loss 0.013378 valid loss 0.010744\n",
      "epoch  730 train loss 0.010051 valid loss 0.016494\n",
      "epoch  731 train loss 0.014506 valid loss 0.010639\n",
      "epoch  732 train loss 0.011100 valid loss 0.013112\n",
      "epoch  733 train loss 0.012991 valid loss 0.011893\n",
      "epoch  734 train loss 0.012190 valid loss 0.011439\n",
      "epoch  735 train loss 0.011832 valid loss 0.010681\n",
      "epoch  736 train loss 0.009752 valid loss 0.016716\n",
      "epoch  737 train loss 0.012373 valid loss 0.009426\n",
      "epoch  738 train loss 0.009839 valid loss 0.011213\n",
      "epoch  739 train loss 0.012754 valid loss 0.010142\n",
      "epoch  740 train loss 0.011078 valid loss 0.010389\n",
      "epoch  741 train loss 0.010752 valid loss 0.014247\n",
      "epoch  742 train loss 0.013998 valid loss 0.013200\n",
      "epoch  743 train loss 0.011506 valid loss 0.014563\n",
      "epoch  744 train loss 0.011208 valid loss 0.010557\n",
      "epoch  745 train loss 0.009785 valid loss 0.010108\n",
      "epoch  746 train loss 0.009671 valid loss 0.015331\n",
      "epoch  747 train loss 0.012930 valid loss 0.010366\n",
      "epoch  748 train loss 0.008986 valid loss 0.009722\n",
      "epoch  749 train loss 0.012143 valid loss 0.015305\n",
      "epoch  750 train loss 0.013848 valid loss 0.010826\n",
      "epoch  751 train loss 0.009356 valid loss 0.012620\n",
      "epoch  752 train loss 0.011207 valid loss 0.018075\n",
      "epoch  753 train loss 0.015628 valid loss 0.010516\n",
      "epoch  754 train loss 0.013100 valid loss 0.016383\n",
      "epoch  755 train loss 0.014104 valid loss 0.024209\n",
      "epoch  756 train loss 0.015050 valid loss 0.009500\n",
      "epoch  757 train loss 0.009071 valid loss 0.008046\n",
      "epoch  758 train loss 0.010996 valid loss 0.008158\n",
      "epoch  759 train loss 0.008657 valid loss 0.012361\n",
      "epoch  760 train loss 0.009187 valid loss 0.008231\n",
      "epoch  761 train loss 0.008444 valid loss 0.016464\n",
      "epoch  762 train loss 0.012290 valid loss 0.008715\n",
      "epoch  763 train loss 0.008729 valid loss 0.018060\n",
      "epoch  764 train loss 0.013317 valid loss 0.011978\n",
      "epoch  765 train loss 0.009289 valid loss 0.008230\n",
      "epoch  766 train loss 0.010634 valid loss 0.011281\n",
      "epoch  767 train loss 0.011620 valid loss 0.011351\n",
      "epoch  768 train loss 0.010875 valid loss 0.009992\n",
      "epoch  769 train loss 0.013175 valid loss 0.009279\n",
      "epoch  770 train loss 0.014269 valid loss 0.011119\n",
      "epoch  771 train loss 0.008804 valid loss 0.008836\n",
      "epoch  772 train loss 0.009673 valid loss 0.009513\n",
      "epoch  773 train loss 0.009381 valid loss 0.012824\n",
      "epoch  774 train loss 0.011055 valid loss 0.011075\n",
      "epoch  775 train loss 0.012177 valid loss 0.011228\n",
      "epoch  776 train loss 0.009930 valid loss 0.011757\n",
      "epoch  777 train loss 0.010958 valid loss 0.019092\n",
      "epoch  778 train loss 0.012841 valid loss 0.009546\n",
      "epoch  779 train loss 0.009202 valid loss 0.007477\n",
      "epoch  780 train loss 0.008189 valid loss 0.011069\n",
      "epoch  781 train loss 0.010719 valid loss 0.013671\n",
      "epoch  782 train loss 0.010580 valid loss 0.010332\n",
      "epoch  783 train loss 0.009660 valid loss 0.011348\n",
      "epoch  784 train loss 0.009493 valid loss 0.008142\n",
      "epoch  785 train loss 0.008066 valid loss 0.008282\n",
      "epoch  786 train loss 0.008389 valid loss 0.008588\n",
      "epoch  787 train loss 0.009249 valid loss 0.007923\n",
      "epoch  788 train loss 0.009025 valid loss 0.008369\n",
      "epoch  789 train loss 0.008104 valid loss 0.008436\n",
      "epoch  790 train loss 0.007852 valid loss 0.007917\n",
      "epoch  791 train loss 0.008354 valid loss 0.009733\n",
      "epoch  792 train loss 0.008621 valid loss 0.008421\n",
      "epoch  793 train loss 0.008111 valid loss 0.010465\n",
      "epoch  794 train loss 0.009567 valid loss 0.008578\n",
      "epoch  795 train loss 0.008061 valid loss 0.014824\n",
      "epoch  796 train loss 0.011603 valid loss 0.007766\n",
      "epoch  797 train loss 0.007337 valid loss 0.009392\n",
      "epoch  798 train loss 0.008568 valid loss 0.015048\n",
      "epoch  799 train loss 0.014133 valid loss 0.009355\n",
      "epoch  800 train loss 0.008946 valid loss 0.006653\n",
      "epoch  801 train loss 0.008696 valid loss 0.007515\n",
      "epoch  802 train loss 0.007534 valid loss 0.007862\n",
      "epoch  803 train loss 0.007948 valid loss 0.007233\n",
      "epoch  804 train loss 0.008687 valid loss 0.008297\n",
      "epoch  805 train loss 0.008704 valid loss 0.010212\n",
      "epoch  806 train loss 0.009165 valid loss 0.010398\n",
      "epoch  807 train loss 0.009840 valid loss 0.009875\n",
      "epoch  808 train loss 0.009427 valid loss 0.013370\n",
      "epoch  809 train loss 0.010096 valid loss 0.009172\n",
      "epoch  810 train loss 0.009036 valid loss 0.008158\n",
      "epoch  811 train loss 0.009966 valid loss 0.009943\n",
      "epoch  812 train loss 0.010169 valid loss 0.009915\n",
      "epoch  813 train loss 0.009684 valid loss 0.013094\n",
      "epoch  814 train loss 0.010045 valid loss 0.007681\n",
      "epoch  815 train loss 0.008524 valid loss 0.023660\n",
      "epoch  816 train loss 0.015204 valid loss 0.007200\n",
      "epoch  817 train loss 0.008271 valid loss 0.008239\n",
      "epoch  818 train loss 0.009523 valid loss 0.008690\n",
      "epoch  819 train loss 0.008310 valid loss 0.007585\n",
      "epoch  820 train loss 0.007272 valid loss 0.007514\n",
      "epoch  821 train loss 0.008720 valid loss 0.009278\n",
      "epoch  822 train loss 0.011058 valid loss 0.008002\n",
      "epoch  823 train loss 0.009416 valid loss 0.008521\n",
      "epoch  824 train loss 0.009435 valid loss 0.014356\n",
      "epoch  825 train loss 0.011793 valid loss 0.010960\n",
      "epoch  826 train loss 0.014565 valid loss 0.011125\n",
      "epoch  827 train loss 0.013077 valid loss 0.012525\n",
      "epoch  828 train loss 0.011407 valid loss 0.013456\n",
      "epoch  829 train loss 0.010895 valid loss 0.009992\n",
      "epoch  830 train loss 0.010259 valid loss 0.016628\n",
      "epoch  831 train loss 0.016883 valid loss 0.009540\n",
      "epoch  832 train loss 0.010157 valid loss 0.008174\n",
      "epoch  833 train loss 0.008953 valid loss 0.008141\n",
      "epoch  834 train loss 0.007879 valid loss 0.008766\n",
      "epoch  835 train loss 0.009234 valid loss 0.009106\n",
      "epoch  836 train loss 0.008982 valid loss 0.009910\n",
      "epoch  837 train loss 0.010485 valid loss 0.007757\n",
      "epoch  838 train loss 0.009161 valid loss 0.008892\n",
      "epoch  839 train loss 0.007845 valid loss 0.008794\n",
      "epoch  840 train loss 0.008141 valid loss 0.007187\n",
      "epoch  841 train loss 0.008004 valid loss 0.010212\n",
      "epoch  842 train loss 0.009879 valid loss 0.009252\n",
      "epoch  843 train loss 0.009856 valid loss 0.011983\n",
      "epoch  844 train loss 0.011017 valid loss 0.009710\n",
      "epoch  845 train loss 0.009704 valid loss 0.008772\n",
      "epoch  846 train loss 0.009489 valid loss 0.008405\n",
      "epoch  847 train loss 0.007690 valid loss 0.009787\n",
      "epoch  848 train loss 0.009602 valid loss 0.010786\n",
      "epoch  849 train loss 0.012119 valid loss 0.010304\n",
      "epoch  850 train loss 0.010656 valid loss 0.010625\n",
      "epoch  851 train loss 0.010690 valid loss 0.010794\n",
      "epoch  852 train loss 0.009695 valid loss 0.011367\n",
      "epoch  853 train loss 0.009527 valid loss 0.011293\n",
      "epoch  854 train loss 0.009672 valid loss 0.008226\n",
      "epoch  855 train loss 0.010505 valid loss 0.011202\n",
      "epoch  856 train loss 0.008248 valid loss 0.010533\n",
      "epoch  857 train loss 0.007975 valid loss 0.007507\n",
      "epoch  858 train loss 0.007669 valid loss 0.011830\n",
      "epoch  859 train loss 0.009797 valid loss 0.010073\n",
      "epoch  860 train loss 0.010826 valid loss 0.008773\n",
      "epoch  861 train loss 0.007505 valid loss 0.008926\n",
      "epoch  862 train loss 0.007949 valid loss 0.007749\n",
      "epoch  863 train loss 0.007324 valid loss 0.014571\n",
      "epoch  864 train loss 0.010593 valid loss 0.010495\n",
      "epoch  865 train loss 0.008894 valid loss 0.010547\n",
      "epoch  866 train loss 0.010530 valid loss 0.010381\n",
      "epoch  867 train loss 0.008382 valid loss 0.008854\n",
      "epoch  868 train loss 0.009858 valid loss 0.008454\n",
      "epoch  869 train loss 0.008304 valid loss 0.010564\n",
      "epoch  870 train loss 0.008493 valid loss 0.008437\n",
      "epoch  871 train loss 0.008438 valid loss 0.013121\n",
      "epoch  872 train loss 0.010982 valid loss 0.007017\n",
      "epoch  873 train loss 0.008287 valid loss 0.008555\n",
      "epoch  874 train loss 0.008097 valid loss 0.010111\n",
      "epoch  875 train loss 0.009781 valid loss 0.011454\n",
      "epoch  876 train loss 0.009857 valid loss 0.011846\n",
      "epoch  877 train loss 0.008904 valid loss 0.009479\n",
      "epoch  878 train loss 0.008690 valid loss 0.008146\n",
      "epoch  879 train loss 0.008223 valid loss 0.007999\n",
      "epoch  880 train loss 0.008639 valid loss 0.009693\n",
      "epoch  881 train loss 0.009844 valid loss 0.009880\n",
      "epoch  882 train loss 0.010742 valid loss 0.008995\n",
      "epoch  883 train loss 0.008982 valid loss 0.010109\n",
      "epoch  884 train loss 0.008755 valid loss 0.013308\n",
      "epoch  885 train loss 0.010644 valid loss 0.010115\n",
      "epoch  886 train loss 0.011457 valid loss 0.010082\n",
      "epoch  887 train loss 0.010344 valid loss 0.009611\n",
      "epoch  888 train loss 0.008603 valid loss 0.008333\n",
      "epoch  889 train loss 0.008637 valid loss 0.010512\n",
      "epoch  890 train loss 0.011629 valid loss 0.008825\n",
      "epoch  891 train loss 0.008750 valid loss 0.010178\n",
      "epoch  892 train loss 0.009947 valid loss 0.007533\n",
      "epoch  893 train loss 0.007145 valid loss 0.007679\n",
      "epoch  894 train loss 0.007781 valid loss 0.010455\n",
      "epoch  895 train loss 0.010739 valid loss 0.011484\n",
      "epoch  896 train loss 0.009384 valid loss 0.009288\n",
      "epoch  897 train loss 0.008143 valid loss 0.008340\n",
      "epoch  898 train loss 0.008784 valid loss 0.009032\n",
      "epoch  899 train loss 0.010827 valid loss 0.009433\n",
      "epoch  900 train loss 0.008563 valid loss 0.009469\n",
      "epoch  901 train loss 0.008909 valid loss 0.009380\n",
      "epoch  902 train loss 0.008748 valid loss 0.007902\n",
      "epoch  903 train loss 0.007613 valid loss 0.009698\n",
      "epoch  904 train loss 0.009020 valid loss 0.009816\n",
      "epoch  905 train loss 0.008626 valid loss 0.007588\n",
      "epoch  906 train loss 0.007599 valid loss 0.009603\n",
      "epoch  907 train loss 0.008340 valid loss 0.008940\n",
      "epoch  908 train loss 0.008560 valid loss 0.007396\n",
      "epoch  909 train loss 0.008149 valid loss 0.007491\n",
      "epoch  910 train loss 0.009626 valid loss 0.008647\n",
      "epoch  911 train loss 0.008209 valid loss 0.010755\n",
      "epoch  912 train loss 0.008936 valid loss 0.008459\n",
      "epoch  913 train loss 0.009007 valid loss 0.010744\n",
      "epoch  914 train loss 0.009058 valid loss 0.007744\n",
      "epoch  915 train loss 0.006611 valid loss 0.008180\n",
      "epoch  916 train loss 0.008790 valid loss 0.009676\n",
      "epoch  917 train loss 0.010585 valid loss 0.007622\n",
      "epoch  918 train loss 0.008010 valid loss 0.010131\n",
      "epoch  919 train loss 0.008914 valid loss 0.009606\n",
      "epoch  920 train loss 0.008828 valid loss 0.013167\n",
      "epoch  921 train loss 0.009870 valid loss 0.009706\n",
      "epoch  922 train loss 0.009347 valid loss 0.008427\n",
      "epoch  923 train loss 0.007587 valid loss 0.007384\n",
      "epoch  924 train loss 0.007758 valid loss 0.010801\n",
      "epoch  925 train loss 0.009523 valid loss 0.009595\n",
      "epoch  926 train loss 0.009844 valid loss 0.009252\n",
      "epoch  927 train loss 0.008347 valid loss 0.007325\n",
      "epoch  928 train loss 0.008100 valid loss 0.009986\n",
      "epoch  929 train loss 0.010794 valid loss 0.007951\n",
      "epoch  930 train loss 0.009867 valid loss 0.009007\n",
      "epoch  931 train loss 0.009816 valid loss 0.010000\n",
      "epoch  932 train loss 0.008379 valid loss 0.008409\n",
      "epoch  933 train loss 0.008065 valid loss 0.008119\n",
      "epoch  934 train loss 0.009127 valid loss 0.009149\n",
      "epoch  935 train loss 0.008646 valid loss 0.007918\n",
      "epoch  936 train loss 0.008945 valid loss 0.007886\n",
      "epoch  937 train loss 0.007591 valid loss 0.009836\n",
      "epoch  938 train loss 0.008408 valid loss 0.008178\n",
      "epoch  939 train loss 0.008938 valid loss 0.008162\n",
      "epoch  940 train loss 0.008235 valid loss 0.008462\n",
      "epoch  941 train loss 0.008529 valid loss 0.007834\n",
      "epoch  942 train loss 0.008067 valid loss 0.008514\n",
      "epoch  943 train loss 0.008481 valid loss 0.009781\n",
      "epoch  944 train loss 0.008941 valid loss 0.009588\n",
      "epoch  945 train loss 0.008485 valid loss 0.008878\n",
      "epoch  946 train loss 0.008325 valid loss 0.008928\n",
      "epoch  947 train loss 0.007852 valid loss 0.010066\n",
      "epoch  948 train loss 0.008786 valid loss 0.011697\n",
      "epoch  949 train loss 0.009944 valid loss 0.009606\n",
      "epoch  950 train loss 0.008046 valid loss 0.009749\n",
      "epoch  951 train loss 0.009059 valid loss 0.008076\n",
      "epoch  952 train loss 0.008486 valid loss 0.008085\n",
      "epoch  953 train loss 0.007036 valid loss 0.011354\n",
      "epoch  954 train loss 0.008830 valid loss 0.006552\n",
      "epoch  955 train loss 0.007667 valid loss 0.008942\n",
      "epoch  956 train loss 0.009300 valid loss 0.008313\n",
      "epoch  957 train loss 0.007454 valid loss 0.007623\n",
      "epoch  958 train loss 0.008234 valid loss 0.007828\n",
      "epoch  959 train loss 0.007858 valid loss 0.007708\n",
      "epoch  960 train loss 0.007501 valid loss 0.008345\n",
      "epoch  961 train loss 0.007749 valid loss 0.008546\n",
      "epoch  962 train loss 0.007552 valid loss 0.006816\n",
      "epoch  963 train loss 0.008339 valid loss 0.007717\n",
      "epoch  964 train loss 0.007875 valid loss 0.008408\n",
      "epoch  965 train loss 0.008109 valid loss 0.007196\n",
      "epoch  966 train loss 0.007195 valid loss 0.007378\n",
      "epoch  967 train loss 0.008306 valid loss 0.008146\n",
      "epoch  968 train loss 0.008313 valid loss 0.008136\n",
      "epoch  969 train loss 0.009036 valid loss 0.009006\n",
      "epoch  970 train loss 0.007875 valid loss 0.007880\n",
      "epoch  971 train loss 0.008647 valid loss 0.007208\n",
      "epoch  972 train loss 0.008077 valid loss 0.013344\n",
      "epoch  973 train loss 0.009584 valid loss 0.007695\n",
      "epoch  974 train loss 0.007530 valid loss 0.007840\n",
      "epoch  975 train loss 0.007161 valid loss 0.007449\n",
      "epoch  976 train loss 0.007785 valid loss 0.010102\n",
      "epoch  977 train loss 0.008209 valid loss 0.007923\n",
      "epoch  978 train loss 0.008888 valid loss 0.007753\n",
      "epoch  979 train loss 0.007042 valid loss 0.009119\n",
      "epoch  980 train loss 0.008079 valid loss 0.008844\n",
      "epoch  981 train loss 0.009722 valid loss 0.008307\n",
      "epoch  982 train loss 0.008384 valid loss 0.007713\n",
      "epoch  983 train loss 0.008713 valid loss 0.007938\n",
      "epoch  984 train loss 0.007834 valid loss 0.007267\n",
      "epoch  985 train loss 0.007389 valid loss 0.007782\n",
      "epoch  986 train loss 0.007256 valid loss 0.008290\n",
      "epoch  987 train loss 0.008773 valid loss 0.009155\n",
      "epoch  988 train loss 0.008931 valid loss 0.008471\n",
      "epoch  989 train loss 0.007973 valid loss 0.007683\n",
      "epoch  990 train loss 0.009172 valid loss 0.008346\n",
      "epoch  991 train loss 0.007500 valid loss 0.010756\n",
      "epoch  992 train loss 0.009723 valid loss 0.007941\n",
      "epoch  993 train loss 0.007308 valid loss 0.009709\n",
      "epoch  994 train loss 0.008397 valid loss 0.008203\n",
      "epoch  995 train loss 0.008353 valid loss 0.008550\n",
      "epoch  996 train loss 0.009261 valid loss 0.008165\n",
      "epoch  997 train loss 0.008775 valid loss 0.007958\n",
      "epoch  998 train loss 0.007883 valid loss 0.009021\n",
      "epoch  999 train loss 0.008556 valid loss 0.010239\n",
      "epoch 1000 train loss 0.008183 valid loss 0.007796\n",
      "epoch 1001 train loss 0.007648 valid loss 0.010217\n",
      "epoch 1002 train loss 0.008381 valid loss 0.008142\n",
      "epoch 1003 train loss 0.007825 valid loss 0.007712\n",
      "epoch 1004 train loss 0.007251 valid loss 0.008759\n",
      "epoch 1005 train loss 0.008301 valid loss 0.008335\n",
      "epoch 1006 train loss 0.007585 valid loss 0.008046\n",
      "epoch 1007 train loss 0.007564 valid loss 0.007922\n",
      "epoch 1008 train loss 0.008295 valid loss 0.007834\n",
      "epoch 1009 train loss 0.007766 valid loss 0.011618\n",
      "epoch 1010 train loss 0.008543 valid loss 0.008452\n",
      "epoch 1011 train loss 0.007816 valid loss 0.006826\n",
      "epoch 1012 train loss 0.006861 valid loss 0.007446\n",
      "epoch 1013 train loss 0.007631 valid loss 0.007135\n",
      "epoch 1014 train loss 0.007525 valid loss 0.007804\n",
      "epoch 1015 train loss 0.007434 valid loss 0.007695\n",
      "epoch 1016 train loss 0.007947 valid loss 0.007643\n",
      "epoch 1017 train loss 0.007774 valid loss 0.008831\n",
      "epoch 1018 train loss 0.008646 valid loss 0.007653\n",
      "epoch 1019 train loss 0.007107 valid loss 0.006990\n",
      "epoch 1020 train loss 0.007870 valid loss 0.010650\n",
      "epoch 1021 train loss 0.009013 valid loss 0.009147\n",
      "epoch 1022 train loss 0.008321 valid loss 0.009406\n",
      "epoch 1023 train loss 0.010450 valid loss 0.007644\n",
      "epoch 1024 train loss 0.008372 valid loss 0.008905\n",
      "epoch 1025 train loss 0.007372 valid loss 0.007459\n",
      "epoch 1026 train loss 0.009407 valid loss 0.007261\n",
      "epoch 1027 train loss 0.008780 valid loss 0.008963\n",
      "epoch 1028 train loss 0.009133 valid loss 0.007508\n",
      "epoch 1029 train loss 0.007323 valid loss 0.007992\n",
      "epoch 1030 train loss 0.007771 valid loss 0.010313\n",
      "epoch 1031 train loss 0.008167 valid loss 0.007212\n",
      "epoch 1032 train loss 0.006946 valid loss 0.007727\n",
      "epoch 1033 train loss 0.008110 valid loss 0.010787\n",
      "epoch 1034 train loss 0.009154 valid loss 0.008436\n",
      "epoch 1035 train loss 0.007616 valid loss 0.007295\n",
      "epoch 1036 train loss 0.007229 valid loss 0.009618\n",
      "epoch 1037 train loss 0.008293 valid loss 0.006847\n",
      "epoch 1038 train loss 0.006700 valid loss 0.007931\n",
      "epoch 1039 train loss 0.007978 valid loss 0.008236\n",
      "epoch 1040 train loss 0.008131 valid loss 0.008623\n",
      "epoch 1041 train loss 0.006925 valid loss 0.006737\n",
      "epoch 1042 train loss 0.008292 valid loss 0.008261\n",
      "epoch 1043 train loss 0.008960 valid loss 0.006620\n",
      "epoch 1044 train loss 0.006605 valid loss 0.007695\n",
      "epoch 1045 train loss 0.007886 valid loss 0.006969\n",
      "epoch 1046 train loss 0.007186 valid loss 0.009463\n",
      "epoch 1047 train loss 0.008363 valid loss 0.008696\n",
      "epoch 1048 train loss 0.009188 valid loss 0.007913\n",
      "epoch 1049 train loss 0.007254 valid loss 0.008439\n",
      "epoch 1050 train loss 0.008487 valid loss 0.010387\n",
      "epoch 1051 train loss 0.007996 valid loss 0.008133\n",
      "epoch 1052 train loss 0.008069 valid loss 0.007227\n",
      "epoch 1053 train loss 0.006767 valid loss 0.006891\n",
      "epoch 1054 train loss 0.006869 valid loss 0.010385\n",
      "epoch 1055 train loss 0.008320 valid loss 0.007410\n",
      "epoch 1056 train loss 0.007826 valid loss 0.007518\n",
      "epoch 1057 train loss 0.007874 valid loss 0.006996\n",
      "epoch 1058 train loss 0.006481 valid loss 0.007636\n",
      "epoch 1059 train loss 0.007514 valid loss 0.009358\n",
      "epoch 1060 train loss 0.007704 valid loss 0.006855\n",
      "epoch 1061 train loss 0.006950 valid loss 0.008030\n",
      "epoch 1062 train loss 0.008020 valid loss 0.007638\n",
      "epoch 1063 train loss 0.009339 valid loss 0.009533\n",
      "epoch 1064 train loss 0.009305 valid loss 0.008930\n",
      "epoch 1065 train loss 0.007620 valid loss 0.008278\n",
      "epoch 1066 train loss 0.007781 valid loss 0.009576\n",
      "epoch 1067 train loss 0.008744 valid loss 0.010292\n",
      "epoch 1068 train loss 0.007899 valid loss 0.007653\n",
      "epoch 1069 train loss 0.007530 valid loss 0.006984\n",
      "epoch 1070 train loss 0.006657 valid loss 0.009478\n",
      "epoch 1071 train loss 0.008597 valid loss 0.007307\n",
      "epoch 1072 train loss 0.007186 valid loss 0.007089\n",
      "epoch 1073 train loss 0.006974 valid loss 0.007780\n",
      "epoch 1074 train loss 0.008198 valid loss 0.008506\n",
      "epoch 1075 train loss 0.007207 valid loss 0.007644\n",
      "epoch 1076 train loss 0.007341 valid loss 0.007701\n",
      "epoch 1077 train loss 0.007798 valid loss 0.009475\n",
      "epoch 1078 train loss 0.008131 valid loss 0.009359\n",
      "epoch 1079 train loss 0.007887 valid loss 0.006943\n",
      "epoch 1080 train loss 0.008234 valid loss 0.009261\n",
      "epoch 1081 train loss 0.008317 valid loss 0.007513\n",
      "epoch 1082 train loss 0.007147 valid loss 0.006427\n",
      "epoch 1083 train loss 0.006936 valid loss 0.007661\n",
      "epoch 1084 train loss 0.007004 valid loss 0.007133\n",
      "epoch 1085 train loss 0.007740 valid loss 0.007799\n",
      "epoch 1086 train loss 0.007162 valid loss 0.010557\n",
      "epoch 1087 train loss 0.008502 valid loss 0.007121\n",
      "epoch 1088 train loss 0.006884 valid loss 0.007865\n",
      "epoch 1089 train loss 0.007183 valid loss 0.007133\n",
      "epoch 1090 train loss 0.007014 valid loss 0.007225\n",
      "epoch 1091 train loss 0.007790 valid loss 0.006906\n",
      "epoch 1092 train loss 0.006470 valid loss 0.009561\n",
      "epoch 1093 train loss 0.007785 valid loss 0.007765\n",
      "epoch 1094 train loss 0.007157 valid loss 0.007233\n",
      "epoch 1095 train loss 0.006619 valid loss 0.007493\n",
      "epoch 1096 train loss 0.007937 valid loss 0.008902\n",
      "epoch 1097 train loss 0.007786 valid loss 0.007299\n",
      "epoch 1098 train loss 0.007299 valid loss 0.006791\n",
      "epoch 1099 train loss 0.007228 valid loss 0.008556\n",
      "epoch 1100 train loss 0.007543 valid loss 0.011411\n",
      "epoch 1101 train loss 0.010628 valid loss 0.007502\n",
      "epoch 1102 train loss 0.006826 valid loss 0.009022\n",
      "epoch 1103 train loss 0.007679 valid loss 0.008224\n",
      "epoch 1104 train loss 0.006982 valid loss 0.007960\n",
      "epoch 1105 train loss 0.008048 valid loss 0.006715\n",
      "epoch 1106 train loss 0.007234 valid loss 0.007277\n",
      "epoch 1107 train loss 0.006812 valid loss 0.007838\n",
      "epoch 1108 train loss 0.007624 valid loss 0.008818\n",
      "epoch 1109 train loss 0.008464 valid loss 0.007819\n",
      "epoch 1110 train loss 0.008465 valid loss 0.008496\n",
      "epoch 1111 train loss 0.007559 valid loss 0.008136\n",
      "epoch 1112 train loss 0.008142 valid loss 0.008958\n",
      "epoch 1113 train loss 0.008486 valid loss 0.009033\n",
      "epoch 1114 train loss 0.008369 valid loss 0.009502\n",
      "epoch 1115 train loss 0.008694 valid loss 0.009386\n",
      "epoch 1116 train loss 0.008272 valid loss 0.011840\n",
      "epoch 1117 train loss 0.009057 valid loss 0.006341\n",
      "epoch 1118 train loss 0.007286 valid loss 0.007274\n",
      "epoch 1119 train loss 0.007611 valid loss 0.008935\n",
      "epoch 1120 train loss 0.007581 valid loss 0.008874\n",
      "epoch 1121 train loss 0.007450 valid loss 0.006961\n",
      "epoch 1122 train loss 0.007335 valid loss 0.007234\n",
      "epoch 1123 train loss 0.007167 valid loss 0.008219\n",
      "epoch 1124 train loss 0.007363 valid loss 0.007826\n",
      "epoch 1125 train loss 0.007545 valid loss 0.010389\n",
      "epoch 1126 train loss 0.007823 valid loss 0.008240\n",
      "epoch 1127 train loss 0.009106 valid loss 0.007120\n",
      "epoch 1128 train loss 0.006875 valid loss 0.006975\n",
      "epoch 1129 train loss 0.007550 valid loss 0.007976\n",
      "epoch 1130 train loss 0.007710 valid loss 0.007879\n",
      "epoch 1131 train loss 0.007294 valid loss 0.007503\n",
      "epoch 1132 train loss 0.007725 valid loss 0.006621\n",
      "epoch 1133 train loss 0.006222 valid loss 0.007228\n",
      "epoch 1134 train loss 0.008052 valid loss 0.010469\n",
      "epoch 1135 train loss 0.007857 valid loss 0.008977\n",
      "epoch 1136 train loss 0.007793 valid loss 0.010088\n",
      "epoch 1137 train loss 0.007971 valid loss 0.006869\n",
      "epoch 1138 train loss 0.007399 valid loss 0.006806\n",
      "epoch 1139 train loss 0.006935 valid loss 0.007041\n",
      "epoch 1140 train loss 0.006748 valid loss 0.008482\n",
      "epoch 1141 train loss 0.006871 valid loss 0.006500\n",
      "epoch 1142 train loss 0.006790 valid loss 0.006889\n",
      "epoch 1143 train loss 0.007923 valid loss 0.008457\n",
      "epoch 1144 train loss 0.007989 valid loss 0.008500\n",
      "epoch 1145 train loss 0.007337 valid loss 0.007454\n",
      "epoch 1146 train loss 0.007240 valid loss 0.007503\n",
      "epoch 1147 train loss 0.006904 valid loss 0.007420\n",
      "epoch 1148 train loss 0.006858 valid loss 0.007064\n",
      "epoch 1149 train loss 0.007200 valid loss 0.007091\n",
      "epoch 1150 train loss 0.006461 valid loss 0.006432\n",
      "epoch 1151 train loss 0.006648 valid loss 0.006755\n",
      "epoch 1152 train loss 0.006338 valid loss 0.007088\n",
      "epoch 1153 train loss 0.006569 valid loss 0.006428\n",
      "epoch 1154 train loss 0.006393 valid loss 0.006728\n",
      "epoch 1155 train loss 0.006436 valid loss 0.006758\n",
      "epoch 1156 train loss 0.006883 valid loss 0.006632\n",
      "epoch 1157 train loss 0.006789 valid loss 0.007331\n",
      "epoch 1158 train loss 0.007471 valid loss 0.007275\n",
      "epoch 1159 train loss 0.007624 valid loss 0.007712\n",
      "epoch 1160 train loss 0.007061 valid loss 0.006992\n",
      "epoch 1161 train loss 0.006518 valid loss 0.007263\n",
      "epoch 1162 train loss 0.007103 valid loss 0.007524\n",
      "epoch 1163 train loss 0.007779 valid loss 0.007193\n",
      "epoch 1164 train loss 0.006538 valid loss 0.007946\n",
      "epoch 1165 train loss 0.007453 valid loss 0.009104\n",
      "epoch 1166 train loss 0.008448 valid loss 0.007498\n",
      "epoch 1167 train loss 0.007136 valid loss 0.006798\n",
      "epoch 1168 train loss 0.006579 valid loss 0.006974\n",
      "epoch 1169 train loss 0.007336 valid loss 0.006313\n",
      "epoch 1170 train loss 0.006229 valid loss 0.006847\n",
      "epoch 1171 train loss 0.006378 valid loss 0.006383\n",
      "epoch 1172 train loss 0.009491 valid loss 0.007945\n",
      "epoch 1173 train loss 0.007895 valid loss 0.006677\n",
      "epoch 1174 train loss 0.006292 valid loss 0.006418\n",
      "epoch 1175 train loss 0.006489 valid loss 0.008880\n",
      "epoch 1176 train loss 0.008177 valid loss 0.009932\n",
      "epoch 1177 train loss 0.007958 valid loss 0.007168\n",
      "epoch 1178 train loss 0.007170 valid loss 0.009199\n",
      "epoch 1179 train loss 0.007692 valid loss 0.006172\n",
      "epoch 1180 train loss 0.006932 valid loss 0.006798\n",
      "epoch 1181 train loss 0.006749 valid loss 0.010102\n",
      "epoch 1182 train loss 0.007684 valid loss 0.008616\n",
      "epoch 1183 train loss 0.008437 valid loss 0.007671\n",
      "epoch 1184 train loss 0.006972 valid loss 0.008363\n",
      "epoch 1185 train loss 0.007009 valid loss 0.008646\n",
      "epoch 1186 train loss 0.007055 valid loss 0.007531\n",
      "epoch 1187 train loss 0.006601 valid loss 0.006430\n",
      "epoch 1188 train loss 0.007075 valid loss 0.007643\n",
      "epoch 1189 train loss 0.007581 valid loss 0.007431\n",
      "epoch 1190 train loss 0.006907 valid loss 0.007240\n",
      "epoch 1191 train loss 0.007611 valid loss 0.008410\n",
      "epoch 1192 train loss 0.008116 valid loss 0.006763\n",
      "epoch 1193 train loss 0.006232 valid loss 0.006447\n",
      "epoch 1194 train loss 0.007130 valid loss 0.007841\n",
      "epoch 1195 train loss 0.007431 valid loss 0.006721\n",
      "epoch 1196 train loss 0.006338 valid loss 0.006516\n",
      "epoch 1197 train loss 0.006898 valid loss 0.006535\n",
      "epoch 1198 train loss 0.006664 valid loss 0.006613\n",
      "epoch 1199 train loss 0.006085 valid loss 0.006477\n",
      "epoch 1200 train loss 0.005900 valid loss 0.007234\n",
      "epoch 1201 train loss 0.006851 valid loss 0.006455\n",
      "epoch 1202 train loss 0.006757 valid loss 0.006838\n",
      "epoch 1203 train loss 0.006699 valid loss 0.006331\n",
      "epoch 1204 train loss 0.006455 valid loss 0.006182\n",
      "epoch 1205 train loss 0.006181 valid loss 0.006243\n",
      "epoch 1206 train loss 0.006437 valid loss 0.006364\n",
      "epoch 1207 train loss 0.006308 valid loss 0.009413\n",
      "epoch 1208 train loss 0.007061 valid loss 0.006711\n",
      "epoch 1209 train loss 0.006513 valid loss 0.008246\n",
      "epoch 1210 train loss 0.006957 valid loss 0.006340\n",
      "epoch 1211 train loss 0.005945 valid loss 0.006132\n",
      "epoch 1212 train loss 0.005971 valid loss 0.006465\n",
      "epoch 1213 train loss 0.006540 valid loss 0.006460\n",
      "epoch 1214 train loss 0.006320 valid loss 0.007675\n",
      "epoch 1215 train loss 0.007402 valid loss 0.006641\n",
      "epoch 1216 train loss 0.006906 valid loss 0.008334\n",
      "epoch 1217 train loss 0.007352 valid loss 0.008173\n",
      "epoch 1218 train loss 0.007432 valid loss 0.006689\n",
      "epoch 1219 train loss 0.006168 valid loss 0.006431\n",
      "epoch 1220 train loss 0.007793 valid loss 0.006495\n",
      "epoch 1221 train loss 0.006118 valid loss 0.006265\n",
      "epoch 1222 train loss 0.006431 valid loss 0.008202\n",
      "epoch 1223 train loss 0.007395 valid loss 0.006260\n",
      "epoch 1224 train loss 0.006356 valid loss 0.007838\n",
      "epoch 1225 train loss 0.007188 valid loss 0.006715\n",
      "epoch 1226 train loss 0.007116 valid loss 0.006893\n",
      "epoch 1227 train loss 0.006763 valid loss 0.007314\n",
      "epoch 1228 train loss 0.006434 valid loss 0.006567\n",
      "epoch 1229 train loss 0.006516 valid loss 0.007672\n",
      "epoch 1230 train loss 0.006790 valid loss 0.007557\n",
      "epoch 1231 train loss 0.007059 valid loss 0.006062\n",
      "epoch 1232 train loss 0.007399 valid loss 0.007224\n",
      "epoch 1233 train loss 0.007588 valid loss 0.007395\n",
      "epoch 1234 train loss 0.006729 valid loss 0.006843\n",
      "epoch 1235 train loss 0.006650 valid loss 0.007046\n",
      "epoch 1236 train loss 0.006742 valid loss 0.007819\n",
      "epoch 1237 train loss 0.007044 valid loss 0.006531\n",
      "epoch 1238 train loss 0.006942 valid loss 0.009146\n",
      "epoch 1239 train loss 0.007288 valid loss 0.007039\n",
      "epoch 1240 train loss 0.006180 valid loss 0.008424\n",
      "epoch 1241 train loss 0.007322 valid loss 0.007216\n",
      "epoch 1242 train loss 0.007343 valid loss 0.007906\n",
      "epoch 1243 train loss 0.008257 valid loss 0.007360\n",
      "epoch 1244 train loss 0.007264 valid loss 0.007197\n",
      "epoch 1245 train loss 0.007225 valid loss 0.008747\n",
      "epoch 1246 train loss 0.006934 valid loss 0.006632\n",
      "epoch 1247 train loss 0.006347 valid loss 0.009364\n",
      "epoch 1248 train loss 0.007803 valid loss 0.006896\n",
      "epoch 1249 train loss 0.006556 valid loss 0.006774\n",
      "epoch 1250 train loss 0.007689 valid loss 0.007000\n",
      "epoch 1251 train loss 0.007077 valid loss 0.006774\n",
      "epoch 1252 train loss 0.006385 valid loss 0.007259\n",
      "epoch 1253 train loss 0.007625 valid loss 0.006452\n",
      "epoch 1254 train loss 0.007534 valid loss 0.006651\n",
      "epoch 1255 train loss 0.006500 valid loss 0.006454\n",
      "epoch 1256 train loss 0.007246 valid loss 0.007249\n",
      "epoch 1257 train loss 0.007444 valid loss 0.007911\n",
      "epoch 1258 train loss 0.007820 valid loss 0.007448\n",
      "epoch 1259 train loss 0.007593 valid loss 0.007335\n",
      "epoch 1260 train loss 0.006171 valid loss 0.007032\n",
      "epoch 1261 train loss 0.006494 valid loss 0.006774\n",
      "epoch 1262 train loss 0.006408 valid loss 0.007522\n",
      "epoch 1263 train loss 0.006341 valid loss 0.006681\n",
      "epoch 1264 train loss 0.006625 valid loss 0.006833\n",
      "epoch 1265 train loss 0.006461 valid loss 0.008362\n",
      "epoch 1266 train loss 0.007054 valid loss 0.007966\n",
      "epoch 1267 train loss 0.007040 valid loss 0.006469\n",
      "epoch 1268 train loss 0.006933 valid loss 0.007072\n",
      "epoch 1269 train loss 0.007129 valid loss 0.007679\n",
      "epoch 1270 train loss 0.007185 valid loss 0.006656\n",
      "epoch 1271 train loss 0.007896 valid loss 0.007552\n",
      "epoch 1272 train loss 0.006342 valid loss 0.006827\n",
      "epoch 1273 train loss 0.006657 valid loss 0.007305\n",
      "epoch 1274 train loss 0.007011 valid loss 0.007467\n",
      "epoch 1275 train loss 0.007150 valid loss 0.006786\n",
      "epoch 1276 train loss 0.007221 valid loss 0.006735\n",
      "epoch 1277 train loss 0.006591 valid loss 0.006712\n",
      "epoch 1278 train loss 0.006551 valid loss 0.006459\n",
      "epoch 1279 train loss 0.006419 valid loss 0.010118\n",
      "epoch 1280 train loss 0.008231 valid loss 0.006761\n",
      "epoch 1281 train loss 0.006522 valid loss 0.008715\n",
      "epoch 1282 train loss 0.007262 valid loss 0.006816\n",
      "epoch 1283 train loss 0.006596 valid loss 0.006796\n",
      "epoch 1284 train loss 0.006623 valid loss 0.007106\n",
      "epoch 1285 train loss 0.007253 valid loss 0.007268\n",
      "epoch 1286 train loss 0.006715 valid loss 0.006682\n",
      "epoch 1287 train loss 0.006798 valid loss 0.007611\n",
      "epoch 1288 train loss 0.007413 valid loss 0.006867\n",
      "epoch 1289 train loss 0.007852 valid loss 0.009747\n",
      "epoch 1290 train loss 0.008760 valid loss 0.008008\n",
      "epoch 1291 train loss 0.006706 valid loss 0.007017\n",
      "epoch 1292 train loss 0.006570 valid loss 0.007635\n",
      "epoch 1293 train loss 0.007095 valid loss 0.007168\n",
      "epoch 1294 train loss 0.007264 valid loss 0.006682\n",
      "epoch 1295 train loss 0.006657 valid loss 0.008461\n",
      "epoch 1296 train loss 0.008636 valid loss 0.007201\n",
      "epoch 1297 train loss 0.007654 valid loss 0.006714\n",
      "epoch 1298 train loss 0.007006 valid loss 0.008983\n",
      "epoch 1299 train loss 0.007747 valid loss 0.006171\n",
      "epoch 1300 train loss 0.006717 valid loss 0.006413\n",
      "epoch 1301 train loss 0.006646 valid loss 0.006792\n",
      "epoch 1302 train loss 0.007341 valid loss 0.007474\n",
      "epoch 1303 train loss 0.007037 valid loss 0.006697\n",
      "epoch 1304 train loss 0.006649 valid loss 0.008266\n",
      "epoch 1305 train loss 0.007265 valid loss 0.006256\n",
      "epoch 1306 train loss 0.006504 valid loss 0.006777\n",
      "epoch 1307 train loss 0.006345 valid loss 0.006681\n",
      "epoch 1308 train loss 0.006527 valid loss 0.007275\n",
      "epoch 1309 train loss 0.006491 valid loss 0.006271\n",
      "epoch 1310 train loss 0.006876 valid loss 0.009555\n",
      "epoch 1311 train loss 0.007478 valid loss 0.006353\n",
      "epoch 1312 train loss 0.005995 valid loss 0.006957\n",
      "epoch 1313 train loss 0.006599 valid loss 0.006436\n",
      "epoch 1314 train loss 0.006125 valid loss 0.006791\n",
      "epoch 1315 train loss 0.006733 valid loss 0.006737\n",
      "epoch 1316 train loss 0.006824 valid loss 0.006654\n",
      "epoch 1317 train loss 0.006946 valid loss 0.007459\n",
      "epoch 1318 train loss 0.006945 valid loss 0.007506\n",
      "epoch 1319 train loss 0.006452 valid loss 0.007167\n",
      "epoch 1320 train loss 0.007312 valid loss 0.007333\n",
      "epoch 1321 train loss 0.007635 valid loss 0.013185\n",
      "epoch 1322 train loss 0.009220 valid loss 0.007393\n",
      "epoch 1323 train loss 0.007309 valid loss 0.006777\n",
      "epoch 1324 train loss 0.006817 valid loss 0.006980\n",
      "epoch 1325 train loss 0.008168 valid loss 0.006963\n",
      "epoch 1326 train loss 0.006943 valid loss 0.006888\n",
      "epoch 1327 train loss 0.006950 valid loss 0.006499\n",
      "epoch 1328 train loss 0.006419 valid loss 0.007221\n",
      "epoch 1329 train loss 0.006803 valid loss 0.008747\n",
      "epoch 1330 train loss 0.007103 valid loss 0.007535\n",
      "epoch 1331 train loss 0.006945 valid loss 0.006404\n",
      "epoch 1332 train loss 0.006234 valid loss 0.007112\n",
      "epoch 1333 train loss 0.006916 valid loss 0.007103\n",
      "epoch 1334 train loss 0.006790 valid loss 0.008613\n",
      "epoch 1335 train loss 0.007200 valid loss 0.006485\n",
      "epoch 1336 train loss 0.006935 valid loss 0.007044\n",
      "epoch 1337 train loss 0.006293 valid loss 0.006709\n",
      "epoch 1338 train loss 0.006554 valid loss 0.006175\n",
      "epoch 1339 train loss 0.006228 valid loss 0.006492\n",
      "epoch 1340 train loss 0.006656 valid loss 0.006677\n",
      "epoch 1341 train loss 0.006361 valid loss 0.008683\n",
      "epoch 1342 train loss 0.007177 valid loss 0.007457\n",
      "epoch 1343 train loss 0.006409 valid loss 0.007385\n",
      "epoch 1344 train loss 0.006640 valid loss 0.006189\n",
      "epoch 1345 train loss 0.006210 valid loss 0.007671\n",
      "epoch 1346 train loss 0.006723 valid loss 0.006733\n",
      "epoch 1347 train loss 0.006276 valid loss 0.006794\n",
      "epoch 1348 train loss 0.006443 valid loss 0.006312\n",
      "epoch 1349 train loss 0.006381 valid loss 0.006382\n",
      "epoch 1350 train loss 0.006152 valid loss 0.007051\n",
      "epoch 1351 train loss 0.006333 valid loss 0.006114\n",
      "epoch 1352 train loss 0.006207 valid loss 0.006703\n",
      "epoch 1353 train loss 0.006284 valid loss 0.006134\n",
      "epoch 1354 train loss 0.005957 valid loss 0.006007\n",
      "epoch 1355 train loss 0.005814 valid loss 0.006538\n",
      "epoch 1356 train loss 0.006312 valid loss 0.007838\n",
      "epoch 1357 train loss 0.006744 valid loss 0.006853\n",
      "epoch 1358 train loss 0.006724 valid loss 0.007097\n",
      "epoch 1359 train loss 0.007472 valid loss 0.006750\n",
      "epoch 1360 train loss 0.006415 valid loss 0.007475\n",
      "epoch 1361 train loss 0.008121 valid loss 0.008384\n",
      "epoch 1362 train loss 0.007410 valid loss 0.006397\n",
      "epoch 1363 train loss 0.007366 valid loss 0.006283\n",
      "epoch 1364 train loss 0.006050 valid loss 0.007464\n",
      "epoch 1365 train loss 0.006884 valid loss 0.006087\n",
      "epoch 1366 train loss 0.005985 valid loss 0.006715\n",
      "epoch 1367 train loss 0.006230 valid loss 0.006385\n",
      "epoch 1368 train loss 0.006440 valid loss 0.006772\n",
      "epoch 1369 train loss 0.007240 valid loss 0.006974\n",
      "epoch 1370 train loss 0.006858 valid loss 0.007041\n",
      "epoch 1371 train loss 0.006792 valid loss 0.006985\n",
      "epoch 1372 train loss 0.006637 valid loss 0.006682\n",
      "epoch 1373 train loss 0.006611 valid loss 0.006757\n",
      "epoch 1374 train loss 0.006526 valid loss 0.008197\n",
      "epoch 1375 train loss 0.006805 valid loss 0.007770\n",
      "epoch 1376 train loss 0.006697 valid loss 0.007280\n",
      "epoch 1377 train loss 0.007177 valid loss 0.008224\n",
      "epoch 1378 train loss 0.007169 valid loss 0.006369\n",
      "epoch 1379 train loss 0.005996 valid loss 0.006632\n",
      "epoch 1380 train loss 0.006436 valid loss 0.006499\n",
      "epoch 1381 train loss 0.006398 valid loss 0.007262\n",
      "epoch 1382 train loss 0.007213 valid loss 0.006327\n",
      "epoch 1383 train loss 0.006245 valid loss 0.007236\n",
      "epoch 1384 train loss 0.006296 valid loss 0.006402\n",
      "epoch 1385 train loss 0.006036 valid loss 0.006492\n",
      "epoch 1386 train loss 0.006481 valid loss 0.007424\n",
      "epoch 1387 train loss 0.006260 valid loss 0.006885\n",
      "epoch 1388 train loss 0.006859 valid loss 0.006823\n",
      "epoch 1389 train loss 0.006169 valid loss 0.006608\n",
      "epoch 1390 train loss 0.006223 valid loss 0.006780\n",
      "epoch 1391 train loss 0.006675 valid loss 0.007045\n",
      "epoch 1392 train loss 0.006633 valid loss 0.009298\n",
      "epoch 1393 train loss 0.007195 valid loss 0.007325\n",
      "epoch 1394 train loss 0.007476 valid loss 0.006739\n",
      "epoch 1395 train loss 0.006420 valid loss 0.006698\n",
      "epoch 1396 train loss 0.006506 valid loss 0.007011\n",
      "epoch 1397 train loss 0.006731 valid loss 0.007482\n",
      "epoch 1398 train loss 0.006599 valid loss 0.008091\n",
      "epoch 1399 train loss 0.007211 valid loss 0.007546\n",
      "epoch 1400 train loss 0.006252 valid loss 0.007527\n",
      "epoch 1401 train loss 0.007467 valid loss 0.006586\n",
      "epoch 1402 train loss 0.006450 valid loss 0.007432\n",
      "epoch 1403 train loss 0.007897 valid loss 0.008284\n",
      "epoch 1404 train loss 0.006663 valid loss 0.006550\n",
      "epoch 1405 train loss 0.006327 valid loss 0.006123\n",
      "epoch 1406 train loss 0.006150 valid loss 0.006843\n",
      "epoch 1407 train loss 0.006576 valid loss 0.009150\n",
      "epoch 1408 train loss 0.007240 valid loss 0.006387\n",
      "epoch 1409 train loss 0.006460 valid loss 0.007139\n",
      "epoch 1410 train loss 0.006525 valid loss 0.006592\n",
      "epoch 1411 train loss 0.006598 valid loss 0.006391\n",
      "epoch 1412 train loss 0.006265 valid loss 0.006102\n",
      "epoch 1413 train loss 0.006319 valid loss 0.007419\n",
      "epoch 1414 train loss 0.006990 valid loss 0.006516\n",
      "epoch 1415 train loss 0.006252 valid loss 0.006019\n",
      "epoch 1416 train loss 0.006609 valid loss 0.006786\n",
      "epoch 1417 train loss 0.006724 valid loss 0.007306\n",
      "epoch 1418 train loss 0.007153 valid loss 0.006904\n",
      "epoch 1419 train loss 0.006281 valid loss 0.007288\n",
      "epoch 1420 train loss 0.007292 valid loss 0.006577\n",
      "epoch 1421 train loss 0.006699 valid loss 0.006579\n",
      "epoch 1422 train loss 0.006877 valid loss 0.007877\n",
      "epoch 1423 train loss 0.006710 valid loss 0.006269\n",
      "epoch 1424 train loss 0.006631 valid loss 0.006246\n",
      "epoch 1425 train loss 0.006440 valid loss 0.006918\n",
      "epoch 1426 train loss 0.006425 valid loss 0.007199\n",
      "epoch 1427 train loss 0.007018 valid loss 0.006952\n",
      "epoch 1428 train loss 0.006472 valid loss 0.006632\n",
      "epoch 1429 train loss 0.006322 valid loss 0.006261\n",
      "epoch 1430 train loss 0.006005 valid loss 0.006184\n",
      "epoch 1431 train loss 0.006627 valid loss 0.006608\n",
      "epoch 1432 train loss 0.006428 valid loss 0.007160\n",
      "epoch 1433 train loss 0.006484 valid loss 0.006371\n",
      "epoch 1434 train loss 0.007040 valid loss 0.007983\n",
      "epoch 1435 train loss 0.007149 valid loss 0.007254\n",
      "epoch 1436 train loss 0.006913 valid loss 0.006599\n",
      "epoch 1437 train loss 0.006272 valid loss 0.007513\n",
      "epoch 1438 train loss 0.006976 valid loss 0.006096\n",
      "epoch 1439 train loss 0.006073 valid loss 0.006220\n",
      "epoch 1440 train loss 0.006373 valid loss 0.007130\n",
      "epoch 1441 train loss 0.006424 valid loss 0.006674\n",
      "epoch 1442 train loss 0.006252 valid loss 0.006140\n",
      "epoch 1443 train loss 0.006179 valid loss 0.006676\n",
      "epoch 1444 train loss 0.006820 valid loss 0.006985\n",
      "epoch 1445 train loss 0.006347 valid loss 0.006477\n",
      "epoch 1446 train loss 0.006384 valid loss 0.006554\n",
      "epoch 1447 train loss 0.007171 valid loss 0.007065\n",
      "epoch 1448 train loss 0.006234 valid loss 0.006030\n",
      "epoch 1449 train loss 0.006136 valid loss 0.006061\n",
      "epoch 1450 train loss 0.005857 valid loss 0.006219\n",
      "epoch 1451 train loss 0.006506 valid loss 0.006292\n",
      "epoch 1452 train loss 0.006557 valid loss 0.007343\n",
      "epoch 1453 train loss 0.006340 valid loss 0.006867\n",
      "epoch 1454 train loss 0.006164 valid loss 0.005799\n",
      "epoch 1455 train loss 0.005614 valid loss 0.006645\n",
      "epoch 1456 train loss 0.006255 valid loss 0.006248\n",
      "epoch 1457 train loss 0.006062 valid loss 0.007254\n",
      "epoch 1458 train loss 0.006636 valid loss 0.006151\n",
      "epoch 1459 train loss 0.006723 valid loss 0.006429\n",
      "epoch 1460 train loss 0.006326 valid loss 0.008010\n",
      "epoch 1461 train loss 0.006497 valid loss 0.006267\n",
      "epoch 1462 train loss 0.006216 valid loss 0.007665\n",
      "epoch 1463 train loss 0.007144 valid loss 0.006569\n",
      "epoch 1464 train loss 0.006246 valid loss 0.006556\n",
      "epoch 1465 train loss 0.006633 valid loss 0.006773\n",
      "epoch 1466 train loss 0.006124 valid loss 0.006541\n",
      "epoch 1467 train loss 0.006452 valid loss 0.006483\n",
      "epoch 1468 train loss 0.006120 valid loss 0.006456\n",
      "epoch 1469 train loss 0.006211 valid loss 0.005963\n",
      "epoch 1470 train loss 0.006420 valid loss 0.007250\n",
      "epoch 1471 train loss 0.005918 valid loss 0.006416\n",
      "epoch 1472 train loss 0.006068 valid loss 0.007472\n",
      "epoch 1473 train loss 0.006623 valid loss 0.006073\n",
      "epoch 1474 train loss 0.006502 valid loss 0.007044\n",
      "epoch 1475 train loss 0.006143 valid loss 0.006347\n",
      "epoch 1476 train loss 0.005934 valid loss 0.006281\n",
      "epoch 1477 train loss 0.005972 valid loss 0.006080\n",
      "epoch 1478 train loss 0.006190 valid loss 0.006556\n",
      "epoch 1479 train loss 0.006331 valid loss 0.007288\n",
      "epoch 1480 train loss 0.006275 valid loss 0.006696\n",
      "epoch 1481 train loss 0.006606 valid loss 0.006472\n",
      "epoch 1482 train loss 0.005994 valid loss 0.006659\n",
      "epoch 1483 train loss 0.006359 valid loss 0.007018\n",
      "epoch 1484 train loss 0.006431 valid loss 0.006115\n",
      "epoch 1485 train loss 0.005829 valid loss 0.005814\n",
      "epoch 1486 train loss 0.005809 valid loss 0.006562\n",
      "epoch 1487 train loss 0.006666 valid loss 0.007171\n",
      "epoch 1488 train loss 0.006531 valid loss 0.006399\n",
      "epoch 1489 train loss 0.006011 valid loss 0.006299\n",
      "epoch 1490 train loss 0.005953 valid loss 0.006522\n",
      "epoch 1491 train loss 0.006124 valid loss 0.006176\n",
      "epoch 1492 train loss 0.005860 valid loss 0.005994\n",
      "epoch 1493 train loss 0.005923 valid loss 0.007292\n",
      "epoch 1494 train loss 0.006384 valid loss 0.006389\n",
      "epoch 1495 train loss 0.006088 valid loss 0.006240\n",
      "epoch 1496 train loss 0.005703 valid loss 0.005912\n",
      "epoch 1497 train loss 0.005733 valid loss 0.006241\n",
      "epoch 1498 train loss 0.006075 valid loss 0.006878\n",
      "epoch 1499 train loss 0.005953 valid loss 0.006556\n",
      "epoch 1500 train loss 0.006211 valid loss 0.006288\n",
      "epoch 1501 train loss 0.005920 valid loss 0.006336\n",
      "epoch 1502 train loss 0.005713 valid loss 0.005692\n",
      "epoch 1503 train loss 0.006258 valid loss 0.007015\n",
      "epoch 1504 train loss 0.006514 valid loss 0.006437\n",
      "epoch 1505 train loss 0.006554 valid loss 0.006056\n",
      "epoch 1506 train loss 0.006344 valid loss 0.006705\n",
      "epoch 1507 train loss 0.006605 valid loss 0.006125\n",
      "epoch 1508 train loss 0.006138 valid loss 0.006112\n",
      "epoch 1509 train loss 0.005973 valid loss 0.006091\n",
      "epoch 1510 train loss 0.005831 valid loss 0.006628\n",
      "epoch 1511 train loss 0.006064 valid loss 0.005985\n",
      "epoch 1512 train loss 0.005900 valid loss 0.005879\n",
      "epoch 1513 train loss 0.006110 valid loss 0.007087\n",
      "epoch 1514 train loss 0.006719 valid loss 0.006656\n",
      "epoch 1515 train loss 0.006036 valid loss 0.006185\n",
      "epoch 1516 train loss 0.006171 valid loss 0.006593\n",
      "epoch 1517 train loss 0.006350 valid loss 0.006754\n",
      "epoch 1518 train loss 0.006175 valid loss 0.006699\n",
      "epoch 1519 train loss 0.006392 valid loss 0.006423\n",
      "epoch 1520 train loss 0.005946 valid loss 0.006234\n",
      "epoch 1521 train loss 0.006352 valid loss 0.006376\n",
      "epoch 1522 train loss 0.006003 valid loss 0.006024\n",
      "epoch 1523 train loss 0.005812 valid loss 0.006321\n",
      "epoch 1524 train loss 0.006035 valid loss 0.006350\n",
      "epoch 1525 train loss 0.005821 valid loss 0.006193\n",
      "epoch 1526 train loss 0.006058 valid loss 0.006145\n",
      "epoch 1527 train loss 0.005626 valid loss 0.006542\n",
      "epoch 1528 train loss 0.006240 valid loss 0.006759\n",
      "epoch 1529 train loss 0.006344 valid loss 0.006009\n",
      "epoch 1530 train loss 0.005684 valid loss 0.006464\n",
      "epoch 1531 train loss 0.006137 valid loss 0.006338\n",
      "epoch 1532 train loss 0.006460 valid loss 0.005930\n",
      "epoch 1533 train loss 0.006124 valid loss 0.006190\n",
      "epoch 1534 train loss 0.005768 valid loss 0.006195\n",
      "epoch 1535 train loss 0.006054 valid loss 0.007047\n",
      "epoch 1536 train loss 0.006115 valid loss 0.006345\n",
      "epoch 1537 train loss 0.006003 valid loss 0.006550\n",
      "epoch 1538 train loss 0.006061 valid loss 0.005933\n",
      "epoch 1539 train loss 0.006122 valid loss 0.006728\n",
      "epoch 1540 train loss 0.006372 valid loss 0.007663\n",
      "epoch 1541 train loss 0.006360 valid loss 0.007235\n",
      "epoch 1542 train loss 0.006815 valid loss 0.006056\n",
      "epoch 1543 train loss 0.006085 valid loss 0.005922\n",
      "epoch 1544 train loss 0.005667 valid loss 0.007367\n",
      "epoch 1545 train loss 0.006355 valid loss 0.006332\n",
      "epoch 1546 train loss 0.006085 valid loss 0.006104\n",
      "epoch 1547 train loss 0.005747 valid loss 0.006487\n",
      "epoch 1548 train loss 0.006808 valid loss 0.006138\n",
      "epoch 1549 train loss 0.006074 valid loss 0.006463\n",
      "epoch 1550 train loss 0.005952 valid loss 0.006112\n",
      "epoch 1551 train loss 0.005801 valid loss 0.006231\n",
      "epoch 1552 train loss 0.005917 valid loss 0.006423\n",
      "epoch 1553 train loss 0.006103 valid loss 0.006542\n",
      "epoch 1554 train loss 0.006347 valid loss 0.006481\n",
      "epoch 1555 train loss 0.006065 valid loss 0.006336\n",
      "epoch 1556 train loss 0.006056 valid loss 0.007010\n",
      "epoch 1557 train loss 0.007036 valid loss 0.008208\n",
      "epoch 1558 train loss 0.006568 valid loss 0.006155\n",
      "epoch 1559 train loss 0.006605 valid loss 0.006406\n",
      "epoch 1560 train loss 0.006258 valid loss 0.005888\n",
      "epoch 1561 train loss 0.005693 valid loss 0.005812\n",
      "epoch 1562 train loss 0.005850 valid loss 0.006431\n",
      "epoch 1563 train loss 0.006659 valid loss 0.006355\n",
      "epoch 1564 train loss 0.005896 valid loss 0.006501\n",
      "epoch 1565 train loss 0.006033 valid loss 0.006095\n",
      "epoch 1566 train loss 0.006056 valid loss 0.006126\n",
      "epoch 1567 train loss 0.005970 valid loss 0.006128\n",
      "epoch 1568 train loss 0.005649 valid loss 0.006193\n",
      "epoch 1569 train loss 0.005982 valid loss 0.006600\n",
      "epoch 1570 train loss 0.005933 valid loss 0.006374\n",
      "epoch 1571 train loss 0.005855 valid loss 0.006718\n",
      "epoch 1572 train loss 0.006131 valid loss 0.006132\n",
      "epoch 1573 train loss 0.007025 valid loss 0.006725\n",
      "epoch 1574 train loss 0.006068 valid loss 0.006085\n",
      "epoch 1575 train loss 0.005729 valid loss 0.006284\n",
      "epoch 1576 train loss 0.006021 valid loss 0.005746\n",
      "epoch 1577 train loss 0.005805 valid loss 0.007379\n",
      "epoch 1578 train loss 0.006235 valid loss 0.005980\n",
      "epoch 1579 train loss 0.005650 valid loss 0.011102\n",
      "epoch 1580 train loss 0.007385 valid loss 0.006216\n",
      "epoch 1581 train loss 0.006545 valid loss 0.006518\n",
      "epoch 1582 train loss 0.005754 valid loss 0.006181\n",
      "epoch 1583 train loss 0.005709 valid loss 0.006395\n",
      "epoch 1584 train loss 0.005847 valid loss 0.006049\n",
      "epoch 1585 train loss 0.005785 valid loss 0.006109\n",
      "epoch 1586 train loss 0.005786 valid loss 0.006001\n",
      "epoch 1587 train loss 0.006109 valid loss 0.006399\n",
      "epoch 1588 train loss 0.005837 valid loss 0.007211\n",
      "epoch 1589 train loss 0.007025 valid loss 0.006706\n",
      "epoch 1590 train loss 0.006115 valid loss 0.006158\n",
      "epoch 1591 train loss 0.005975 valid loss 0.006469\n",
      "epoch 1592 train loss 0.005889 valid loss 0.006124\n",
      "epoch 1593 train loss 0.005836 valid loss 0.006298\n",
      "epoch 1594 train loss 0.005923 valid loss 0.005920\n",
      "epoch 1595 train loss 0.005928 valid loss 0.006418\n",
      "epoch 1596 train loss 0.005834 valid loss 0.007729\n",
      "epoch 1597 train loss 0.006636 valid loss 0.006343\n",
      "epoch 1598 train loss 0.005857 valid loss 0.006004\n",
      "epoch 1599 train loss 0.005860 valid loss 0.007129\n",
      "epoch 1600 train loss 0.006180 valid loss 0.006420\n",
      "epoch 1601 train loss 0.005673 valid loss 0.006850\n",
      "epoch 1602 train loss 0.006229 valid loss 0.006249\n",
      "epoch 1603 train loss 0.005940 valid loss 0.005998\n",
      "epoch 1604 train loss 0.006119 valid loss 0.007223\n",
      "epoch 1605 train loss 0.006259 valid loss 0.011943\n",
      "epoch 1606 train loss 0.007765 valid loss 0.006090\n",
      "epoch 1607 train loss 0.005728 valid loss 0.005832\n",
      "epoch 1608 train loss 0.005891 valid loss 0.006250\n",
      "epoch 1609 train loss 0.006242 valid loss 0.006062\n",
      "epoch 1610 train loss 0.005801 valid loss 0.006772\n",
      "epoch 1611 train loss 0.006199 valid loss 0.008211\n",
      "epoch 1612 train loss 0.006811 valid loss 0.006231\n",
      "epoch 1613 train loss 0.005949 valid loss 0.006034\n",
      "epoch 1614 train loss 0.005668 valid loss 0.006422\n",
      "epoch 1615 train loss 0.005669 valid loss 0.005971\n",
      "epoch 1616 train loss 0.005975 valid loss 0.006361\n",
      "epoch 1617 train loss 0.006014 valid loss 0.007779\n",
      "epoch 1618 train loss 0.006882 valid loss 0.006560\n",
      "epoch 1619 train loss 0.005936 valid loss 0.006099\n",
      "epoch 1620 train loss 0.006009 valid loss 0.007190\n",
      "epoch 1621 train loss 0.006508 valid loss 0.007326\n",
      "epoch 1622 train loss 0.006412 valid loss 0.006262\n",
      "epoch 1623 train loss 0.006174 valid loss 0.007363\n",
      "epoch 1624 train loss 0.006438 valid loss 0.006025\n",
      "epoch 1625 train loss 0.006300 valid loss 0.006325\n",
      "epoch 1626 train loss 0.005593 valid loss 0.006578\n",
      "epoch 1627 train loss 0.006030 valid loss 0.006648\n",
      "epoch 1628 train loss 0.006004 valid loss 0.006081\n",
      "epoch 1629 train loss 0.005889 valid loss 0.006392\n",
      "epoch 1630 train loss 0.005828 valid loss 0.006313\n",
      "epoch 1631 train loss 0.005771 valid loss 0.006304\n",
      "epoch 1632 train loss 0.006052 valid loss 0.007198\n",
      "epoch 1633 train loss 0.005974 valid loss 0.008305\n",
      "epoch 1634 train loss 0.007039 valid loss 0.006146\n",
      "epoch 1635 train loss 0.006868 valid loss 0.007132\n",
      "epoch 1636 train loss 0.006406 valid loss 0.006353\n",
      "epoch 1637 train loss 0.005822 valid loss 0.006336\n",
      "epoch 1638 train loss 0.005869 valid loss 0.006341\n",
      "epoch 1639 train loss 0.006327 valid loss 0.005825\n",
      "epoch 1640 train loss 0.005993 valid loss 0.006111\n",
      "epoch 1641 train loss 0.005627 valid loss 0.006234\n",
      "epoch 1642 train loss 0.006186 valid loss 0.006086\n",
      "epoch 1643 train loss 0.005731 valid loss 0.006837\n",
      "epoch 1644 train loss 0.006048 valid loss 0.007172\n",
      "epoch 1645 train loss 0.006325 valid loss 0.006102\n",
      "epoch 1646 train loss 0.005676 valid loss 0.005995\n",
      "epoch 1647 train loss 0.005904 valid loss 0.006014\n",
      "epoch 1648 train loss 0.005991 valid loss 0.005955\n",
      "epoch 1649 train loss 0.005823 valid loss 0.005840\n",
      "epoch 1650 train loss 0.005835 valid loss 0.006114\n",
      "epoch 1651 train loss 0.005941 valid loss 0.006352\n",
      "epoch 1652 train loss 0.006239 valid loss 0.006260\n",
      "epoch 1653 train loss 0.005873 valid loss 0.006038\n",
      "epoch 1654 train loss 0.005836 valid loss 0.006010\n",
      "epoch 1655 train loss 0.005895 valid loss 0.006172\n",
      "epoch 1656 train loss 0.005889 valid loss 0.006294\n",
      "epoch 1657 train loss 0.006140 valid loss 0.006279\n",
      "epoch 1658 train loss 0.006108 valid loss 0.006642\n",
      "epoch 1659 train loss 0.006281 valid loss 0.006214\n",
      "epoch 1660 train loss 0.006108 valid loss 0.005819\n",
      "epoch 1661 train loss 0.006003 valid loss 0.006201\n",
      "epoch 1662 train loss 0.006156 valid loss 0.006937\n",
      "epoch 1663 train loss 0.006182 valid loss 0.006017\n",
      "epoch 1664 train loss 0.005860 valid loss 0.006372\n",
      "epoch 1665 train loss 0.006251 valid loss 0.006444\n",
      "epoch 1666 train loss 0.006125 valid loss 0.006006\n",
      "epoch 1667 train loss 0.005783 valid loss 0.005742\n",
      "epoch 1668 train loss 0.005830 valid loss 0.006472\n",
      "epoch 1669 train loss 0.006015 valid loss 0.006336\n",
      "epoch 1670 train loss 0.006055 valid loss 0.005980\n",
      "epoch 1671 train loss 0.005597 valid loss 0.006445\n",
      "epoch 1672 train loss 0.006106 valid loss 0.006258\n",
      "epoch 1673 train loss 0.006558 valid loss 0.005966\n",
      "epoch 1674 train loss 0.005726 valid loss 0.006090\n",
      "epoch 1675 train loss 0.005834 valid loss 0.006311\n",
      "epoch 1676 train loss 0.006213 valid loss 0.006045\n",
      "epoch 1677 train loss 0.006018 valid loss 0.006268\n",
      "epoch 1678 train loss 0.006140 valid loss 0.006049\n",
      "epoch 1679 train loss 0.006103 valid loss 0.006058\n",
      "epoch 1680 train loss 0.005752 valid loss 0.006039\n",
      "epoch 1681 train loss 0.005978 valid loss 0.006175\n",
      "epoch 1682 train loss 0.005869 valid loss 0.006036\n",
      "epoch 1683 train loss 0.006006 valid loss 0.006041\n",
      "epoch 1684 train loss 0.006290 valid loss 0.006508\n",
      "epoch 1685 train loss 0.006158 valid loss 0.005956\n",
      "epoch 1686 train loss 0.005833 valid loss 0.006891\n",
      "epoch 1687 train loss 0.006159 valid loss 0.006442\n",
      "epoch 1688 train loss 0.006106 valid loss 0.006960\n",
      "epoch 1689 train loss 0.005889 valid loss 0.006750\n",
      "epoch 1690 train loss 0.006069 valid loss 0.007048\n",
      "epoch 1691 train loss 0.006107 valid loss 0.006151\n",
      "epoch 1692 train loss 0.005763 valid loss 0.005780\n",
      "epoch 1693 train loss 0.005915 valid loss 0.006872\n",
      "epoch 1694 train loss 0.006297 valid loss 0.006097\n",
      "epoch 1695 train loss 0.006463 valid loss 0.006511\n",
      "epoch 1696 train loss 0.005968 valid loss 0.006486\n",
      "epoch 1697 train loss 0.005882 valid loss 0.006205\n",
      "epoch 1698 train loss 0.005785 valid loss 0.005939\n",
      "epoch 1699 train loss 0.005565 valid loss 0.006299\n",
      "epoch 1700 train loss 0.005758 valid loss 0.005913\n",
      "epoch 1701 train loss 0.005934 valid loss 0.006460\n",
      "epoch 1702 train loss 0.006070 valid loss 0.007497\n",
      "epoch 1703 train loss 0.006871 valid loss 0.005860\n",
      "epoch 1704 train loss 0.005623 valid loss 0.006274\n",
      "epoch 1705 train loss 0.005845 valid loss 0.006086\n",
      "epoch 1706 train loss 0.005761 valid loss 0.006213\n",
      "epoch 1707 train loss 0.006030 valid loss 0.006392\n",
      "epoch 1708 train loss 0.006021 valid loss 0.006169\n",
      "epoch 1709 train loss 0.005964 valid loss 0.006009\n",
      "epoch 1710 train loss 0.005925 valid loss 0.006939\n",
      "epoch 1711 train loss 0.005942 valid loss 0.006571\n",
      "epoch 1712 train loss 0.006226 valid loss 0.006374\n",
      "epoch 1713 train loss 0.005650 valid loss 0.005866\n",
      "epoch 1714 train loss 0.005418 valid loss 0.006129\n",
      "epoch 1715 train loss 0.005964 valid loss 0.005791\n",
      "epoch 1716 train loss 0.006059 valid loss 0.006805\n",
      "epoch 1717 train loss 0.005909 valid loss 0.005822\n",
      "epoch 1718 train loss 0.005678 valid loss 0.005937\n",
      "epoch 1719 train loss 0.005560 valid loss 0.005900\n",
      "epoch 1720 train loss 0.005799 valid loss 0.006048\n",
      "epoch 1721 train loss 0.005906 valid loss 0.006052\n",
      "epoch 1722 train loss 0.005685 valid loss 0.006691\n",
      "epoch 1723 train loss 0.006202 valid loss 0.005782\n",
      "epoch 1724 train loss 0.005548 valid loss 0.006041\n",
      "epoch 1725 train loss 0.005763 valid loss 0.006795\n",
      "epoch 1726 train loss 0.006077 valid loss 0.006215\n",
      "epoch 1727 train loss 0.005777 valid loss 0.006317\n",
      "epoch 1728 train loss 0.005875 valid loss 0.006150\n",
      "epoch 1729 train loss 0.005992 valid loss 0.006681\n",
      "epoch 1730 train loss 0.006138 valid loss 0.006377\n",
      "epoch 1731 train loss 0.005924 valid loss 0.006324\n",
      "epoch 1732 train loss 0.005955 valid loss 0.006454\n",
      "epoch 1733 train loss 0.006136 valid loss 0.006225\n",
      "epoch 1734 train loss 0.005710 valid loss 0.006018\n",
      "epoch 1735 train loss 0.006564 valid loss 0.006376\n",
      "epoch 1736 train loss 0.006817 valid loss 0.006807\n",
      "epoch 1737 train loss 0.006747 valid loss 0.009173\n",
      "epoch 1738 train loss 0.007274 valid loss 0.005979\n",
      "epoch 1739 train loss 0.006268 valid loss 0.006384\n",
      "epoch 1740 train loss 0.005901 valid loss 0.005918\n",
      "epoch 1741 train loss 0.005852 valid loss 0.007946\n",
      "epoch 1742 train loss 0.006306 valid loss 0.006652\n",
      "epoch 1743 train loss 0.006259 valid loss 0.006053\n",
      "epoch 1744 train loss 0.005949 valid loss 0.006379\n",
      "epoch 1745 train loss 0.006009 valid loss 0.006417\n",
      "epoch 1746 train loss 0.005765 valid loss 0.006110\n",
      "epoch 1747 train loss 0.005669 valid loss 0.006370\n",
      "epoch 1748 train loss 0.005823 valid loss 0.006412\n",
      "epoch 1749 train loss 0.005823 valid loss 0.007088\n",
      "epoch 1750 train loss 0.006549 valid loss 0.006134\n",
      "epoch 1751 train loss 0.006330 valid loss 0.006167\n",
      "epoch 1752 train loss 0.006586 valid loss 0.006172\n",
      "epoch 1753 train loss 0.005949 valid loss 0.005907\n",
      "epoch 1754 train loss 0.005601 valid loss 0.006134\n",
      "epoch 1755 train loss 0.005714 valid loss 0.006634\n",
      "epoch 1756 train loss 0.006039 valid loss 0.005957\n",
      "epoch 1757 train loss 0.005905 valid loss 0.006437\n",
      "epoch 1758 train loss 0.005757 valid loss 0.005839\n",
      "epoch 1759 train loss 0.005711 valid loss 0.006031\n",
      "epoch 1760 train loss 0.005797 valid loss 0.006155\n",
      "epoch 1761 train loss 0.005756 valid loss 0.006734\n",
      "epoch 1762 train loss 0.005865 valid loss 0.005843\n",
      "epoch 1763 train loss 0.005860 valid loss 0.006326\n",
      "epoch 1764 train loss 0.005725 valid loss 0.006591\n",
      "epoch 1765 train loss 0.005930 valid loss 0.005672\n",
      "epoch 1766 train loss 0.005885 valid loss 0.006562\n",
      "epoch 1767 train loss 0.005773 valid loss 0.005801\n",
      "epoch 1768 train loss 0.005757 valid loss 0.006261\n",
      "epoch 1769 train loss 0.005504 valid loss 0.005913\n",
      "epoch 1770 train loss 0.006983 valid loss 0.005887\n",
      "epoch 1771 train loss 0.005720 valid loss 0.006207\n",
      "epoch 1772 train loss 0.005753 valid loss 0.005902\n",
      "epoch 1773 train loss 0.005539 valid loss 0.005884\n",
      "epoch 1774 train loss 0.005512 valid loss 0.005920\n",
      "epoch 1775 train loss 0.005611 valid loss 0.005671\n",
      "epoch 1776 train loss 0.005628 valid loss 0.008657\n",
      "epoch 1777 train loss 0.008286 valid loss 0.008164\n",
      "epoch 1778 train loss 0.006539 valid loss 0.006057\n",
      "epoch 1779 train loss 0.005819 valid loss 0.006983\n",
      "epoch 1780 train loss 0.005840 valid loss 0.005827\n",
      "epoch 1781 train loss 0.005704 valid loss 0.006022\n",
      "epoch 1782 train loss 0.005712 valid loss 0.005890\n",
      "epoch 1783 train loss 0.005782 valid loss 0.005692\n",
      "epoch 1784 train loss 0.005587 valid loss 0.005703\n",
      "epoch 1785 train loss 0.006809 valid loss 0.005733\n",
      "epoch 1786 train loss 0.006042 valid loss 0.006836\n",
      "epoch 1787 train loss 0.006116 valid loss 0.006087\n",
      "epoch 1788 train loss 0.005663 valid loss 0.006891\n",
      "epoch 1789 train loss 0.006105 valid loss 0.006490\n",
      "epoch 1790 train loss 0.005976 valid loss 0.005868\n",
      "epoch 1791 train loss 0.005813 valid loss 0.005834\n",
      "epoch 1792 train loss 0.005618 valid loss 0.005903\n",
      "epoch 1793 train loss 0.006064 valid loss 0.005952\n",
      "epoch 1794 train loss 0.005585 valid loss 0.006388\n",
      "epoch 1795 train loss 0.005721 valid loss 0.005803\n",
      "epoch 1796 train loss 0.005683 valid loss 0.005828\n",
      "epoch 1797 train loss 0.005518 valid loss 0.005878\n",
      "epoch 1798 train loss 0.006164 valid loss 0.006034\n",
      "epoch 1799 train loss 0.005853 valid loss 0.005993\n",
      "epoch 1800 train loss 0.005764 valid loss 0.006295\n",
      "epoch 1801 train loss 0.005911 valid loss 0.005951\n",
      "epoch 1802 train loss 0.005812 valid loss 0.005855\n",
      "epoch 1803 train loss 0.005782 valid loss 0.005986\n",
      "epoch 1804 train loss 0.005538 valid loss 0.006898\n",
      "epoch 1805 train loss 0.006394 valid loss 0.005812\n",
      "epoch 1806 train loss 0.005885 valid loss 0.006081\n",
      "epoch 1807 train loss 0.005732 valid loss 0.006217\n",
      "epoch 1808 train loss 0.005903 valid loss 0.006342\n",
      "epoch 1809 train loss 0.005849 valid loss 0.005949\n",
      "epoch 1810 train loss 0.005778 valid loss 0.005973\n",
      "epoch 1811 train loss 0.005699 valid loss 0.006149\n",
      "epoch 1812 train loss 0.005770 valid loss 0.006091\n",
      "epoch 1813 train loss 0.005764 valid loss 0.006226\n",
      "epoch 1814 train loss 0.005611 valid loss 0.005874\n",
      "epoch 1815 train loss 0.005425 valid loss 0.006034\n",
      "epoch 1816 train loss 0.005692 valid loss 0.005742\n",
      "epoch 1817 train loss 0.005709 valid loss 0.005866\n",
      "epoch 1818 train loss 0.005546 valid loss 0.006015\n",
      "epoch 1819 train loss 0.005468 valid loss 0.005934\n",
      "epoch 1820 train loss 0.005698 valid loss 0.005992\n",
      "epoch 1821 train loss 0.005760 valid loss 0.005973\n",
      "epoch 1822 train loss 0.005471 valid loss 0.005760\n",
      "epoch 1823 train loss 0.005513 valid loss 0.005976\n",
      "epoch 1824 train loss 0.005699 valid loss 0.006437\n",
      "epoch 1825 train loss 0.005892 valid loss 0.006100\n",
      "epoch 1826 train loss 0.005869 valid loss 0.007149\n",
      "epoch 1827 train loss 0.005934 valid loss 0.005989\n",
      "epoch 1828 train loss 0.005623 valid loss 0.006444\n",
      "epoch 1829 train loss 0.005740 valid loss 0.005651\n",
      "epoch 1830 train loss 0.005594 valid loss 0.005676\n",
      "epoch 1831 train loss 0.005510 valid loss 0.005914\n",
      "epoch 1832 train loss 0.005465 valid loss 0.005979\n",
      "epoch 1833 train loss 0.005629 valid loss 0.005803\n",
      "epoch 1834 train loss 0.005425 valid loss 0.005911\n",
      "epoch 1835 train loss 0.005884 valid loss 0.005742\n",
      "epoch 1836 train loss 0.005709 valid loss 0.005864\n",
      "epoch 1837 train loss 0.005552 valid loss 0.005902\n",
      "epoch 1838 train loss 0.005453 valid loss 0.005758\n",
      "epoch 1839 train loss 0.005467 valid loss 0.005892\n",
      "epoch 1840 train loss 0.005545 valid loss 0.006131\n",
      "epoch 1841 train loss 0.005612 valid loss 0.006303\n",
      "epoch 1842 train loss 0.005600 valid loss 0.006138\n",
      "epoch 1843 train loss 0.005648 valid loss 0.006097\n",
      "epoch 1844 train loss 0.005775 valid loss 0.005869\n",
      "epoch 1845 train loss 0.005505 valid loss 0.006216\n",
      "epoch 1846 train loss 0.005756 valid loss 0.006005\n",
      "epoch 1847 train loss 0.005638 valid loss 0.005888\n",
      "epoch 1848 train loss 0.005923 valid loss 0.006441\n",
      "epoch 1849 train loss 0.005620 valid loss 0.005606\n",
      "epoch 1850 train loss 0.005567 valid loss 0.006545\n",
      "epoch 1851 train loss 0.005653 valid loss 0.005734\n",
      "epoch 1852 train loss 0.005493 valid loss 0.005680\n",
      "epoch 1853 train loss 0.005632 valid loss 0.005690\n",
      "epoch 1854 train loss 0.005616 valid loss 0.006420\n",
      "epoch 1855 train loss 0.006290 valid loss 0.005863\n",
      "epoch 1856 train loss 0.005846 valid loss 0.005929\n",
      "epoch 1857 train loss 0.005585 valid loss 0.005902\n",
      "epoch 1858 train loss 0.006073 valid loss 0.005996\n",
      "epoch 1859 train loss 0.005676 valid loss 0.006294\n",
      "epoch 1860 train loss 0.006606 valid loss 0.007905\n",
      "epoch 1861 train loss 0.006385 valid loss 0.005999\n",
      "epoch 1862 train loss 0.005829 valid loss 0.006214\n",
      "epoch 1863 train loss 0.005913 valid loss 0.005892\n",
      "epoch 1864 train loss 0.006247 valid loss 0.005980\n",
      "epoch 1865 train loss 0.005605 valid loss 0.005970\n",
      "epoch 1866 train loss 0.005765 valid loss 0.006069\n",
      "epoch 1867 train loss 0.005671 valid loss 0.005787\n",
      "epoch 1868 train loss 0.005655 valid loss 0.006074\n",
      "epoch 1869 train loss 0.005732 valid loss 0.005998\n",
      "epoch 1870 train loss 0.005643 valid loss 0.006028\n",
      "epoch 1871 train loss 0.005711 valid loss 0.006172\n",
      "epoch 1872 train loss 0.006064 valid loss 0.006274\n",
      "epoch 1873 train loss 0.005581 valid loss 0.006360\n",
      "epoch 1874 train loss 0.005872 valid loss 0.006050\n",
      "epoch 1875 train loss 0.005728 valid loss 0.005927\n",
      "epoch 1876 train loss 0.005686 valid loss 0.006335\n",
      "epoch 1877 train loss 0.005838 valid loss 0.006080\n",
      "epoch 1878 train loss 0.005623 valid loss 0.006025\n",
      "epoch 1879 train loss 0.005658 valid loss 0.006368\n",
      "epoch 1880 train loss 0.005945 valid loss 0.005795\n",
      "epoch 1881 train loss 0.005740 valid loss 0.006071\n",
      "epoch 1882 train loss 0.005696 valid loss 0.005961\n",
      "epoch 1883 train loss 0.005651 valid loss 0.006069\n",
      "epoch 1884 train loss 0.005745 valid loss 0.006289\n",
      "epoch 1885 train loss 0.005972 valid loss 0.006459\n",
      "epoch 1886 train loss 0.005937 valid loss 0.005950\n",
      "epoch 1887 train loss 0.006096 valid loss 0.005860\n",
      "epoch 1888 train loss 0.005723 valid loss 0.005788\n",
      "epoch 1889 train loss 0.005384 valid loss 0.005783\n",
      "epoch 1890 train loss 0.005612 valid loss 0.006030\n",
      "epoch 1891 train loss 0.005854 valid loss 0.005897\n",
      "epoch 1892 train loss 0.005785 valid loss 0.006050\n",
      "epoch 1893 train loss 0.005611 valid loss 0.006006\n",
      "epoch 1894 train loss 0.005731 valid loss 0.006088\n",
      "epoch 1895 train loss 0.005646 valid loss 0.005861\n",
      "epoch 1896 train loss 0.005808 valid loss 0.005833\n",
      "epoch 1897 train loss 0.005526 valid loss 0.006018\n",
      "epoch 1898 train loss 0.005850 valid loss 0.006176\n",
      "epoch 1899 train loss 0.005852 valid loss 0.005957\n",
      "epoch 1900 train loss 0.005544 valid loss 0.006185\n",
      "epoch 1901 train loss 0.005735 valid loss 0.006387\n",
      "epoch 1902 train loss 0.005883 valid loss 0.006254\n",
      "epoch 1903 train loss 0.005744 valid loss 0.006626\n",
      "epoch 1904 train loss 0.005754 valid loss 0.005969\n",
      "epoch 1905 train loss 0.005660 valid loss 0.005721\n",
      "epoch 1906 train loss 0.005597 valid loss 0.005884\n",
      "epoch 1907 train loss 0.005505 valid loss 0.005696\n",
      "epoch 1908 train loss 0.005536 valid loss 0.005804\n",
      "epoch 1909 train loss 0.005819 valid loss 0.006304\n",
      "epoch 1910 train loss 0.006211 valid loss 0.005960\n",
      "epoch 1911 train loss 0.005519 valid loss 0.005913\n",
      "epoch 1912 train loss 0.005487 valid loss 0.005845\n",
      "epoch 1913 train loss 0.005583 valid loss 0.006406\n",
      "epoch 1914 train loss 0.005923 valid loss 0.005825\n",
      "epoch 1915 train loss 0.005575 valid loss 0.006244\n",
      "epoch 1916 train loss 0.005693 valid loss 0.005889\n",
      "epoch 1917 train loss 0.005856 valid loss 0.006165\n",
      "epoch 1918 train loss 0.006193 valid loss 0.006013\n",
      "epoch 1919 train loss 0.005775 valid loss 0.006036\n",
      "epoch 1920 train loss 0.005903 valid loss 0.005898\n",
      "epoch 1921 train loss 0.005698 valid loss 0.006019\n",
      "epoch 1922 train loss 0.005747 valid loss 0.005984\n",
      "epoch 1923 train loss 0.006365 valid loss 0.005954\n",
      "epoch 1924 train loss 0.005595 valid loss 0.005980\n",
      "epoch 1925 train loss 0.005669 valid loss 0.006371\n",
      "epoch 1926 train loss 0.005964 valid loss 0.005784\n",
      "epoch 1927 train loss 0.005601 valid loss 0.005815\n",
      "epoch 1928 train loss 0.005548 valid loss 0.006175\n",
      "epoch 1929 train loss 0.006367 valid loss 0.006021\n",
      "epoch 1930 train loss 0.005742 valid loss 0.006160\n",
      "epoch 1931 train loss 0.005655 valid loss 0.005965\n",
      "epoch 1932 train loss 0.006013 valid loss 0.005866\n",
      "epoch 1933 train loss 0.006058 valid loss 0.005899\n",
      "epoch 1934 train loss 0.005669 valid loss 0.006234\n",
      "epoch 1935 train loss 0.005704 valid loss 0.006293\n",
      "epoch 1936 train loss 0.005700 valid loss 0.005694\n",
      "epoch 1937 train loss 0.005504 valid loss 0.005782\n",
      "epoch 1938 train loss 0.005624 valid loss 0.006051\n",
      "epoch 1939 train loss 0.005846 valid loss 0.005925\n",
      "epoch 1940 train loss 0.005516 valid loss 0.005836\n",
      "epoch 1941 train loss 0.005894 valid loss 0.005862\n",
      "epoch 1942 train loss 0.005487 valid loss 0.005963\n",
      "epoch 1943 train loss 0.005873 valid loss 0.005992\n",
      "epoch 1944 train loss 0.005761 valid loss 0.005718\n",
      "epoch 1945 train loss 0.005699 valid loss 0.005916\n",
      "epoch 1946 train loss 0.005643 valid loss 0.006110\n",
      "epoch 1947 train loss 0.006616 valid loss 0.005763\n",
      "epoch 1948 train loss 0.005513 valid loss 0.006116\n",
      "epoch 1949 train loss 0.005829 valid loss 0.005861\n",
      "epoch 1950 train loss 0.005708 valid loss 0.005691\n",
      "epoch 1951 train loss 0.005626 valid loss 0.006125\n",
      "epoch 1952 train loss 0.006196 valid loss 0.006060\n",
      "epoch 1953 train loss 0.005443 valid loss 0.005741\n",
      "epoch 1954 train loss 0.005408 valid loss 0.005709\n",
      "epoch 1955 train loss 0.005538 valid loss 0.005821\n",
      "epoch 1956 train loss 0.005745 valid loss 0.005964\n",
      "epoch 1957 train loss 0.005688 valid loss 0.005750\n",
      "epoch 1958 train loss 0.005479 valid loss 0.005881\n",
      "epoch 1959 train loss 0.005853 valid loss 0.005901\n",
      "epoch 1960 train loss 0.005578 valid loss 0.006018\n",
      "epoch 1961 train loss 0.005882 valid loss 0.006069\n",
      "epoch 1962 train loss 0.005751 valid loss 0.006126\n",
      "epoch 1963 train loss 0.005699 valid loss 0.005718\n",
      "epoch 1964 train loss 0.005631 valid loss 0.005976\n",
      "epoch 1965 train loss 0.005796 valid loss 0.005716\n",
      "epoch 1966 train loss 0.005554 valid loss 0.005785\n",
      "epoch 1967 train loss 0.005623 valid loss 0.005855\n",
      "epoch 1968 train loss 0.005684 valid loss 0.005731\n",
      "epoch 1969 train loss 0.005418 valid loss 0.005869\n",
      "epoch 1970 train loss 0.005626 valid loss 0.006044\n",
      "epoch 1971 train loss 0.005573 valid loss 0.005800\n",
      "epoch 1972 train loss 0.005487 valid loss 0.006348\n",
      "epoch 1973 train loss 0.005712 valid loss 0.005931\n",
      "epoch 1974 train loss 0.005403 valid loss 0.005766\n",
      "epoch 1975 train loss 0.005790 valid loss 0.005980\n",
      "epoch 1976 train loss 0.005832 valid loss 0.006154\n",
      "epoch 1977 train loss 0.005892 valid loss 0.005921\n",
      "epoch 1978 train loss 0.005499 valid loss 0.006573\n",
      "epoch 1979 train loss 0.005915 valid loss 0.005785\n",
      "epoch 1980 train loss 0.005453 valid loss 0.005965\n",
      "epoch 1981 train loss 0.005902 valid loss 0.005774\n",
      "epoch 1982 train loss 0.005996 valid loss 0.005855\n",
      "epoch 1983 train loss 0.005650 valid loss 0.006283\n",
      "epoch 1984 train loss 0.005982 valid loss 0.005964\n",
      "epoch 1985 train loss 0.005423 valid loss 0.005988\n",
      "epoch 1986 train loss 0.005679 valid loss 0.005731\n",
      "epoch 1987 train loss 0.005438 valid loss 0.006468\n",
      "epoch 1988 train loss 0.005715 valid loss 0.006232\n",
      "epoch 1989 train loss 0.005868 valid loss 0.008174\n",
      "epoch 1990 train loss 0.006275 valid loss 0.006038\n",
      "epoch 1991 train loss 0.005685 valid loss 0.006156\n",
      "epoch 1992 train loss 0.005827 valid loss 0.006033\n",
      "epoch 1993 train loss 0.005644 valid loss 0.006221\n",
      "epoch 1994 train loss 0.006197 valid loss 0.005914\n",
      "epoch 1995 train loss 0.005831 valid loss 0.005885\n",
      "epoch 1996 train loss 0.005720 valid loss 0.006492\n",
      "epoch 1997 train loss 0.005865 valid loss 0.005879\n",
      "epoch 1998 train loss 0.006087 valid loss 0.007227\n",
      "epoch 1999 train loss 0.006418 valid loss 0.006316\n",
      "epoch 2000 train loss 0.006105 valid loss 0.006196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Dipole_Moment()\n",
    "params, list_train_loss, list_val_loss = train_model(\n",
    "    key=train_key,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    valid_data=valid_data,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model_save_path = (\n",
    "    \"mode_training_Si16Vplus..DFT.SP-GRD.wB97X-D.tight.Data.5042.R_E_F_D_Q.pkl\"\n",
    ")\n",
    "with open(model_save_path, \"wb\") as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, positions, target_all = (\n",
    "    valid_data[\"atomic_numbers\"],\n",
    "    valid_data[\"positions\"],\n",
    "    valid_data[\"dipole_moment\"],\n",
    ")\n",
    "#positions -= positions[0, ...]\n",
    "#positions = positions - positions[:, 0:1, :]\n",
    "prediction_all = model.apply(params, Z, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, degrees\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "\n",
    "\n",
    "def angle_between(a, b):\n",
    "\n",
    "    theta_degrees = degrees(acos((np.dot(a, b)) / (la.norm(a) * la.norm(b))))\n",
    "    return theta_degrees\n",
    "\n",
    "\n",
    "total = 0\n",
    "list_angle = []\n",
    "for i in range(len((prediction_all))):\n",
    "    v = prediction_all[i]\n",
    "    u = target_all[i]\n",
    "    angle_degrees = angle_between(v, u)\n",
    "    list_angle.append(angle_degrees)\n",
    "    total += angle_degrees**2\n",
    "\n",
    "print(np.sqrt(total / len(prediction_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    list_angle,\n",
    "    bins=40,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(\"Ángulo (°)\")\n",
    "plt.ylabel(\"Densidad de Frecuencia\")\n",
    "plt.title(\"Histograma de la Distribución de Ángulos\")\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_sq\n",
    "\n",
    "def calcular_magnitud(vector):\n",
    "\n",
    "    vector = np.array(vector)\n",
    "    magnitud = np.linalg.norm(vector)\n",
    "    return magnitud\n",
    "\n",
    "\n",
    "total = 0\n",
    "targets_mag = []\n",
    "predictions_mag = []\n",
    "\n",
    "for i in range(len((prediction_all))):\n",
    "    v = prediction_all[i]\n",
    "    u = target_all[i]\n",
    "    mag_u = calcular_magnitud(u)\n",
    "    mag_v = calcular_magnitud(v)\n",
    "    diff = mag_u - mag_v\n",
    "    total += diff\n",
    "\n",
    "    targets_mag.append(mag_u)\n",
    "    predictions_mag.append(mag_v)\n",
    "\n",
    "print(np.abs(total / len(prediction_all)))\n",
    "print(\n",
    "    \"mean_absolute_error magnitud:\", mean_absolute_error(targets_mag, predictions_mag)\n",
    ")\n",
    "print(\n",
    "    \"mean squared error magnitud\",\n",
    "    jnp.mean((np.array(predictions_mag) - np.array(targets_mag)) ** 2),\n",
    ")\n",
    "print(\"RMSE magnitud \", root_mean_squared_error(targets_mag, predictions_mag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04944158\n",
      "0.024755895\n",
      "0.003927753\n",
      "0.19397697\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normas = np.linalg.norm(prediction_all, axis=1)\n",
    "#normas = normas[400:600]\n",
    "# Graficar las normas\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(normas)), normas, color=\"b\")\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(\"Índice del vector\")\n",
    "plt.ylabel(\"Norma del vector\")\n",
    "plt.title(\"Normas de los vectores en la matriz\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std predict : 0.031521846\n",
      "std target  : 0.10553296\n",
      "mean_absolute_error : 0.08224416\n",
      "mean squared error 0.011213878\n",
      "RMSE  0.10589421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "print(\"std predict :\", np.std(prediction_all))\n",
    "print(\"std target  :\", np.std(target_all))\n",
    "print(\"mean_absolute_error :\", mean_absolute_error(target_all, prediction_all))\n",
    "print(\"mean squared error\", jnp.mean((prediction_all - target_all) ** 2))\n",
    "print(\"RMSE \", root_mean_squared_error(target_all, prediction_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\n",
      "typ\n",
      "InLine_txt\n",
      "name_original\n",
      "bead\n",
      "number_beads\n",
      "theory_level\n",
      "SuperCell\n",
      "R (1, 200000, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "# huziel\n",
    "filename = \"/home/beemoqc2/Documents/e3x_tranfer/docs/source/examples/datoshuz.npz\"\n",
    "dataset = np.load(filename, allow_pickle=True)\n",
    "for key in dataset.keys():\n",
    "    print(key)\n",
    "\n",
    "print(\"R\", dataset[\"R\"].shape)\n",
    "# Modificar el array \"R\"\n",
    "dataset_modified = {\n",
    "    key: np.squeeze(value, axis=0) if key == \"R\" else value\n",
    "    for key, value in dataset.items()\n",
    "}\n",
    "\n",
    "# Guardar el dataset modificado\n",
    "np.savez(\n",
    "    \"/home/beemoqc2/Documents/e3x_tranfer/SI16VPLUS.E3X.RETRAINED.WB97X-D.TIGHT.TRP.100K.1B.01.POSITION_0_reshape.npz\",\n",
    "    **dataset_modified,\n",
    "    allow_pickle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_datasets_test(filename, key, num_train, num_valid, batch_size):\n",
    "    # Load the dataset.\n",
    "    dataset = np.load(filename)\n",
    "    num_data = len(dataset[\"R\"])\n",
    "\n",
    "    Z = jnp.full(1, 23)\n",
    "    Z = jnp.append(Z, jnp.full(16, 14))\n",
    "    Z = jnp.expand_dims(Z, axis=0)\n",
    "    Z = jnp.repeat(Z, num_data, axis=0)\n",
    "    num_draw = num_train + num_valid\n",
    "    if num_draw > num_data:\n",
    "        raise RuntimeError(\n",
    "            f\"datasets only contains {num_data} points, requested num_train={num_train}, num_valid={num_valid}\"\n",
    "        )\n",
    "\n",
    "    # Randomly draw train and validation sets from dataset.\n",
    "    choice = np.asarray(\n",
    "        jax.random.choice(key, num_data, shape=(num_draw,), replace=False)\n",
    "    )\n",
    "    train_choice = choice[:num_train]\n",
    "    valid_choice = choice[num_train:]\n",
    "\n",
    "    # Collect and return train and validation sets.\n",
    "    train_data = dict(\n",
    "        atomic_numbers=jnp.asarray(Z[train_choice]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][train_choice]),\n",
    "    )\n",
    "    valid_data = dict(\n",
    "        atomic_numbers=jnp.asarray(Z[valid_choice]),\n",
    "        positions=jnp.asarray(dataset[\"R\"][valid_choice]),\n",
    "    )\n",
    "\n",
    "    # Split the training data into batches\n",
    "    train_batches = []\n",
    "    #train_data[\"positions\"] -= train_data[\"positions\"][0, ...]\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        batch_data = {\n",
    "            \"atomic_numbers\": train_data[\"atomic_numbers\"][i : i + batch_size],\n",
    "            \"positions\": train_data[\"positions\"][i : i + batch_size],\n",
    "        }\n",
    "        train_batches.append(batch_data)\n",
    "\n",
    "    return train_batches, valid_data\n",
    "\n",
    "\n",
    "num_train = 200000\n",
    "num_val = 1\n",
    "batch_size = 1024  # Ajusta este valor según la capacidad de tu memoria\n",
    "filename = \"/home/beemoqc2/Documents/e3x_tranfer/SI16VPLUS_E3X_RETRAINED_WB97X_D_TIGHT_TRP_400K_1B_01_POSITION_0_reshape.npz\"\n",
    "key = jax.random.PRNGKey(0)\n",
    "train_batches, valid_data = prepare_datasets_test(\n",
    "    filename, key, num_train, num_val, batch_size\n",
    ")\n",
    "\n",
    "i = 0\n",
    "print(len(train_batches))\n",
    "data_final = []\n",
    "for batch in train_batches:\n",
    "    i +=1\n",
    "    print(i)\n",
    "    Z, positions = (\n",
    "        batch[\"atomic_numbers\"],\n",
    "        batch[\"positions\"],\n",
    "    )\n",
    "    \n",
    "    prediction = model.apply(params, Z, positions)\n",
    "    data_final.append(prediction)\n",
    "\n",
    "    # Aquí puedes continuar con el procesamiento o entrenamiento con `prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06705698 -0.04933084  0.00866679]\n",
      " [ 0.03873657  0.02406242  0.01783218]\n",
      " [ 0.04842487 -0.09198944  0.13651752]\n",
      " ...\n",
      " [ 0.04310646  0.01008602 -0.03125652]\n",
      " [ 0.0114958   0.03146298  0.01469914]\n",
      " [ 0.01103562  0.07003415  0.01485054]]\n"
     ]
    }
   ],
   "source": [
    "# Concatenar todos los arrays verticalmente\n",
    "data_final_final = np.vstack(data_final)\n",
    "print(data_final_final)\n",
    "# Guardar el array combinado en un archivo .npz\n",
    "np.savez(\n",
    "    \"/home/beemoqc2/Documents/e3x_tranfer/dipole_SI16VPLUS_E3X_RETRAINED_WB97X_D_TIGHT_TRP_400K_1B_01_POSITION_0_v2.npz\",\n",
    "    data=data_final_final,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normas = np.linalg.norm(data_final_final, axis=1)\n",
    "\n",
    "# Graficar las normas\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(range(len(normas)), normas, color=\"b\")\n",
    "\n",
    "# Etiquetas y título\n",
    "plt.xlabel(\"Índice del vector\")\n",
    "plt.ylabel(\"Norma del vector\")\n",
    "plt.title(\"Normas de los vectores en la matriz\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"/home/beemoqc2/Documents/e3x_tranfer/dipole_SI16VPLUS_E3X_RETRAINED_WB97X_D_TIGHT_TRP_400K_1B_01_POSITION_0_v2.npz\"\n",
    "dataset_data = np.load(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another npz huziel\n",
    "filename = \"/home/beemoqc2/Documents/e3x_tranfer/SI16VPLUS_E3X_RETRAINED_WB97X_D_TIGHT_TRP_400K_1B_01_POSITION_0.npz\"\n",
    "dataset = np.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "from typing import List\n",
    "\n",
    "def create_loss_plot(\n",
    "    train_loss: List[np.ndarray],\n",
    "    val_loss: List[np.ndarray],\n",
    "    train_label: str,\n",
    "    val_label: str,\n",
    "    title: str,\n",
    "    filename: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a Plotly figure with training and validation loss curves and save it as an HTML file.\n",
    "\n",
    "    Args:\n",
    "        train_loss (List[np.ndarray]): List of training loss values.\n",
    "        val_loss (List[np.ndarray]): List of validation loss values.\n",
    "        train_label (str): Label for the training loss curve.\n",
    "        val_label (str): Label for the validation loss curve.\n",
    "        title (str): Title of the plot.\n",
    "        filename (str): Filename to save the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    train_loss_list = [float(loss) for loss in train_loss]\n",
    "    val_loss_list = [float(loss) for loss in val_loss]\n",
    "\n",
    "    trace_train = go.Scatter(y=train_loss_list, mode=\"lines\", name=train_label)\n",
    "    trace_val = go.Scatter(y=val_loss_list, mode=\"lines\", name=val_label)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(trace_train)\n",
    "    fig.add_trace(trace_val)\n",
    "    fig.update_layout(\n",
    "        title=title, xaxis_title=\"Epoch\", yaxis_title=\"Loss\", legend_title=\"Legend\"\n",
    "    )\n",
    "    pio.write_html(fig, filename)\n",
    "    #fig.show()\n",
    "\n",
    "create_loss_plot(\n",
    "    list_train_loss,\n",
    "    list_val_loss,\n",
    "    \"Training Loss\",\n",
    "    \"Validation Loss\",\n",
    "    \"Training vs Validation Loss (Train)\",\n",
    "    \"train_vs_val_train_dipole_moment.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "print('positions \\n', positions)\n",
    "\n",
    "error_porcentual = abs(target - prediction) / abs(target) * 100\n",
    "error_porcentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 17, 3)\n",
      "(1, 17)\n",
      "target\n",
      "[ 0.174127   -0.10796383 -0.07853264]\n",
      "prediction\n",
      "[[-0.0212365  -0.03098422 -0.00646497]]\n",
      "mean squared error 0.016428836\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "# positions -= positions[0, ...]\n",
    "#positions = positions - positions[0:1, :]\n",
    "positions = np.expand_dims(positions, axis=0)\n",
    "Z = np.expand_dims(Z, axis=0)\n",
    "print(positions.shape)\n",
    "print(Z.shape)\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "# print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 17, 3)\n",
      "(1, 17)\n",
      "target\n",
      "[ 0.174127   -0.10796383 -0.07853264]\n",
      "prediction\n",
      "[[-0.02008789 -0.03048658 -0.00758984]]\n",
      "mean squared error 0.016251676\n"
     ]
    }
   ],
   "source": [
    "i = 45\n",
    "Z, positions, target = valid_data['atomic_numbers'][i], valid_data['positions'][i], valid_data['dipole_moment'][i]\n",
    "#positions = positions - positions[0:1, :]\n",
    "positions = np.expand_dims(positions, axis=0)\n",
    "Z = np.expand_dims(Z, axis=0)\n",
    "print(positions.shape)\n",
    "print(Z.shape)\n",
    "positions+=jnp.array([0,0,1000])\n",
    "#print(positions)\n",
    "prediction = model.apply(params, Z, positions)\n",
    "\n",
    "print('target')\n",
    "print(target)\n",
    "print('prediction')\n",
    "print(prediction)\n",
    "print('mean squared error', jnp.mean((prediction-target)**2))\n",
    "# print('positions \\n', positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP Dipole Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MP_Dipole_Moment(nn.Module):\n",
    "    features: int = 32\n",
    "    max_degree: int = 2\n",
    "    num_iterations: int = 3\n",
    "    num_basis_functions: int = 8\n",
    "    cutoff: float = 5.0\n",
    "    max_atomic_number: int = 118  # This is overkill for most applications.\n",
    "\n",
    "    def dipole_moment(\n",
    "        self, atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size\n",
    "    ):\n",
    "        # 1. Calculate displacement vectors.\n",
    "        positions_dst = e3x.ops.gather_dst(positions, dst_idx=dst_idx)\n",
    "        positions_src = e3x.ops.gather_src(positions, src_idx=src_idx)\n",
    "        displacements = positions_src - positions_dst  # Shape (num_pairs, 3).\n",
    "\n",
    "        # 2. Expand displacement vectors in basis functions.\n",
    "        basis = e3x.nn.basis(  # Shape (num_pairs, 1, (max_degree+1)**2, num_basis_functions).\n",
    "            displacements,\n",
    "            num=self.num_basis_functions,\n",
    "            max_degree=self.max_degree,\n",
    "            radial_fn=e3x.nn.reciprocal_bernstein,\n",
    "            cutoff_fn=functools.partial(e3x.nn.smooth_cutoff, cutoff=self.cutoff),\n",
    "        )\n",
    "\n",
    "        # 3. Embed atomic numbers in feature space, x has shape (num_atoms, 1, 1, features).\n",
    "        x = e3x.nn.Embed(\n",
    "            num_embeddings=self.max_atomic_number + 1, features=self.features\n",
    "        )(atomic_numbers)\n",
    "        #print('Embed',x.shape)\n",
    "        #print('Basis',basis.shape)\n",
    "\n",
    "        # 4. Perform iterations (message-passing + atom-wise refinement).\n",
    "        for i in range(self.num_iterations):\n",
    "            # Message-pass.\n",
    "            if i == self.num_iterations - 1:  # Final iteration.\n",
    "                # Since we will only use scalar features after the final message-pass, we do not want to produce non-scalar\n",
    "                # features for efficiency reasons.\n",
    "                y = e3x.nn.MessagePass(max_degree=2, include_pseudotensors=False)(\n",
    "                    x, basis, dst_idx=dst_idx, src_idx=src_idx\n",
    "                )\n",
    "                #print('Final',y.shape)\n",
    "                # After the final message pass, we can safely throw away all non-scalar features.\n",
    "                x = e3x.nn.change_max_degree_or_type(\n",
    "                    x, max_degree=2, include_pseudotensors=False\n",
    "                )\n",
    "            else:\n",
    "                # In intermediate iterations, the message-pass should consider all possible coupling paths.\n",
    "                y = e3x.nn.MessagePass()(x, basis, dst_idx=dst_idx, src_idx=src_idx)\n",
    "                #print('Message',y.shape)\n",
    "            y = e3x.nn.add(x, y)\n",
    "\n",
    "            # Atom-wise refinement MLP.\n",
    "            y = e3x.nn.Dense(self.features)(y)\n",
    "            y = e3x.nn.silu(y)\n",
    "            y = e3x.nn.Dense(self.features, kernel_init=jax.nn.initializers.zeros)(y)\n",
    "\n",
    "            # Residual connection.\n",
    "            x = e3x.nn.add(x, y)\n",
    "            #print('Residual',x.shape)\n",
    "\n",
    "        # 5. Predict atomic energies with an ordinary dense layer.\n",
    "        #element_bias = self.param(\n",
    "        #    \"element_bias\",\n",
    "        #    lambda rng, shape: jnp.zeros(shape),\n",
    "        #    (self.max_atomic_number + 1),\n",
    "        #)\n",
    "        x = nn.Dense(1, use_bias=False, kernel_init=jax.nn.initializers.zeros)(x)  # (..., Natoms, 1, 9, 1)\n",
    "        #print('After dense:',x.shape)\n",
    "        x=jnp.sum(x, axis=-4) \n",
    "        #print(\"After sum:\", x.shape)\n",
    "        x = x[..., 1:4, 0]\n",
    "        #print('After slicing:' ,x.shape)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self,\n",
    "        atomic_numbers,\n",
    "        positions,\n",
    "        dst_idx,\n",
    "        src_idx,\n",
    "        batch_segments=None,\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        if batch_segments is None:\n",
    "            batch_segments = jnp.zeros_like(atomic_numbers)\n",
    "            batch_size = 1\n",
    "\n",
    "        # Since we want to also predict forces, i.e. the gradient of the energy w.r.t. positions (argument 1), we use\n",
    "        # jax.value_and_grad to create a function for predicting both energy and forces for us.\n",
    "        \n",
    "        dipole = self.dipole_moment(atomic_numbers, positions, dst_idx, src_idx, batch_segments, batch_size)\n",
    "\n",
    "        return dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "Embed (17, 1, 1, 32)\n",
      "Basis (272, 1, 9, 8)\n",
      "Message (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Message (17, 2, 9, 32)\n",
      "Residual (17, 2, 9, 32)\n",
      "Final (17, 1, 9, 32)\n",
      "Residual (17, 1, 9, 32)\n",
      "After dense: (17, 1, 9, 1)\n",
      "After sum: (1, 9, 1)\n",
      "After slicing: (1, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "dm_model = MP_Dipole_Moment()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Generate train and test datasets.\n",
    "key, data_key = jax.random.split(key)\n",
    "num_train=10\n",
    "num_val=2\n",
    "train_data,valid_data=prepare_datasets(filename,key, num_train,num_val)\n",
    "dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(17)\n",
    "params = dm_model.init(key,\n",
    "    atomic_numbers=train_data['atomic_numbers'][0],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "moment = dm_model.apply(\n",
    "            params,\n",
    "            atomic_numbers=train_data[\"atomic_numbers\"][0],\n",
    "            positions=train_data[\"positions\"][0],\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=None,\n",
    "            batch_size=1,\n",
    "        )\n",
    "print(moment.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(key, data, batch_size):\n",
    "    # Determine the number of training steps per epoch.\n",
    "    data_size = len(data[\"dipole_moment\"])\n",
    "    steps_per_epoch = data_size // batch_size\n",
    "\n",
    "    # Draw random permutations for fetching batches from the train data.\n",
    "    perms = jax.random.permutation(key, data_size)\n",
    "    perms = perms[\n",
    "        : steps_per_epoch * batch_size\n",
    "    ]  # Skip the last batch (if incomplete).\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    # Prepare entries that are identical for each batch.\n",
    "    num_atoms = len(data[\"atomic_numbers\"])\n",
    "    batch_segments = jnp.repeat(jnp.arange(batch_size), num_atoms)\n",
    "    atomic_numbers = jnp.tile(data[\"atomic_numbers\"], batch_size)\n",
    "    offsets = jnp.arange(batch_size) * num_atoms\n",
    "    dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(num_atoms)\n",
    "    dst_idx = (dst_idx + offsets[:, None]).reshape(-1)\n",
    "    src_idx = (src_idx + offsets[:, None]).reshape(-1)\n",
    "\n",
    "    # Assemble and return batches.\n",
    "    return [\n",
    "        dict(\n",
    "            dipole_moment=data[\"dipole_moment\"][perm].reshape(-1, 3),\n",
    "            atomic_numbers=atomic_numbers,\n",
    "            positions=data[\"positions\"][perm].reshape(-1, 3),\n",
    "            dst_idx=dst_idx,\n",
    "            src_idx=src_idx,\n",
    "            batch_segments=batch_segments,\n",
    "        )\n",
    "        for perm in perms\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'optimizer_update', 'batch_size'))\n",
    "def train_step(model_apply, optimizer_update, batch, batch_size, opt_state, params):\n",
    "  def loss_fn(params):\n",
    "    dipole = model_apply(\n",
    "      params,\n",
    "      atomic_numbers=batch['atomic_numbers'],\n",
    "      positions=batch['positions'],\n",
    "      dst_idx=batch['dst_idx'],\n",
    "      src_idx=batch['src_idx'],\n",
    "      batch_segments=batch['batch_segments'],\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "    loss = mean_squared_loss(\n",
    "      dipole_prediction=dipole,\n",
    "      dipole_target=batch['dipole_moment']\n",
    "    )\n",
    "    return loss\n",
    "  loss, grad = jax.value_and_grad(loss_fn)(params)\n",
    "  updates, opt_state = optimizer_update(grad, opt_state, params)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "  return params, opt_state, loss\n",
    "\n",
    "\n",
    "@functools.partial(jax.jit, static_argnames=('model_apply', 'batch_size'))\n",
    "def eval_step(model_apply, batch, batch_size, params):\n",
    "  dipole = model_apply(\n",
    "    params,\n",
    "    atomic_numbers=batch['atomic_numbers'],\n",
    "    positions=batch['positions'],\n",
    "    dst_idx=batch['dst_idx'],\n",
    "    src_idx=batch['src_idx'],\n",
    "    batch_segments=batch['batch_segments'],\n",
    "    batch_size=batch_size\n",
    "  )\n",
    "  loss = mean_squared_loss(\n",
    "    energy_prediction=dipole,\n",
    "    energy_target=batch['dipole_moment']\n",
    "  )\n",
    "  return loss\n",
    "\n",
    "\n",
    "def train_model(key, model, train_data, valid_data, num_epochs, learning_rate, batch_size):\n",
    "  # Initialize model parameters and optimizer state.\n",
    "  key, init_key = jax.random.split(key)\n",
    "  optimizer = optax.adam(learning_rate)\n",
    "  dst_idx, src_idx = e3x.ops.sparse_pairwise_indices(len(train_data['atomic_numbers']))\n",
    "  params = model.init(init_key,\n",
    "    atomic_numbers=train_data['atomic_numbers'],\n",
    "    positions=train_data['positions'][0],\n",
    "    dst_idx=dst_idx,\n",
    "    src_idx=src_idx,\n",
    "  )\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Batches for the validation set need to be prepared only once.\n",
    "  key, shuffle_key = jax.random.split(key)\n",
    "  valid_batches = prepare_batches(shuffle_key, valid_data, batch_size)\n",
    "\n",
    "  # Train for 'num_epochs' epochs.\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    # Prepare batches.\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    train_batches = prepare_batches(shuffle_key, train_data, batch_size)\n",
    "\n",
    "    # Loop over train batches.\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_batches):\n",
    "      params, opt_state, loss= train_step(\n",
    "        model_apply=model.apply,\n",
    "        optimizer_update=optimizer.update,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        opt_state=opt_state,\n",
    "        params=params\n",
    "      )\n",
    "      train_loss += (loss - train_loss)/(i+1)\n",
    "\n",
    "    # Evaluate on validation set.\n",
    "    valid_loss = 0.0\n",
    "    for i, batch in enumerate(valid_batches):\n",
    "      loss = eval_step(\n",
    "        model_apply=model.apply,\n",
    "        batch=batch,\n",
    "        batch_size=batch_size,\n",
    "        params=params\n",
    "      )\n",
    "      valid_loss += (loss - valid_loss)/(i+1)\n",
    "\n",
    "    # Print progress.\n",
    "    print(f\"epoch: {epoch: 3d}                    train:   valid:\")\n",
    "    print(f\"    loss [a.u.]             {train_loss : 8.3f} {valid_loss : 8.3f}\")\n",
    "\n",
    "\n",
    "  # Return final model parameters.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
